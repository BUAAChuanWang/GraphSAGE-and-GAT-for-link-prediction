{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import visdom\n",
    "\n",
    "from datasets import link_prediction\n",
    "from layers import MeanAggregator, LSTMAggregator, MaxPoolAggregator, MeanPoolAggregator\n",
    "import models\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"dataset\" : \"CollegeMsg\",\n",
    "    \"dataset_path\" : \"/Users/raunak/Documents/Datasets/temporal-networks-snap/CollegeMsg.txt\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"neg_examples_path\" : \"/Users/raunak/Documents/Datasets/temporal-networks-snap/CollegeMsg_neg_examples_train.txt\",\n",
    "    \"task\" : \"link_prediction\",\n",
    "    \"agg_class\" : \"MaxPoolAggregator\",\n",
    "    \"dropout\" : 0.5,\n",
    "    \"cuda\" : \"True\",\n",
    "    \"hidden_dims\" : [64],\n",
    "    \"num_samples\" : -1,\n",
    "    \"batch_size\" : 32,\n",
    "    \"epochs\" : 2,\n",
    "    \"lr\" : 5e-4,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \n",
    "    \"self_loop\" : False,\n",
    "    \"normalize_adj\" : False,\n",
    "    \n",
    "    \"load\" : False,\n",
    "    \"save\" : False\n",
    "}\n",
    "config = args\n",
    "config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "\n",
    "if config['cuda'] and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading CollegeMsg dataset from /Users/raunak/Documents/Datasets/temporal-networks-snap/CollegeMsg.txt\n",
      "Finished reading data.\n",
      "Setting up data structures.\n",
      "Finished setting up data structures.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['neg_examples_path'], 'train',\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "input_dim, output_dim = dataset.get_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphSAGE(\n",
       "  (aggregators): ModuleList(\n",
       "    (0): MaxPoolAggregator(\n",
       "      (fc1): Linear(in_features=1733, out_features=1733, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): MaxPoolAggregator(\n",
       "      (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (fcs): ModuleList(\n",
       "    (0): Linear(in_features=3466, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (bns): ModuleList(\n",
       "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_class = utils.get_agg_class(config['agg_class'])\n",
    "model = models.GraphSAGE(input_dim, config['hidden_dims'], output_dim, config['dropout'],\n",
    "                         agg_class, config['num_samples'], device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset.\n",
      "    Batch 1 / 248\n",
      "    Batch 2 / 248\n",
      "    Batch 3 / 248\n",
      "    Batch 4 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 6 / 248\n",
      "    Batch 7 / 248\n",
      "    Batch 8 / 248\n",
      "    Batch 9 / 248\n",
      "    Batch 10 / 248\n",
      "    Batch 11 / 248\n",
      "    Batch 12 / 248\n",
      "    Batch 13 / 248\n",
      "    Batch 14 / 248\n",
      "    Batch 15 / 248\n",
      "    Batch 16 / 248\n",
      "    Batch 17 / 248\n",
      "    Batch 18 / 248\n",
      "    Batch 19 / 248\n",
      "    Batch 20 / 248\n",
      "    Batch 21 / 248\n",
      "    Batch 22 / 248\n",
      "    Batch 23 / 248\n",
      "    Batch 24 / 248\n",
      "    Batch 25 / 248\n",
      "    Batch 26 / 248\n",
      "    Batch 27 / 248\n",
      "    Batch 28 / 248\n",
      "    Batch 29 / 248\n",
      "    Batch 30 / 248\n",
      "    Batch 31 / 248\n",
      "    Batch 32 / 248\n",
      "    Batch 33 / 248\n",
      "    Batch 34 / 248\n",
      "    Batch 35 / 248\n",
      "    Batch 36 / 248\n",
      "    Batch 37 / 248\n",
      "    Batch 38 / 248\n",
      "    Batch 39 / 248\n",
      "    Batch 40 / 248\n",
      "    Batch 41 / 248\n",
      "    Batch 42 / 248\n",
      "    Batch 43 / 248\n",
      "    Batch 44 / 248\n",
      "    Batch 45 / 248\n",
      "    Batch 46 / 248\n",
      "    Batch 47 / 248\n",
      "    Batch 48 / 248\n",
      "    Batch 49 / 248\n",
      "    Batch 50 / 248\n",
      "    Batch 51 / 248\n",
      "    Batch 52 / 248\n",
      "    Batch 53 / 248\n",
      "    Batch 54 / 248\n",
      "    Batch 55 / 248\n",
      "    Batch 56 / 248\n",
      "    Batch 57 / 248\n",
      "    Batch 58 / 248\n",
      "    Batch 59 / 248\n",
      "    Batch 60 / 248\n",
      "    Batch 61 / 248\n",
      "    Batch 62 / 248\n",
      "    Batch 63 / 248\n",
      "    Batch 64 / 248\n",
      "    Batch 65 / 248\n",
      "    Batch 66 / 248\n",
      "    Batch 67 / 248\n",
      "    Batch 68 / 248\n",
      "    Batch 69 / 248\n",
      "    Batch 70 / 248\n",
      "    Batch 71 / 248\n",
      "    Batch 72 / 248\n",
      "    Batch 73 / 248\n",
      "    Batch 74 / 248\n",
      "    Batch 75 / 248\n",
      "    Batch 76 / 248\n",
      "    Batch 77 / 248\n",
      "    Batch 78 / 248\n",
      "    Batch 79 / 248\n",
      "    Batch 80 / 248\n",
      "    Batch 81 / 248\n",
      "    Batch 82 / 248\n",
      "    Batch 83 / 248\n",
      "    Batch 84 / 248\n",
      "    Batch 85 / 248\n",
      "    Batch 86 / 248\n",
      "    Batch 87 / 248\n",
      "    Batch 88 / 248\n",
      "    Batch 89 / 248\n",
      "    Batch 90 / 248\n",
      "    Batch 91 / 248\n",
      "    Batch 92 / 248\n",
      "    Batch 93 / 248\n",
      "    Batch 94 / 248\n",
      "    Batch 95 / 248\n",
      "    Batch 96 / 248\n",
      "    Batch 97 / 248\n",
      "    Batch 98 / 248\n",
      "    Batch 99 / 248\n",
      "    Batch 100 / 248\n",
      "    Batch 101 / 248\n",
      "    Batch 102 / 248\n",
      "    Batch 103 / 248\n",
      "    Batch 104 / 248\n",
      "    Batch 105 / 248\n",
      "    Batch 106 / 248\n",
      "    Batch 107 / 248\n",
      "    Batch 108 / 248\n",
      "    Batch 109 / 248\n",
      "    Batch 110 / 248\n",
      "    Batch 111 / 248\n",
      "    Batch 112 / 248\n",
      "    Batch 113 / 248\n",
      "    Batch 114 / 248\n",
      "    Batch 115 / 248\n",
      "    Batch 116 / 248\n",
      "    Batch 117 / 248\n",
      "    Batch 118 / 248\n",
      "    Batch 119 / 248\n",
      "    Batch 120 / 248\n",
      "    Batch 121 / 248\n",
      "    Batch 122 / 248\n",
      "    Batch 123 / 248\n",
      "    Batch 124 / 248\n",
      "    Batch 125 / 248\n",
      "    Batch 126 / 248\n",
      "    Batch 127 / 248\n",
      "    Batch 128 / 248\n",
      "    Batch 129 / 248\n",
      "    Batch 130 / 248\n",
      "    Batch 131 / 248\n",
      "    Batch 132 / 248\n",
      "    Batch 133 / 248\n",
      "    Batch 134 / 248\n",
      "    Batch 135 / 248\n",
      "    Batch 136 / 248\n",
      "    Batch 137 / 248\n",
      "    Batch 138 / 248\n",
      "    Batch 139 / 248\n",
      "    Batch 140 / 248\n",
      "    Batch 141 / 248\n",
      "    Batch 142 / 248\n",
      "    Batch 143 / 248\n",
      "    Batch 144 / 248\n",
      "    Batch 145 / 248\n",
      "    Batch 146 / 248\n",
      "    Batch 147 / 248\n",
      "    Batch 148 / 248\n",
      "    Batch 149 / 248\n",
      "    Batch 150 / 248\n",
      "    Batch 151 / 248\n",
      "    Batch 152 / 248\n",
      "    Batch 153 / 248\n",
      "    Batch 154 / 248\n",
      "    Batch 155 / 248\n",
      "    Batch 156 / 248\n",
      "    Batch 157 / 248\n",
      "    Batch 158 / 248\n",
      "    Batch 159 / 248\n",
      "    Batch 160 / 248\n",
      "    Batch 161 / 248\n",
      "    Batch 162 / 248\n",
      "    Batch 163 / 248\n",
      "    Batch 164 / 248\n",
      "    Batch 165 / 248\n",
      "    Batch 166 / 248\n",
      "    Batch 167 / 248\n",
      "    Batch 168 / 248\n",
      "    Batch 169 / 248\n",
      "    Batch 170 / 248\n",
      "    Batch 171 / 248\n",
      "    Batch 172 / 248\n",
      "    Batch 173 / 248\n",
      "    Batch 174 / 248\n",
      "    Batch 175 / 248\n",
      "    Batch 176 / 248\n",
      "    Batch 177 / 248\n",
      "    Batch 178 / 248\n",
      "    Batch 179 / 248\n",
      "    Batch 180 / 248\n",
      "    Batch 181 / 248\n",
      "    Batch 182 / 248\n",
      "    Batch 183 / 248\n",
      "    Batch 184 / 248\n",
      "    Batch 185 / 248\n",
      "    Batch 186 / 248\n",
      "    Batch 187 / 248\n",
      "    Batch 188 / 248\n",
      "    Batch 189 / 248\n",
      "    Batch 190 / 248\n",
      "    Batch 191 / 248\n",
      "    Batch 192 / 248\n",
      "    Batch 193 / 248\n",
      "    Batch 194 / 248\n",
      "    Batch 195 / 248\n",
      "    Batch 196 / 248\n",
      "    Batch 197 / 248\n",
      "    Batch 198 / 248\n",
      "    Batch 199 / 248\n",
      "    Batch 200 / 248\n",
      "    Batch 201 / 248\n",
      "    Batch 202 / 248\n",
      "    Batch 203 / 248\n",
      "    Batch 204 / 248\n",
      "    Batch 205 / 248\n",
      "    Batch 206 / 248\n",
      "    Batch 207 / 248\n",
      "    Batch 208 / 248\n",
      "    Batch 209 / 248\n",
      "    Batch 210 / 248\n",
      "    Batch 211 / 248\n",
      "    Batch 212 / 248\n",
      "    Batch 213 / 248\n",
      "    Batch 214 / 248\n",
      "    Batch 215 / 248\n",
      "    Batch 216 / 248\n",
      "    Batch 217 / 248\n",
      "    Batch 218 / 248\n",
      "    Batch 219 / 248\n",
      "    Batch 220 / 248\n",
      "    Batch 221 / 248\n",
      "    Batch 222 / 248\n",
      "    Batch 223 / 248\n",
      "    Batch 224 / 248\n",
      "    Batch 225 / 248\n",
      "    Batch 226 / 248\n",
      "    Batch 227 / 248\n",
      "    Batch 228 / 248\n",
      "    Batch 229 / 248\n",
      "    Batch 230 / 248\n",
      "    Batch 231 / 248\n",
      "    Batch 232 / 248\n",
      "    Batch 233 / 248\n",
      "    Batch 234 / 248\n",
      "    Batch 235 / 248\n",
      "    Batch 236 / 248\n",
      "    Batch 237 / 248\n",
      "    Batch 238 / 248\n",
      "    Batch 239 / 248\n",
      "    Batch 240 / 248\n",
      "    Batch 241 / 248\n",
      "    Batch 242 / 248\n",
      "    Batch 243 / 248\n",
      "    Batch 244 / 248\n",
      "    Batch 245 / 248\n",
      "    Batch 246 / 248\n",
      "    Batch 247 / 248\n",
      "    Batch 248 / 248\n",
      "ROC-AUC score: 0.5087810592629932\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {}'.format(area))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Training.\n",
      "Epoch 1 / 2\n",
      "    Batch 3 / 248: loss 0.6933706204096476\n",
      "    ROC-AUC score: 0.42063492063492064\n",
      "    Batch 6 / 248: loss 0.6928354303042094\n",
      "    ROC-AUC score: 0.48235294117647065\n",
      "    Batch 9 / 248: loss 0.6908673445383707\n",
      "    ROC-AUC score: 0.6640625\n",
      "    Batch 12 / 248: loss 0.6901973485946655\n",
      "    ROC-AUC score: 0.7099567099567099\n",
      "    Batch 15 / 248: loss 0.6898095806439718\n",
      "    ROC-AUC score: 0.7686274509803922\n",
      "    Batch 18 / 248: loss 0.6923949122428894\n",
      "    ROC-AUC score: 0.5515873015873016\n",
      "    Batch 21 / 248: loss 0.6905598441759745\n",
      "    ROC-AUC score: 0.6431372549019608\n",
      "    Batch 24 / 248: loss 0.6897528568903605\n",
      "    ROC-AUC score: 0.6190476190476191\n",
      "    Batch 27 / 248: loss 0.6875479420026144\n",
      "    ROC-AUC score: 0.8097165991902835\n",
      "    Batch 30 / 248: loss 0.6898790597915649\n",
      "    ROC-AUC score: 0.6753246753246754\n",
      "    Batch 33 / 248: loss 0.6861851414044698\n",
      "    ROC-AUC score: 0.8218623481781376\n",
      "    Batch 36 / 248: loss 0.6838402350743612\n",
      "    ROC-AUC score: 0.7420634920634921\n",
      "    Batch 39 / 248: loss 0.6836388111114502\n",
      "    ROC-AUC score: 0.880952380952381\n",
      "    Batch 42 / 248: loss 0.6743343869845072\n",
      "    ROC-AUC score: 0.7813765182186235\n",
      "    Batch 45 / 248: loss 0.681921124458313\n",
      "    ROC-AUC score: 0.8611111111111112\n",
      "    Batch 48 / 248: loss 0.6709067424138387\n",
      "    ROC-AUC score: 0.8156862745098039\n",
      "    Batch 51 / 248: loss 0.6701017816861471\n",
      "    ROC-AUC score: 0.8958333333333335\n",
      "    Batch 54 / 248: loss 0.6813546021779379\n",
      "    ROC-AUC score: 0.7416666666666667\n",
      "    Batch 57 / 248: loss 0.6673697630564371\n",
      "    ROC-AUC score: 0.773809523809524\n",
      "    Batch 60 / 248: loss 0.6605012615521749\n",
      "    ROC-AUC score: 0.753968253968254\n",
      "    Batch 63 / 248: loss 0.6580861409505209\n",
      "    ROC-AUC score: 0.8588235294117647\n",
      "    Batch 66 / 248: loss 0.6602564056714376\n",
      "    ROC-AUC score: 0.757085020242915\n",
      "    Batch 69 / 248: loss 0.6370923717816671\n",
      "    ROC-AUC score: 0.9563492063492064\n",
      "    Batch 72 / 248: loss 0.6363215843836466\n",
      "    ROC-AUC score: 0.9843137254901961\n",
      "    Batch 75 / 248: loss 0.6334549983342489\n",
      "    ROC-AUC score: 0.91015625\n",
      "    Batch 78 / 248: loss 0.6387239098548889\n",
      "    ROC-AUC score: 0.9635627530364372\n",
      "    Batch 81 / 248: loss 0.6241080363591512\n",
      "    ROC-AUC score: 0.9176470588235295\n",
      "    Batch 84 / 248: loss 0.6091978748639425\n",
      "    ROC-AUC score: 0.8166666666666667\n",
      "    Batch 87 / 248: loss 0.6283952196439108\n",
      "    ROC-AUC score: 0.9227272727272728\n",
      "    Batch 90 / 248: loss 0.5895447731018066\n",
      "    ROC-AUC score: 0.9841269841269842\n",
      "    Batch 93 / 248: loss 0.6236494978268942\n",
      "    ROC-AUC score: 0.8253968253968254\n",
      "    Batch 96 / 248: loss 0.6065810124079386\n",
      "    ROC-AUC score: 0.992063492063492\n",
      "    Batch 99 / 248: loss 0.5709282159805298\n",
      "    ROC-AUC score: 0.9595141700404859\n",
      "    Batch 102 / 248: loss 0.6038019855817159\n",
      "    ROC-AUC score: 0.8015873015873016\n",
      "    Batch 105 / 248: loss 0.6093560655911764\n",
      "    ROC-AUC score: 0.9333333333333333\n",
      "    Batch 108 / 248: loss 0.5901221036911011\n",
      "    ROC-AUC score: 0.90234375\n",
      "    Batch 111 / 248: loss 0.6061332821846008\n",
      "    ROC-AUC score: 0.8663967611336033\n",
      "    Batch 114 / 248: loss 0.5947703321774801\n",
      "    ROC-AUC score: 0.8134920634920635\n",
      "    Batch 117 / 248: loss 0.597136934598287\n",
      "    ROC-AUC score: 0.9541666666666666\n",
      "    Batch 120 / 248: loss 0.5832528471946716\n",
      "    ROC-AUC score: 0.8980392156862746\n",
      "    Batch 123 / 248: loss 0.5750270485877991\n",
      "    ROC-AUC score: 0.8980392156862745\n",
      "    Batch 126 / 248: loss 0.5607139269510905\n",
      "    ROC-AUC score: 0.9125000000000001\n",
      "    Batch 129 / 248: loss 0.5086720784505209\n",
      "    ROC-AUC score: 0.9149797570850201\n",
      "    Batch 132 / 248: loss 0.5728577772776285\n",
      "    ROC-AUC score: 0.828125\n",
      "    Batch 135 / 248: loss 0.5847188433011373\n",
      "    ROC-AUC score: 0.9607843137254901\n",
      "    Batch 138 / 248: loss 0.5730293790499369\n",
      "    ROC-AUC score: 0.8968253968253969\n",
      "    Batch 141 / 248: loss 0.5298672417799631\n",
      "    ROC-AUC score: 0.9761904761904763\n",
      "    Batch 144 / 248: loss 0.5199632942676544\n",
      "    ROC-AUC score: 0.8705882352941177\n",
      "    Batch 147 / 248: loss 0.5348137418429056\n",
      "    ROC-AUC score: 0.8984375\n",
      "    Batch 150 / 248: loss 0.534002552429835\n",
      "    ROC-AUC score: 0.9333333333333333\n",
      "    Batch 153 / 248: loss 0.5478130976359049\n",
      "    ROC-AUC score: 0.8117647058823529\n",
      "    Batch 156 / 248: loss 0.523188054561615\n",
      "    ROC-AUC score: 0.8987854251012145\n",
      "    Batch 159 / 248: loss 0.5961097280184428\n",
      "    ROC-AUC score: 0.8421052631578948\n",
      "    Batch 162 / 248: loss 0.5201402703921\n",
      "    ROC-AUC score: 0.9682539682539681\n",
      "    Batch 165 / 248: loss 0.5385635693868002\n",
      "    ROC-AUC score: 1.0\n",
      "    Batch 168 / 248: loss 0.6158032218615214\n",
      "    ROC-AUC score: 0.9291666666666667\n",
      "    Batch 171 / 248: loss 0.525372326374054\n",
      "    ROC-AUC score: 0.8984375\n",
      "    Batch 174 / 248: loss 0.5741863449414571\n",
      "    ROC-AUC score: 0.803921568627451\n",
      "    Batch 177 / 248: loss 0.4963056842486064\n",
      "    ROC-AUC score: 0.8787878787878789\n",
      "    Batch 180 / 248: loss 0.5627654194831848\n",
      "    ROC-AUC score: 0.96875\n",
      "    Batch 183 / 248: loss 0.5553095936775208\n",
      "    ROC-AUC score: 0.9294117647058824\n",
      "    Batch 186 / 248: loss 0.5321845809618632\n",
      "    ROC-AUC score: 0.8509803921568627\n",
      "    Batch 189 / 248: loss 0.5448175370693207\n",
      "    ROC-AUC score: 0.9314285714285715\n",
      "    Batch 192 / 248: loss 0.468610684076945\n",
      "    ROC-AUC score: 1.0\n",
      "    Batch 195 / 248: loss 0.4861705998579661\n",
      "    ROC-AUC score: 0.8704453441295545\n",
      "    Batch 198 / 248: loss 0.5451757113138834\n",
      "    ROC-AUC score: 0.8253968253968254\n",
      "    Batch 201 / 248: loss 0.46886706352233887\n",
      "    ROC-AUC score: 0.9372549019607843\n",
      "    Batch 204 / 248: loss 0.510519802570343\n",
      "    ROC-AUC score: 0.921875\n",
      "    Batch 207 / 248: loss 0.5832352240880331\n",
      "    ROC-AUC score: 0.9433198380566802\n",
      "    Batch 210 / 248: loss 0.4324922164281209\n",
      "    ROC-AUC score: 0.961352657004831\n",
      "    Batch 213 / 248: loss 0.5298296213150024\n",
      "    ROC-AUC score: 0.8293650793650794\n",
      "    Batch 216 / 248: loss 0.5380240380764008\n",
      "    ROC-AUC score: 0.8412698412698413\n",
      "    Batch 219 / 248: loss 0.6102517445882162\n",
      "    ROC-AUC score: 0.861111111111111\n",
      "    Batch 222 / 248: loss 0.5281451940536499\n",
      "    ROC-AUC score: 0.89453125\n",
      "    Batch 225 / 248: loss 0.49453239639600116\n",
      "    ROC-AUC score: 0.9878542510121459\n",
      "    Batch 228 / 248: loss 0.48636812965075177\n",
      "    ROC-AUC score: 0.9722222222222222\n",
      "    Batch 231 / 248: loss 0.43780070543289185\n",
      "    ROC-AUC score: 0.8541666666666667\n",
      "    Batch 234 / 248: loss 0.5468232234319051\n",
      "    ROC-AUC score: 0.9134199134199135\n",
      "    Batch 237 / 248: loss 0.5576071937878927\n",
      "    ROC-AUC score: 0.9583333333333334\n",
      "    Batch 240 / 248: loss 0.5631746550401052\n",
      "    ROC-AUC score: 0.6944444444444444\n",
      "    Batch 243 / 248: loss 0.5113929708798727\n",
      "    ROC-AUC score: 0.803921568627451\n",
      "    Batch 246 / 248: loss 0.5049674908320109\n",
      "    ROC-AUC score: 0.9801587301587302\n",
      "Epoch 2 / 2\n",
      "    Batch 3 / 248: loss 0.47552551825841266\n",
      "    ROC-AUC score: 0.9473684210526316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f25fe37d89c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    vis = visdom.Visdom()\n",
    "    loss_window = None\n",
    "    criterion = utils.get_criterion(config['task'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                           weight_decay=config['weight_decay'])\n",
    "    epochs = config['epochs']\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.8)\n",
    "    model.train()\n",
    "    print('--------------------------------')\n",
    "    print('Training.')\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "        running_loss = 0.0\n",
    "        # num_correct, num_examples = 0, 0\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            loss = criterion(scores, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "                # predictions = (scores >= 0.5).long()\n",
    "                # num_correct = torch.sum(predictions == labels.long()).item()\n",
    "                # num_examples += len(labels)\n",
    "            if (idx + 1) % stats_per_batch == 0:\n",
    "                running_loss /= stats_per_batch\n",
    "                # accuracy = num_correct / num_examples\n",
    "                area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "                # print('    Batch {} / {}: loss {}, accuracy {}'.format(\n",
    "                #     idx+1, num_batches, running_loss, accuracy))\n",
    "                print('    Batch {} / {}: loss {}'.format(\n",
    "                    idx+1, num_batches, running_loss))\n",
    "                print('    ROC-AUC score: {}'.format(area))\n",
    "                running_loss = 0.0\n",
    "                num_correct, num_examples = 0, 0\n",
    "            if loss_window is None:\n",
    "                loss_window = vis.line(\n",
    "                    Y=[loss.item()],\n",
    "                    X=[epoch*num_batches+idx],\n",
    "                    opts=dict(xlabel='batch', ylabel='Loss', title='Training Loss', legend=['Loss']))\n",
    "            else:\n",
    "                vis.line(\n",
    "                    [loss.item()],\n",
    "                    [epoch*num_batches+idx],\n",
    "                    win=loss_window,\n",
    "                    update='append')\n",
    "            scheduler.step()\n",
    "    vis.close(win=loss_window)\n",
    "    print('Finished training.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Saving model at /Users/raunak/Documents/Projects/Temporal-Graph-Neural-Networks/trained_models/graphsage_agg_class_MaxPoolAggregator_hidden_dims_64_num_samples_-1_batch_size_32_epochs_2_lr_0.0005_weight_decay_0.0005.pth\n",
      "Finished saving model.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    config['save'] = True\n",
    "    if config['save']:\n",
    "        print('--------------------------------')\n",
    "        directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                'trained_models')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        fname = utils.get_fname(config)\n",
    "        path = os.path.join(directory, fname)\n",
    "        print('Saving model at {}'.format(path))\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print('Finished saving model.')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset.\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 5 / 248\n",
      "ROC-AUC score: 0.9364938313046703\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {}'.format(area))\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    tpr, fpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    tnr = 1 - fpr\n",
    "    plt.plot(thresholds, tpr, label='tpr')\n",
    "    plt.plot(thresholds, tnr, label='tnr')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.title('TPR / FPR vs Threshold')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 1 / 248\n",
      "    Batch 2 / 248\n",
      "    Batch 3 / 248\n",
      "    Batch 4 / 248\n",
      "    Batch 5 / 248\n",
      "    Batch 6 / 248\n",
      "    Batch 7 / 248\n",
      "    Batch 8 / 248\n",
      "    Batch 9 / 248\n",
      "    Batch 10 / 248\n",
      "    Batch 11 / 248\n",
      "    Batch 12 / 248\n",
      "    Batch 13 / 248\n",
      "    Batch 14 / 248\n",
      "    Batch 15 / 248\n",
      "    Batch 16 / 248\n",
      "    Batch 17 / 248\n",
      "    Batch 18 / 248\n",
      "    Batch 19 / 248\n",
      "    Batch 20 / 248\n",
      "    Batch 21 / 248\n",
      "    Batch 22 / 248\n",
      "    Batch 23 / 248\n",
      "    Batch 24 / 248\n",
      "    Batch 25 / 248\n",
      "    Batch 26 / 248\n",
      "    Batch 27 / 248\n",
      "    Batch 28 / 248\n",
      "    Batch 29 / 248\n",
      "    Batch 30 / 248\n",
      "    Batch 31 / 248\n",
      "    Batch 32 / 248\n",
      "    Batch 33 / 248\n",
      "    Batch 34 / 248\n",
      "    Batch 35 / 248\n",
      "    Batch 36 / 248\n",
      "    Batch 37 / 248\n",
      "    Batch 38 / 248\n",
      "    Batch 39 / 248\n",
      "    Batch 40 / 248\n",
      "    Batch 41 / 248\n",
      "    Batch 42 / 248\n",
      "    Batch 43 / 248\n",
      "    Batch 44 / 248\n",
      "    Batch 45 / 248\n",
      "    Batch 46 / 248\n",
      "    Batch 47 / 248\n",
      "    Batch 48 / 248\n",
      "    Batch 49 / 248\n",
      "    Batch 50 / 248\n",
      "    Batch 51 / 248\n",
      "    Batch 52 / 248\n",
      "    Batch 53 / 248\n",
      "    Batch 54 / 248\n",
      "    Batch 55 / 248\n",
      "    Batch 56 / 248\n",
      "    Batch 57 / 248\n",
      "    Batch 58 / 248\n",
      "    Batch 59 / 248\n",
      "    Batch 60 / 248\n",
      "    Batch 61 / 248\n",
      "    Batch 62 / 248\n",
      "    Batch 63 / 248\n",
      "    Batch 64 / 248\n",
      "    Batch 65 / 248\n",
      "    Batch 66 / 248\n",
      "    Batch 67 / 248\n",
      "    Batch 68 / 248\n",
      "    Batch 69 / 248\n",
      "    Batch 70 / 248\n",
      "    Batch 71 / 248\n",
      "    Batch 72 / 248\n",
      "    Batch 73 / 248\n",
      "    Batch 74 / 248\n",
      "    Batch 75 / 248\n",
      "    Batch 76 / 248\n",
      "    Batch 77 / 248\n",
      "    Batch 78 / 248\n",
      "    Batch 79 / 248\n",
      "    Batch 80 / 248\n",
      "    Batch 81 / 248\n",
      "    Batch 82 / 248\n",
      "    Batch 83 / 248\n",
      "    Batch 84 / 248\n",
      "    Batch 85 / 248\n",
      "    Batch 86 / 248\n",
      "    Batch 87 / 248\n",
      "    Batch 88 / 248\n",
      "    Batch 89 / 248\n",
      "    Batch 90 / 248\n",
      "    Batch 91 / 248\n",
      "    Batch 92 / 248\n",
      "    Batch 93 / 248\n",
      "    Batch 94 / 248\n",
      "    Batch 95 / 248\n",
      "    Batch 96 / 248\n",
      "    Batch 97 / 248\n",
      "    Batch 98 / 248\n",
      "    Batch 99 / 248\n",
      "    Batch 100 / 248\n",
      "    Batch 101 / 248\n",
      "    Batch 102 / 248\n",
      "    Batch 103 / 248\n",
      "    Batch 104 / 248\n",
      "    Batch 105 / 248\n",
      "    Batch 106 / 248\n",
      "    Batch 107 / 248\n",
      "    Batch 108 / 248\n",
      "    Batch 109 / 248\n",
      "    Batch 110 / 248\n",
      "    Batch 111 / 248\n",
      "    Batch 112 / 248\n",
      "    Batch 113 / 248\n",
      "    Batch 114 / 248\n",
      "    Batch 115 / 248\n",
      "    Batch 116 / 248\n",
      "    Batch 117 / 248\n",
      "    Batch 118 / 248\n",
      "    Batch 119 / 248\n",
      "    Batch 120 / 248\n",
      "    Batch 121 / 248\n",
      "    Batch 122 / 248\n",
      "    Batch 123 / 248\n",
      "    Batch 124 / 248\n",
      "    Batch 125 / 248\n",
      "    Batch 126 / 248\n",
      "    Batch 127 / 248\n",
      "    Batch 128 / 248\n",
      "    Batch 129 / 248\n",
      "    Batch 130 / 248\n",
      "    Batch 131 / 248\n",
      "    Batch 132 / 248\n",
      "    Batch 133 / 248\n",
      "    Batch 134 / 248\n",
      "    Batch 135 / 248\n",
      "    Batch 136 / 248\n",
      "    Batch 137 / 248\n",
      "    Batch 138 / 248\n",
      "    Batch 139 / 248\n",
      "    Batch 140 / 248\n",
      "    Batch 141 / 248\n",
      "    Batch 142 / 248\n",
      "    Batch 143 / 248\n",
      "    Batch 144 / 248\n",
      "    Batch 145 / 248\n",
      "    Batch 146 / 248\n",
      "    Batch 147 / 248\n",
      "    Batch 148 / 248\n",
      "    Batch 149 / 248\n",
      "    Batch 150 / 248\n",
      "    Batch 151 / 248\n",
      "    Batch 152 / 248\n",
      "    Batch 153 / 248\n",
      "    Batch 154 / 248\n",
      "    Batch 155 / 248\n",
      "    Batch 156 / 248\n",
      "    Batch 157 / 248\n",
      "    Batch 158 / 248\n",
      "    Batch 159 / 248\n",
      "    Batch 160 / 248\n",
      "    Batch 161 / 248\n",
      "    Batch 162 / 248\n",
      "    Batch 163 / 248\n",
      "    Batch 164 / 248\n",
      "    Batch 165 / 248\n",
      "    Batch 166 / 248\n",
      "    Batch 167 / 248\n",
      "    Batch 168 / 248\n",
      "    Batch 169 / 248\n",
      "    Batch 170 / 248\n",
      "    Batch 171 / 248\n",
      "    Batch 172 / 248\n",
      "    Batch 173 / 248\n",
      "    Batch 174 / 248\n",
      "    Batch 175 / 248\n",
      "    Batch 176 / 248\n",
      "    Batch 177 / 248\n",
      "    Batch 178 / 248\n",
      "    Batch 179 / 248\n",
      "    Batch 180 / 248\n",
      "    Batch 181 / 248\n",
      "    Batch 182 / 248\n",
      "    Batch 183 / 248\n",
      "    Batch 184 / 248\n",
      "    Batch 185 / 248\n",
      "    Batch 186 / 248\n",
      "    Batch 187 / 248\n",
      "    Batch 188 / 248\n",
      "    Batch 189 / 248\n",
      "    Batch 190 / 248\n",
      "    Batch 191 / 248\n",
      "    Batch 192 / 248\n",
      "    Batch 193 / 248\n",
      "    Batch 194 / 248\n",
      "    Batch 195 / 248\n",
      "    Batch 196 / 248\n",
      "    Batch 197 / 248\n",
      "    Batch 198 / 248\n",
      "    Batch 199 / 248\n",
      "    Batch 200 / 248\n",
      "    Batch 201 / 248\n",
      "    Batch 202 / 248\n",
      "    Batch 203 / 248\n",
      "    Batch 204 / 248\n",
      "    Batch 205 / 248\n",
      "    Batch 206 / 248\n",
      "    Batch 207 / 248\n",
      "    Batch 208 / 248\n",
      "    Batch 209 / 248\n",
      "    Batch 210 / 248\n",
      "    Batch 211 / 248\n",
      "    Batch 212 / 248\n",
      "    Batch 213 / 248\n",
      "    Batch 214 / 248\n",
      "    Batch 215 / 248\n",
      "    Batch 216 / 248\n",
      "    Batch 217 / 248\n",
      "    Batch 218 / 248\n",
      "    Batch 219 / 248\n",
      "    Batch 220 / 248\n",
      "    Batch 221 / 248\n",
      "    Batch 222 / 248\n",
      "    Batch 223 / 248\n",
      "    Batch 224 / 248\n",
      "    Batch 225 / 248\n",
      "    Batch 226 / 248\n",
      "    Batch 227 / 248\n",
      "    Batch 228 / 248\n",
      "    Batch 229 / 248\n",
      "    Batch 230 / 248\n",
      "    Batch 231 / 248\n",
      "    Batch 232 / 248\n",
      "    Batch 233 / 248\n",
      "    Batch 234 / 248\n",
      "    Batch 235 / 248\n",
      "    Batch 236 / 248\n",
      "    Batch 237 / 248\n",
      "    Batch 238 / 248\n",
      "    Batch 239 / 248\n",
      "    Batch 240 / 248\n",
      "    Batch 241 / 248\n",
      "    Batch 242 / 248\n",
      "    Batch 243 / 248\n",
      "    Batch 244 / 248\n",
      "    Batch 245 / 248\n",
      "    Batch 246 / 248\n",
      "    Batch 247 / 248\n",
      "    Batch 248 / 248\n",
      "threshold: 0.45535165071487427, accuracy: 0.9009350518069245\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.90      0.90      3957\n",
      "         1.0       0.90      0.90      0.90      3957\n",
      "\n",
      "    accuracy                           0.90      7914\n",
      "   macro avg       0.90      0.90      0.90      7914\n",
      "weighted avg       0.90      0.90      0.90      7914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx1 = np.where(tpr <= tnr)[0]\n",
    "idx2 = np.where(tpr >= tnr)[0]\n",
    "# t = 0.5 * (thresholds[idx1[-1]] + thresholds[idx2[-1]])\n",
    "# t = thresholds[idx2[-1]]\n",
    "t = thresholds[idx1[-1]]\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "with torch.no_grad():\n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        edges, features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        all_pairs = torch.mm(out, out.t())\n",
    "        scores = all_pairs[edges.T]\n",
    "        predictions = (scores >= t).long()\n",
    "        y_true.extend(labels.detach().numpy())\n",
    "        y_pred.extend(predictions.detach().numpy())\n",
    "        total_correct += torch.sum(predictions == labels.long()).item()\n",
    "        total_examples += len(labels) \n",
    "        print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "print('threshold: {}, accuracy: {}'.format(t, total_correct / total_examples))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading CollegeMsg dataset from /Users/raunak/Documents/Datasets/temporal-networks-snap/CollegeMsg.txt\n",
      "Finished reading data.\n",
      "Setting up data structures.\n",
      "Finished setting up data structures.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Testing.\n",
      "    Batch 3 / 248: loss 0.4930980900923411, accuracy 0.9166666666666666\n",
      "    ROC-AUC score: 0.9444444444444444\n",
      "    Batch 6 / 248: loss 0.44799428184827167, accuracy 0.90625\n",
      "    ROC-AUC score: 0.9529411764705882\n",
      "    Batch 9 / 248: loss 0.4564826488494873, accuracy 0.875\n",
      "    ROC-AUC score: 0.9285714285714286\n",
      "    Batch 12 / 248: loss 0.501114030679067, accuracy 0.90625\n",
      "    ROC-AUC score: 0.9801587301587302\n",
      "    Batch 15 / 248: loss 0.47460023562113446, accuracy 0.8541666666666666\n",
      "    ROC-AUC score: 0.9490196078431372\n",
      "    Batch 18 / 248: loss 0.487228920062383, accuracy 0.90625\n",
      "    ROC-AUC score: 1.0\n",
      "    Batch 21 / 248: loss 0.5528845191001892, accuracy 0.8541666666666666\n",
      "    ROC-AUC score: 0.9019607843137255\n",
      "    Batch 24 / 248: loss 0.44714035590489704, accuracy 0.9166666666666666\n",
      "    ROC-AUC score: 0.9874999999999999\n",
      "    Batch 27 / 248: loss 0.48846713701883954, accuracy 0.875\n",
      "    ROC-AUC score: 0.828125\n",
      "    Batch 30 / 248: loss 0.5131466388702393, accuracy 0.875\n",
      "    ROC-AUC score: 0.8196078431372549\n",
      "    Batch 33 / 248: loss 0.4660392105579376, accuracy 0.90625\n",
      "    ROC-AUC score: 0.9765625\n",
      "    Batch 36 / 248: loss 0.46055908997853595, accuracy 0.875\n",
      "    ROC-AUC score: 0.9125000000000001\n",
      "    Batch 39 / 248: loss 0.5004941622416178, accuracy 0.875\n",
      "    ROC-AUC score: 0.9682539682539683\n",
      "    Batch 42 / 248: loss 0.4894547164440155, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.9838056680161944\n",
      "    Batch 45 / 248: loss 0.4584743877251943, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.9764705882352941\n",
      "    Batch 48 / 248: loss 0.44884313146273297, accuracy 0.9375\n",
      "    ROC-AUC score: 1.0\n",
      "    Batch 51 / 248: loss 0.5446491638819376, accuracy 0.84375\n",
      "    ROC-AUC score: 0.8744939271255061\n",
      "    Batch 54 / 248: loss 0.4826845626036326, accuracy 0.875\n",
      "    ROC-AUC score: 0.996078431372549\n",
      "    Batch 57 / 248: loss 0.4755403200785319, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.996078431372549\n",
      "    Batch 60 / 248: loss 0.47613751888275146, accuracy 0.8541666666666666\n",
      "    ROC-AUC score: 0.83984375\n",
      "    Batch 63 / 248: loss 0.5277803540229797, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.8458333333333333\n",
      "    Batch 66 / 248: loss 0.44638245304425556, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.9529411764705882\n",
      "    Batch 69 / 248: loss 0.469325711329778, accuracy 0.8854166666666666\n",
      "    ROC-AUC score: 0.9603174603174602\n",
      "    Batch 72 / 248: loss 0.4284358620643616, accuracy 0.9166666666666666\n",
      "    ROC-AUC score: 0.9590909090909091\n",
      "    Batch 75 / 248: loss 0.444337397813797, accuracy 0.9375\n",
      "    ROC-AUC score: 0.9833333333333334\n",
      "    Batch 78 / 248: loss 0.5102777679761251, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.9411764705882354\n",
      "    Batch 81 / 248: loss 0.43591614564259845, accuracy 0.9583333333333334\n",
      "    ROC-AUC score: 0.9523809523809523\n",
      "    Batch 84 / 248: loss 0.47584614157676697, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.9490196078431373\n",
      "    Batch 87 / 248: loss 0.5315490563710531, accuracy 0.9166666666666666\n",
      "    ROC-AUC score: 0.9134199134199135\n",
      "    Batch 90 / 248: loss 0.4313811957836151, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.876984126984127\n",
      "    Batch 93 / 248: loss 0.49865251779556274, accuracy 0.8854166666666666\n",
      "    ROC-AUC score: 0.9285714285714286\n",
      "    Batch 96 / 248: loss 0.49250562985738117, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.9523809523809523\n",
      "    Batch 99 / 248: loss 0.5380743145942688, accuracy 0.8645833333333334\n",
      "    ROC-AUC score: 0.8987854251012147\n",
      "    Batch 102 / 248: loss 0.43461180726687115, accuracy 0.9479166666666666\n",
      "    ROC-AUC score: 0.96484375\n",
      "    Batch 105 / 248: loss 0.39454879363377887, accuracy 0.9479166666666666\n",
      "    ROC-AUC score: 0.9420289855072463\n",
      "    Batch 108 / 248: loss 0.5036520461241404, accuracy 0.875\n",
      "    ROC-AUC score: 0.9484126984126985\n",
      "    Batch 111 / 248: loss 0.4830438494682312, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.7692307692307694\n",
      "    Batch 114 / 248: loss 0.4491020341714223, accuracy 0.9583333333333334\n",
      "    ROC-AUC score: 0.9686274509803922\n",
      "    Batch 117 / 248: loss 0.5344547828038534, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.8666666666666667\n",
      "    Batch 120 / 248: loss 0.49532588322957355, accuracy 0.875\n",
      "    ROC-AUC score: 0.9372549019607843\n",
      "    Batch 123 / 248: loss 0.5494846304257711, accuracy 0.8541666666666666\n",
      "    ROC-AUC score: 0.8906882591093119\n",
      "    Batch 126 / 248: loss 0.4574173291524251, accuracy 0.875\n",
      "    ROC-AUC score: 0.8583333333333334\n",
      "    Batch 129 / 248: loss 0.4376285870869954, accuracy 0.96875\n",
      "    ROC-AUC score: 1.0\n",
      "    Batch 132 / 248: loss 0.435541828473409, accuracy 0.90625\n",
      "    ROC-AUC score: 0.876984126984127\n",
      "    Batch 135 / 248: loss 0.42895782987276715, accuracy 0.90625\n",
      "    ROC-AUC score: 0.9913419913419913\n",
      "    Batch 138 / 248: loss 0.4672152598698934, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 1.0\n",
      "    Batch 141 / 248: loss 0.46277793248494464, accuracy 0.8854166666666666\n",
      "    ROC-AUC score: 0.968627450980392\n",
      "    Batch 144 / 248: loss 0.4586317241191864, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.8958333333333333\n",
      "    Batch 147 / 248: loss 0.49490678310394287, accuracy 0.9375\n",
      "    ROC-AUC score: 0.9047619047619048\n",
      "    Batch 150 / 248: loss 0.49999772508939105, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.987012987012987\n",
      "    Batch 153 / 248: loss 0.4575626055399577, accuracy 0.8645833333333334\n",
      "    ROC-AUC score: 0.9307359307359306\n",
      "    Batch 156 / 248: loss 0.4631318151950836, accuracy 0.8645833333333334\n",
      "    ROC-AUC score: 0.923076923076923\n",
      "    Batch 159 / 248: loss 0.5109869241714478, accuracy 0.8645833333333334\n",
      "    ROC-AUC score: 0.9372549019607843\n",
      "    Batch 162 / 248: loss 0.49553027749061584, accuracy 0.9375\n",
      "    ROC-AUC score: 0.9545454545454546\n",
      "    Batch 165 / 248: loss 0.5047440926233927, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.8571428571428572\n",
      "    Batch 168 / 248: loss 0.47865496079126996, accuracy 0.8854166666666666\n",
      "    ROC-AUC score: 0.9554655870445344\n",
      "    Batch 171 / 248: loss 0.5176113347212473, accuracy 0.90625\n",
      "    ROC-AUC score: 0.9098039215686274\n",
      "    Batch 174 / 248: loss 0.47360138098398846, accuracy 0.8854166666666666\n",
      "    ROC-AUC score: 0.94921875\n",
      "    Batch 177 / 248: loss 0.5002287725607554, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.9325396825396826\n",
      "    Batch 180 / 248: loss 0.4205872019131978, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.9757085020242915\n",
      "    Batch 183 / 248: loss 0.49635828534762066, accuracy 0.8854166666666666\n",
      "    ROC-AUC score: 0.9682539682539683\n",
      "    Batch 186 / 248: loss 0.518328974644343, accuracy 0.875\n",
      "    ROC-AUC score: 0.98046875\n",
      "    Batch 189 / 248: loss 0.4676375687122345, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.9444444444444444\n",
      "    Batch 192 / 248: loss 0.4797489146391551, accuracy 0.9583333333333334\n",
      "    ROC-AUC score: 1.0\n",
      "    Batch 195 / 248: loss 0.47883176803588867, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.953125\n",
      "    Batch 198 / 248: loss 0.47716911633809406, accuracy 0.8854166666666666\n",
      "    ROC-AUC score: 0.9375\n",
      "    Batch 201 / 248: loss 0.4606049557526906, accuracy 0.90625\n",
      "    ROC-AUC score: 0.93359375\n",
      "    Batch 204 / 248: loss 0.5268696447213491, accuracy 0.8541666666666666\n",
      "    ROC-AUC score: 0.8745098039215686\n",
      "    Batch 207 / 248: loss 0.4854739308357239, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.9473684210526316\n",
      "    Batch 210 / 248: loss 0.4786638418833415, accuracy 0.90625\n",
      "    ROC-AUC score: 0.98828125\n",
      "    Batch 213 / 248: loss 0.43756623069445294, accuracy 0.9166666666666666\n",
      "    ROC-AUC score: 1.0\n",
      "    Batch 216 / 248: loss 0.4993365903695424, accuracy 0.9270833333333334\n",
      "    ROC-AUC score: 0.9271255060728746\n",
      "    Batch 219 / 248: loss 0.4823986887931824, accuracy 0.9375\n",
      "    ROC-AUC score: 0.9149797570850204\n",
      "    Batch 222 / 248: loss 0.4902712007363637, accuracy 0.8645833333333334\n",
      "    ROC-AUC score: 0.91796875\n",
      "    Batch 225 / 248: loss 0.4759734670321147, accuracy 0.9166666666666666\n",
      "    ROC-AUC score: 0.9045454545454544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 228 / 248: loss 0.5214638014634451, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.7125506072874495\n",
      "    Batch 231 / 248: loss 0.46113114555676776, accuracy 0.9583333333333334\n",
      "    ROC-AUC score: 0.9958333333333333\n",
      "    Batch 234 / 248: loss 0.5070223112901052, accuracy 0.8958333333333334\n",
      "    ROC-AUC score: 0.9880952380952381\n",
      "    Batch 237 / 248: loss 0.4411312937736511, accuracy 0.875\n",
      "    ROC-AUC score: 0.9624999999999999\n",
      "    Batch 240 / 248: loss 0.47854315241177875, accuracy 0.90625\n",
      "    ROC-AUC score: 0.9843137254901961\n",
      "    Batch 243 / 248: loss 0.505590945482254, accuracy 0.8854166666666666\n",
      "    ROC-AUC score: 0.9882352941176471\n",
      "    Batch 246 / 248: loss 0.4858866532643636, accuracy 0.9166666666666666\n",
      "    ROC-AUC score: 0.95\n",
      "Loss 0.4791728828943545, accuracy 0.9019459186252211\n",
      "ROC-AUC score: 0.9369781890220044\n",
      "Finished testing.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['neg_examples_path'], 'train',\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Testing.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores = [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "        print('    Batch {} / {}: loss {}, accuracy {}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        print('    ROC-AUC score: {}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {}, accuracy {}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {}'.format(area))\n",
    "print('Finished testing.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
