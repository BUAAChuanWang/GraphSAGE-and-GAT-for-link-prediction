{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import visdom\n",
    "\n",
    "from datasets import link_prediction\n",
    "from layers import MeanAggregator, LSTMAggregator, MaxPoolAggregator, MeanPoolAggregator\n",
    "import models\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up arguments for datasets, models and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"task\" : \"link_prediction\",\n",
    "    \n",
    "    \"dataset\" : \"IARadoslawEmail\",\n",
    "    \"dataset_path\" : \"/Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-radoslaw-email/ia-radoslaw-email.edges\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"generate_neg_examples\" : True,\n",
    "    \n",
    "    \"duplicate_examples\" : False,\n",
    "    \"repeat_examples\" : True,\n",
    "    \n",
    "    \"self_loop\" : False,\n",
    "    \"normalize_adj\" : False,\n",
    "    \n",
    "    \"cuda\" : \"True\",\n",
    "    \"model\" : \"GraphSAGE\",\n",
    "    \"agg_class\" : \"MaxPoolAggregator\",\n",
    "    \"hidden_dims\" : [64],\n",
    "    \"dropout\" : 0.5,\n",
    "    \"num_samples\" : -1,\n",
    "    \n",
    "    \"epochs\" : 3,\n",
    "    \"batch_size\" : 32,\n",
    "    \"lr\" : 1e-4,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"visdom\" : True,\n",
    "    \n",
    "    \"load\" : False,\n",
    "    \"save\" : False\n",
    "}\n",
    "config = args\n",
    "config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "\n",
    "if config['cuda'] and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "config['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset, dataloader and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-radoslaw-email/ia-radoslaw-email.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: train\n",
      "Number of vertices: 167\n",
      "Number of static edges: 3935\n",
      "Number of temporal edges: 24878\n",
      "Number of examples/datapoints: 5844\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'train',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "input_dim, output_dim = dataset.get_dims()\n",
    "config['input_dim'], config['output_dim'] = input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE(\n",
      "  (aggregators): ModuleList(\n",
      "    (0): MaxPoolAggregator(\n",
      "      (fc1): Linear(in_features=167, out_features=167, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): MaxPoolAggregator(\n",
      "      (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=334, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = utils.get_model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score for the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset before training.\n",
      "    Batch 1 / 183\n",
      "    Batch 2 / 183\n",
      "    Batch 3 / 183\n",
      "    Batch 4 / 183\n",
      "    Batch 5 / 183\n",
      "    Batch 6 / 183\n",
      "    Batch 7 / 183\n",
      "    Batch 8 / 183\n",
      "    Batch 9 / 183\n",
      "    Batch 10 / 183\n",
      "    Batch 11 / 183\n",
      "    Batch 12 / 183\n",
      "    Batch 13 / 183\n",
      "    Batch 14 / 183\n",
      "    Batch 15 / 183\n",
      "    Batch 16 / 183\n",
      "    Batch 17 / 183\n",
      "    Batch 18 / 183\n",
      "    Batch 19 / 183\n",
      "    Batch 20 / 183\n",
      "    Batch 21 / 183\n",
      "    Batch 22 / 183\n",
      "    Batch 23 / 183\n",
      "    Batch 24 / 183\n",
      "    Batch 25 / 183\n",
      "    Batch 26 / 183\n",
      "    Batch 27 / 183\n",
      "    Batch 28 / 183\n",
      "    Batch 29 / 183\n",
      "    Batch 30 / 183\n",
      "    Batch 31 / 183\n",
      "    Batch 32 / 183\n",
      "    Batch 33 / 183\n",
      "    Batch 34 / 183\n",
      "    Batch 35 / 183\n",
      "    Batch 36 / 183\n",
      "    Batch 37 / 183\n",
      "    Batch 38 / 183\n",
      "    Batch 39 / 183\n",
      "    Batch 40 / 183\n",
      "    Batch 41 / 183\n",
      "    Batch 42 / 183\n",
      "    Batch 43 / 183\n",
      "    Batch 44 / 183\n",
      "    Batch 45 / 183\n",
      "    Batch 46 / 183\n",
      "    Batch 47 / 183\n",
      "    Batch 48 / 183\n",
      "    Batch 49 / 183\n",
      "    Batch 50 / 183\n",
      "    Batch 51 / 183\n",
      "    Batch 52 / 183\n",
      "    Batch 53 / 183\n",
      "    Batch 54 / 183\n",
      "    Batch 55 / 183\n",
      "    Batch 56 / 183\n",
      "    Batch 57 / 183\n",
      "    Batch 58 / 183\n",
      "    Batch 59 / 183\n",
      "    Batch 60 / 183\n",
      "    Batch 61 / 183\n",
      "    Batch 62 / 183\n",
      "    Batch 63 / 183\n",
      "    Batch 64 / 183\n",
      "    Batch 65 / 183\n",
      "    Batch 66 / 183\n",
      "    Batch 67 / 183\n",
      "    Batch 68 / 183\n",
      "    Batch 69 / 183\n",
      "    Batch 70 / 183\n",
      "    Batch 71 / 183\n",
      "    Batch 72 / 183\n",
      "    Batch 73 / 183\n",
      "    Batch 74 / 183\n",
      "    Batch 75 / 183\n",
      "    Batch 76 / 183\n",
      "    Batch 77 / 183\n",
      "    Batch 78 / 183\n",
      "    Batch 79 / 183\n",
      "    Batch 80 / 183\n",
      "    Batch 81 / 183\n",
      "    Batch 82 / 183\n",
      "    Batch 83 / 183\n",
      "    Batch 84 / 183\n",
      "    Batch 85 / 183\n",
      "    Batch 86 / 183\n",
      "    Batch 87 / 183\n",
      "    Batch 88 / 183\n",
      "    Batch 89 / 183\n",
      "    Batch 90 / 183\n",
      "    Batch 91 / 183\n",
      "    Batch 92 / 183\n",
      "    Batch 93 / 183\n",
      "    Batch 94 / 183\n",
      "    Batch 95 / 183\n",
      "    Batch 96 / 183\n",
      "    Batch 97 / 183\n",
      "    Batch 98 / 183\n",
      "    Batch 99 / 183\n",
      "    Batch 100 / 183\n",
      "    Batch 101 / 183\n",
      "    Batch 102 / 183\n",
      "    Batch 103 / 183\n",
      "    Batch 104 / 183\n",
      "    Batch 105 / 183\n",
      "    Batch 106 / 183\n",
      "    Batch 107 / 183\n",
      "    Batch 108 / 183\n",
      "    Batch 109 / 183\n",
      "    Batch 110 / 183\n",
      "    Batch 111 / 183\n",
      "    Batch 112 / 183\n",
      "    Batch 113 / 183\n",
      "    Batch 114 / 183\n",
      "    Batch 115 / 183\n",
      "    Batch 116 / 183\n",
      "    Batch 117 / 183\n",
      "    Batch 118 / 183\n",
      "    Batch 119 / 183\n",
      "    Batch 120 / 183\n",
      "    Batch 121 / 183\n",
      "    Batch 122 / 183\n",
      "    Batch 123 / 183\n",
      "    Batch 124 / 183\n",
      "    Batch 125 / 183\n",
      "    Batch 126 / 183\n",
      "    Batch 127 / 183\n",
      "    Batch 128 / 183\n",
      "    Batch 129 / 183\n",
      "    Batch 130 / 183\n",
      "    Batch 131 / 183\n",
      "    Batch 132 / 183\n",
      "    Batch 133 / 183\n",
      "    Batch 134 / 183\n",
      "    Batch 135 / 183\n",
      "    Batch 136 / 183\n",
      "    Batch 137 / 183\n",
      "    Batch 138 / 183\n",
      "    Batch 139 / 183\n",
      "    Batch 140 / 183\n",
      "    Batch 141 / 183\n",
      "    Batch 142 / 183\n",
      "    Batch 143 / 183\n",
      "    Batch 144 / 183\n",
      "    Batch 145 / 183\n",
      "    Batch 146 / 183\n",
      "    Batch 147 / 183\n",
      "    Batch 148 / 183\n",
      "    Batch 149 / 183\n",
      "    Batch 150 / 183\n",
      "    Batch 151 / 183\n",
      "    Batch 152 / 183\n",
      "    Batch 153 / 183\n",
      "    Batch 154 / 183\n",
      "    Batch 155 / 183\n",
      "    Batch 156 / 183\n",
      "    Batch 157 / 183\n",
      "    Batch 158 / 183\n",
      "    Batch 159 / 183\n",
      "    Batch 160 / 183\n",
      "    Batch 161 / 183\n",
      "    Batch 162 / 183\n",
      "    Batch 163 / 183\n",
      "    Batch 164 / 183\n",
      "    Batch 165 / 183\n",
      "    Batch 166 / 183\n",
      "    Batch 167 / 183\n",
      "    Batch 168 / 183\n",
      "    Batch 169 / 183\n",
      "    Batch 170 / 183\n",
      "    Batch 171 / 183\n",
      "    Batch 172 / 183\n",
      "    Batch 173 / 183\n",
      "    Batch 174 / 183\n",
      "    Batch 175 / 183\n",
      "    Batch 176 / 183\n",
      "    Batch 177 / 183\n",
      "    Batch 178 / 183\n",
      "    Batch 179 / 183\n",
      "    Batch 180 / 183\n",
      "    Batch 181 / 183\n",
      "    Batch 182 / 183\n",
      "    Batch 183 / 183\n",
      "ROC-AUC score: 0.5019\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset before training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Training.\n",
      "Epoch 1 / 3\n",
      "    Batch 3 / 183: loss 0.6930\n",
      "    ROC-AUC score: 0.6310\n",
      "    Batch 6 / 183: loss 0.6932\n",
      "    ROC-AUC score: 0.4792\n",
      "    Batch 9 / 183: loss 0.6922\n",
      "    ROC-AUC score: 0.7063\n",
      "    Batch 12 / 183: loss 0.6915\n",
      "    ROC-AUC score: 0.6902\n",
      "    Batch 15 / 183: loss 0.6910\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 18 / 183: loss 0.6914\n",
      "    ROC-AUC score: 0.6194\n",
      "    Batch 21 / 183: loss 0.6904\n",
      "    ROC-AUC score: 0.7216\n",
      "    Batch 24 / 183: loss 0.6887\n",
      "    ROC-AUC score: 0.6542\n",
      "    Batch 27 / 183: loss 0.6907\n",
      "    ROC-AUC score: 0.5961\n",
      "    Batch 30 / 183: loss 0.6875\n",
      "    ROC-AUC score: 0.7965\n",
      "    Batch 33 / 183: loss 0.6881\n",
      "    ROC-AUC score: 0.6680\n",
      "    Batch 36 / 183: loss 0.6881\n",
      "    ROC-AUC score: 0.6562\n",
      "    Batch 39 / 183: loss 0.6868\n",
      "    ROC-AUC score: 0.5569\n",
      "    Batch 42 / 183: loss 0.6859\n",
      "    ROC-AUC score: 0.7373\n",
      "    Batch 45 / 183: loss 0.6847\n",
      "    ROC-AUC score: 0.8314\n",
      "    Batch 48 / 183: loss 0.6823\n",
      "    ROC-AUC score: 0.7817\n",
      "    Batch 51 / 183: loss 0.6800\n",
      "    ROC-AUC score: 0.7412\n",
      "    Batch 54 / 183: loss 0.6834\n",
      "    ROC-AUC score: 0.7625\n",
      "    Batch 57 / 183: loss 0.6848\n",
      "    ROC-AUC score: 0.6471\n",
      "    Batch 60 / 183: loss 0.6807\n",
      "    ROC-AUC score: 0.8009\n",
      "    Batch 63 / 183: loss 0.6738\n",
      "    ROC-AUC score: 0.8398\n",
      "    Batch 66 / 183: loss 0.6759\n",
      "    ROC-AUC score: 0.8213\n",
      "    Batch 69 / 183: loss 0.6837\n",
      "    ROC-AUC score: 0.7812\n",
      "    Batch 72 / 183: loss 0.6826\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 75 / 183: loss 0.6826\n",
      "    ROC-AUC score: 0.8227\n",
      "    Batch 78 / 183: loss 0.6726\n",
      "    ROC-AUC score: 0.9219\n",
      "    Batch 81 / 183: loss 0.6636\n",
      "    ROC-AUC score: 0.9023\n",
      "    Batch 84 / 183: loss 0.6638\n",
      "    ROC-AUC score: 0.9125\n",
      "    Batch 87 / 183: loss 0.6633\n",
      "    ROC-AUC score: 0.9725\n",
      "    Batch 90 / 183: loss 0.6654\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 93 / 183: loss 0.6684\n",
      "    ROC-AUC score: 0.7695\n",
      "    Batch 96 / 183: loss 0.6641\n",
      "    ROC-AUC score: 0.9375\n",
      "    Batch 99 / 183: loss 0.6760\n",
      "    ROC-AUC score: 0.8623\n",
      "    Batch 102 / 183: loss 0.6521\n",
      "    ROC-AUC score: 0.8549\n",
      "    Batch 105 / 183: loss 0.6653\n",
      "    ROC-AUC score: 0.8138\n",
      "    Batch 108 / 183: loss 0.6583\n",
      "    ROC-AUC score: 0.8730\n",
      "    Batch 111 / 183: loss 0.6729\n",
      "    ROC-AUC score: 0.8364\n",
      "    Batch 114 / 183: loss 0.6562\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 117 / 183: loss 0.6541\n",
      "    ROC-AUC score: 0.8658\n",
      "    Batch 120 / 183: loss 0.6714\n",
      "    ROC-AUC score: 0.8664\n",
      "    Batch 123 / 183: loss 0.6749\n",
      "    ROC-AUC score: 0.7725\n",
      "    Batch 126 / 183: loss 0.6475\n",
      "    ROC-AUC score: 0.9083\n",
      "    Batch 129 / 183: loss 0.6453\n",
      "    ROC-AUC score: 0.8594\n",
      "    Batch 132 / 183: loss 0.6360\n",
      "    ROC-AUC score: 0.9412\n",
      "    Batch 135 / 183: loss 0.6608\n",
      "    ROC-AUC score: 0.7804\n",
      "    Batch 138 / 183: loss 0.6700\n",
      "    ROC-AUC score: 0.8543\n",
      "    Batch 141 / 183: loss 0.6470\n",
      "    ROC-AUC score: 0.8542\n",
      "    Batch 144 / 183: loss 0.6324\n",
      "    ROC-AUC score: 0.9683\n",
      "    Batch 147 / 183: loss 0.6516\n",
      "    ROC-AUC score: 0.8810\n",
      "    Batch 150 / 183: loss 0.6301\n",
      "    ROC-AUC score: 0.7583\n",
      "    Batch 153 / 183: loss 0.6274\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 156 / 183: loss 0.6605\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 159 / 183: loss 0.6561\n",
      "    ROC-AUC score: 0.8242\n",
      "    Batch 162 / 183: loss 0.6616\n",
      "    ROC-AUC score: 0.8555\n",
      "    Batch 165 / 183: loss 0.6438\n",
      "    ROC-AUC score: 0.8984\n",
      "    Batch 168 / 183: loss 0.6700\n",
      "    ROC-AUC score: 0.9451\n",
      "    Batch 171 / 183: loss 0.6395\n",
      "    ROC-AUC score: 0.8175\n",
      "    Batch 174 / 183: loss 0.6229\n",
      "    ROC-AUC score: 0.8208\n",
      "    Batch 177 / 183: loss 0.6176\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 180 / 183: loss 0.6569\n",
      "    ROC-AUC score: 0.8672\n",
      "    Batch 183 / 183: loss 0.6443\n",
      "    ROC-AUC score: 1.0000\n",
      "Epoch 2 / 3\n",
      "    Batch 3 / 183: loss 0.6087\n",
      "    ROC-AUC score: 0.8945\n",
      "    Batch 6 / 183: loss 0.6479\n",
      "    ROC-AUC score: 0.8833\n",
      "    Batch 9 / 183: loss 0.6239\n",
      "    ROC-AUC score: 0.9351\n",
      "    Batch 12 / 183: loss 0.6385\n",
      "    ROC-AUC score: 0.8792\n",
      "    Batch 15 / 183: loss 0.6501\n",
      "    ROC-AUC score: 0.8375\n",
      "    Batch 18 / 183: loss 0.6601\n",
      "    ROC-AUC score: 0.7750\n",
      "    Batch 21 / 183: loss 0.5926\n",
      "    ROC-AUC score: 0.7542\n",
      "    Batch 24 / 183: loss 0.6332\n",
      "    ROC-AUC score: 0.9444\n",
      "    Batch 27 / 183: loss 0.6058\n",
      "    ROC-AUC score: 0.9275\n",
      "    Batch 30 / 183: loss 0.6496\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 33 / 183: loss 0.6666\n",
      "    ROC-AUC score: 0.9028\n",
      "    Batch 36 / 183: loss 0.6478\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 39 / 183: loss 0.6099\n",
      "    ROC-AUC score: 0.9294\n",
      "    Batch 42 / 183: loss 0.5956\n",
      "    ROC-AUC score: 0.9625\n",
      "    Batch 45 / 183: loss 0.6111\n",
      "    ROC-AUC score: 0.9393\n",
      "    Batch 48 / 183: loss 0.6308\n",
      "    ROC-AUC score: 0.8492\n",
      "    Batch 51 / 183: loss 0.6355\n",
      "    ROC-AUC score: 0.7778\n",
      "    Batch 54 / 183: loss 0.6582\n",
      "    ROC-AUC score: 0.7216\n",
      "    Batch 57 / 183: loss 0.5968\n",
      "    ROC-AUC score: 0.9083\n",
      "    Batch 60 / 183: loss 0.6030\n",
      "    ROC-AUC score: 0.9048\n",
      "    Batch 63 / 183: loss 0.6336\n",
      "    ROC-AUC score: 0.8417\n",
      "    Batch 66 / 183: loss 0.6434\n",
      "    ROC-AUC score: 0.8788\n",
      "    Batch 69 / 183: loss 0.6291\n",
      "    ROC-AUC score: 0.6964\n",
      "    Batch 72 / 183: loss 0.6238\n",
      "    ROC-AUC score: 0.9563\n",
      "    Batch 75 / 183: loss 0.6188\n",
      "    ROC-AUC score: 0.8314\n",
      "    Batch 78 / 183: loss 0.6226\n",
      "    ROC-AUC score: 0.9325\n",
      "    Batch 81 / 183: loss 0.5939\n",
      "    ROC-AUC score: 0.8730\n",
      "    Batch 84 / 183: loss 0.5814\n",
      "    ROC-AUC score: 0.9453\n",
      "    Batch 87 / 183: loss 0.6019\n",
      "    ROC-AUC score: 0.9725\n",
      "    Batch 90 / 183: loss 0.7081\n",
      "    ROC-AUC score: 0.8458\n",
      "    Batch 93 / 183: loss 0.5576\n",
      "    ROC-AUC score: 0.9524\n",
      "    Batch 96 / 183: loss 0.6096\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 99 / 183: loss 0.5963\n",
      "    ROC-AUC score: 0.8532\n",
      "    Batch 102 / 183: loss 0.6205\n",
      "    ROC-AUC score: 0.9409\n",
      "    Batch 105 / 183: loss 0.6116\n",
      "    ROC-AUC score: 0.8866\n",
      "    Batch 108 / 183: loss 0.5963\n",
      "    ROC-AUC score: 0.8889\n",
      "    Batch 111 / 183: loss 0.6125\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 114 / 183: loss 0.6324\n",
      "    ROC-AUC score: 0.8042\n",
      "    Batch 117 / 183: loss 0.6498\n",
      "    ROC-AUC score: 0.8281\n",
      "    Batch 120 / 183: loss 0.6266\n",
      "    ROC-AUC score: 0.9255\n",
      "    Batch 123 / 183: loss 0.6423\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 126 / 183: loss 0.6206\n",
      "    ROC-AUC score: 0.8706\n",
      "    Batch 129 / 183: loss 0.5658\n",
      "    ROC-AUC score: 0.9766\n",
      "    Batch 132 / 183: loss 0.6030\n",
      "    ROC-AUC score: 0.9098\n",
      "    Batch 135 / 183: loss 0.6159\n",
      "    ROC-AUC score: 0.8167\n",
      "    Batch 138 / 183: loss 0.6505\n",
      "    ROC-AUC score: 0.9176\n",
      "    Batch 141 / 183: loss 0.6376\n",
      "    ROC-AUC score: 0.7897\n",
      "    Batch 144 / 183: loss 0.6005\n",
      "    ROC-AUC score: 0.8182\n",
      "    Batch 147 / 183: loss 0.6107\n",
      "    ROC-AUC score: 0.9555\n",
      "    Batch 150 / 183: loss 0.5648\n",
      "    ROC-AUC score: 0.9569\n",
      "    Batch 153 / 183: loss 0.5604\n",
      "    ROC-AUC score: 0.9177\n",
      "    Batch 156 / 183: loss 0.6125\n",
      "    ROC-AUC score: 0.7381\n",
      "    Batch 159 / 183: loss 0.5922\n",
      "    ROC-AUC score: 0.9688\n",
      "    Batch 162 / 183: loss 0.5993\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 165 / 183: loss 0.6289\n",
      "    ROC-AUC score: 0.9352\n",
      "    Batch 168 / 183: loss 0.5964\n",
      "    ROC-AUC score: 0.9255\n",
      "    Batch 171 / 183: loss 0.6105\n",
      "    ROC-AUC score: 0.8398\n",
      "    Batch 174 / 183: loss 0.5809\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 177 / 183: loss 0.6180\n",
      "    ROC-AUC score: 0.9208\n",
      "    Batch 180 / 183: loss 0.6247\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 183 / 183: loss 0.5825\n",
      "    ROC-AUC score: 0.7692\n",
      "Epoch 3 / 3\n",
      "    Batch 3 / 183: loss 0.6316\n",
      "    ROC-AUC score: 0.8958\n",
      "    Batch 6 / 183: loss 0.6059\n",
      "    ROC-AUC score: 0.9023\n",
      "    Batch 9 / 183: loss 0.5993\n",
      "    ROC-AUC score: 0.8958\n",
      "    Batch 12 / 183: loss 0.6297\n",
      "    ROC-AUC score: 0.8542\n",
      "    Batch 15 / 183: loss 0.6413\n",
      "    ROC-AUC score: 0.9394\n",
      "    Batch 18 / 183: loss 0.5736\n",
      "    ROC-AUC score: 0.9414\n",
      "    Batch 21 / 183: loss 0.5962\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 24 / 183: loss 0.6036\n",
      "    ROC-AUC score: 0.9312\n",
      "    Batch 27 / 183: loss 0.5835\n",
      "    ROC-AUC score: 0.8672\n",
      "    Batch 30 / 183: loss 0.6046\n",
      "    ROC-AUC score: 0.7540\n",
      "    Batch 33 / 183: loss 0.5742\n",
      "    ROC-AUC score: 0.8690\n",
      "    Batch 36 / 183: loss 0.5723\n",
      "    ROC-AUC score: 0.9688\n",
      "    Batch 39 / 183: loss 0.5359\n",
      "    ROC-AUC score: 0.9167\n",
      "    Batch 42 / 183: loss 0.5986\n",
      "    ROC-AUC score: 0.8359\n",
      "    Batch 45 / 183: loss 0.5790\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 48 / 183: loss 0.5300\n",
      "    ROC-AUC score: 0.8543\n",
      "    Batch 51 / 183: loss 0.5730\n",
      "    ROC-AUC score: 0.9373\n",
      "    Batch 54 / 183: loss 0.5966\n",
      "    ROC-AUC score: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 57 / 183: loss 0.5962\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 60 / 183: loss 0.5628\n",
      "    ROC-AUC score: 0.8810\n",
      "    Batch 63 / 183: loss 0.6038\n",
      "    ROC-AUC score: 0.8664\n",
      "    Batch 66 / 183: loss 0.6329\n",
      "    ROC-AUC score: 0.9881\n",
      "    Batch 69 / 183: loss 0.5600\n",
      "    ROC-AUC score: 0.7792\n",
      "    Batch 72 / 183: loss 0.6003\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 75 / 183: loss 0.6248\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 78 / 183: loss 0.6256\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 81 / 183: loss 0.6190\n",
      "    ROC-AUC score: 0.9352\n",
      "    Batch 84 / 183: loss 0.6163\n",
      "    ROC-AUC score: 0.8651\n",
      "    Batch 87 / 183: loss 0.6179\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 90 / 183: loss 0.6047\n",
      "    ROC-AUC score: 0.9484\n",
      "    Batch 93 / 183: loss 0.5535\n",
      "    ROC-AUC score: 0.9325\n",
      "    Batch 96 / 183: loss 0.6293\n",
      "    ROC-AUC score: 0.8500\n",
      "    Batch 99 / 183: loss 0.5819\n",
      "    ROC-AUC score: 0.8988\n",
      "    Batch 102 / 183: loss 0.5935\n",
      "    ROC-AUC score: 0.8203\n",
      "    Batch 105 / 183: loss 0.6169\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 108 / 183: loss 0.5497\n",
      "    ROC-AUC score: 0.8909\n",
      "    Batch 111 / 183: loss 0.5448\n",
      "    ROC-AUC score: 0.9206\n",
      "    Batch 114 / 183: loss 0.6152\n",
      "    ROC-AUC score: 0.8968\n",
      "    Batch 117 / 183: loss 0.5626\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 120 / 183: loss 0.5944\n",
      "    ROC-AUC score: 0.9451\n",
      "    Batch 123 / 183: loss 0.6260\n",
      "    ROC-AUC score: 0.8988\n",
      "    Batch 126 / 183: loss 0.5561\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 129 / 183: loss 0.5827\n",
      "    ROC-AUC score: 0.7917\n",
      "    Batch 132 / 183: loss 0.6061\n",
      "    ROC-AUC score: 0.9271\n",
      "    Batch 135 / 183: loss 0.5577\n",
      "    ROC-AUC score: 0.9325\n",
      "    Batch 138 / 183: loss 0.5488\n",
      "    ROC-AUC score: 0.9098\n",
      "    Batch 141 / 183: loss 0.5954\n",
      "    ROC-AUC score: 0.8704\n",
      "    Batch 144 / 183: loss 0.5485\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 147 / 183: loss 0.6207\n",
      "    ROC-AUC score: 0.9000\n",
      "    Batch 150 / 183: loss 0.5888\n",
      "    ROC-AUC score: 0.9490\n",
      "    Batch 153 / 183: loss 0.5184\n",
      "    ROC-AUC score: 0.9373\n",
      "    Batch 156 / 183: loss 0.5109\n",
      "    ROC-AUC score: 0.9802\n",
      "    Batch 159 / 183: loss 0.5636\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 162 / 183: loss 0.6282\n",
      "    ROC-AUC score: 0.9000\n",
      "    Batch 165 / 183: loss 0.6652\n",
      "    ROC-AUC score: 0.9318\n",
      "    Batch 168 / 183: loss 0.5928\n",
      "    ROC-AUC score: 0.8690\n",
      "    Batch 171 / 183: loss 0.5513\n",
      "    ROC-AUC score: 0.9444\n",
      "    Batch 174 / 183: loss 0.6313\n",
      "    ROC-AUC score: 0.7733\n",
      "    Batch 177 / 183: loss 0.5714\n",
      "    ROC-AUC score: 0.8941\n",
      "    Batch 180 / 183: loss 0.5540\n",
      "    ROC-AUC score: 0.9069\n",
      "    Batch 183 / 183: loss 0.5950\n",
      "    ROC-AUC score: 0.8438\n",
      "Finished training.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    use_visdom = config['visdom']\n",
    "    if use_visdom:\n",
    "        vis = visdom.Visdom()\n",
    "        loss_window = None\n",
    "    criterion = utils.get_criterion(config['task'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                           weight_decay=config['weight_decay'])\n",
    "    epochs = config['epochs']\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.8)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 500, 1000], gamma=0.7)\n",
    "    model.train()\n",
    "    print('--------------------------------')\n",
    "    print('Training.')\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "        running_loss = 0.0\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            loss = criterion(scores, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "            if (idx + 1) % stats_per_batch == 0:\n",
    "                running_loss /= stats_per_batch\n",
    "                print('    Batch {} / {}: loss {:.4f}'.format(\n",
    "                    idx+1, num_batches, running_loss))\n",
    "                if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "                    area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "                    print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "                running_loss = 0.0\n",
    "                num_correct, num_examples = 0, 0\n",
    "            if use_visdom:\n",
    "                if loss_window is None:\n",
    "                    loss_window = vis.line(\n",
    "                        Y=[loss.item()],\n",
    "                        X=[epoch*num_batches+idx],\n",
    "                        opts=dict(xlabel='batch', ylabel='Loss', title='Training Loss', legend=['Loss']))\n",
    "                else:\n",
    "                    vis.line(\n",
    "                        [loss.item()],\n",
    "                        [epoch*num_batches+idx],\n",
    "                        win=loss_window,\n",
    "                        update='append')\n",
    "            scheduler.step()\n",
    "    if use_visdom:\n",
    "        vis.close(win=loss_window)\n",
    "    print('Finished training.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['load']:\n",
    "    if config['save']:\n",
    "        print('--------------------------------')\n",
    "        directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                'trained_models')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        fname = utils.get_fname(config)\n",
    "        path = os.path.join(directory, fname)\n",
    "        print('Saving model at {}'.format(path))\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print('Finished saving model.')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset after training.\n",
      "    Batch 1 / 183\n",
      "    Batch 2 / 183\n",
      "    Batch 3 / 183\n",
      "    Batch 4 / 183\n",
      "    Batch 5 / 183\n",
      "    Batch 6 / 183\n",
      "    Batch 7 / 183\n",
      "    Batch 8 / 183\n",
      "    Batch 9 / 183\n",
      "    Batch 10 / 183\n",
      "    Batch 11 / 183\n",
      "    Batch 12 / 183\n",
      "    Batch 13 / 183\n",
      "    Batch 14 / 183\n",
      "    Batch 15 / 183\n",
      "    Batch 16 / 183\n",
      "    Batch 17 / 183\n",
      "    Batch 18 / 183\n",
      "    Batch 19 / 183\n",
      "    Batch 20 / 183\n",
      "    Batch 21 / 183\n",
      "    Batch 22 / 183\n",
      "    Batch 23 / 183\n",
      "    Batch 24 / 183\n",
      "    Batch 25 / 183\n",
      "    Batch 26 / 183\n",
      "    Batch 27 / 183\n",
      "    Batch 28 / 183\n",
      "    Batch 29 / 183\n",
      "    Batch 30 / 183\n",
      "    Batch 31 / 183\n",
      "    Batch 32 / 183\n",
      "    Batch 33 / 183\n",
      "    Batch 34 / 183\n",
      "    Batch 35 / 183\n",
      "    Batch 36 / 183\n",
      "    Batch 37 / 183\n",
      "    Batch 38 / 183\n",
      "    Batch 39 / 183\n",
      "    Batch 40 / 183\n",
      "    Batch 41 / 183\n",
      "    Batch 42 / 183\n",
      "    Batch 43 / 183\n",
      "    Batch 44 / 183\n",
      "    Batch 45 / 183\n",
      "    Batch 46 / 183\n",
      "    Batch 47 / 183\n",
      "    Batch 48 / 183\n",
      "    Batch 49 / 183\n",
      "    Batch 50 / 183\n",
      "    Batch 51 / 183\n",
      "    Batch 52 / 183\n",
      "    Batch 53 / 183\n",
      "    Batch 54 / 183\n",
      "    Batch 55 / 183\n",
      "    Batch 56 / 183\n",
      "    Batch 57 / 183\n",
      "    Batch 58 / 183\n",
      "    Batch 59 / 183\n",
      "    Batch 60 / 183\n",
      "    Batch 61 / 183\n",
      "    Batch 62 / 183\n",
      "    Batch 63 / 183\n",
      "    Batch 64 / 183\n",
      "    Batch 65 / 183\n",
      "    Batch 66 / 183\n",
      "    Batch 67 / 183\n",
      "    Batch 68 / 183\n",
      "    Batch 69 / 183\n",
      "    Batch 70 / 183\n",
      "    Batch 71 / 183\n",
      "    Batch 72 / 183\n",
      "    Batch 73 / 183\n",
      "    Batch 74 / 183\n",
      "    Batch 75 / 183\n",
      "    Batch 76 / 183\n",
      "    Batch 77 / 183\n",
      "    Batch 78 / 183\n",
      "    Batch 79 / 183\n",
      "    Batch 80 / 183\n",
      "    Batch 81 / 183\n",
      "    Batch 82 / 183\n",
      "    Batch 83 / 183\n",
      "    Batch 84 / 183\n",
      "    Batch 85 / 183\n",
      "    Batch 86 / 183\n",
      "    Batch 87 / 183\n",
      "    Batch 88 / 183\n",
      "    Batch 89 / 183\n",
      "    Batch 90 / 183\n",
      "    Batch 91 / 183\n",
      "    Batch 92 / 183\n",
      "    Batch 93 / 183\n",
      "    Batch 94 / 183\n",
      "    Batch 95 / 183\n",
      "    Batch 96 / 183\n",
      "    Batch 97 / 183\n",
      "    Batch 98 / 183\n",
      "    Batch 99 / 183\n",
      "    Batch 100 / 183\n",
      "    Batch 101 / 183\n",
      "    Batch 102 / 183\n",
      "    Batch 103 / 183\n",
      "    Batch 104 / 183\n",
      "    Batch 105 / 183\n",
      "    Batch 106 / 183\n",
      "    Batch 107 / 183\n",
      "    Batch 108 / 183\n",
      "    Batch 109 / 183\n",
      "    Batch 110 / 183\n",
      "    Batch 111 / 183\n",
      "    Batch 112 / 183\n",
      "    Batch 113 / 183\n",
      "    Batch 114 / 183\n",
      "    Batch 115 / 183\n",
      "    Batch 116 / 183\n",
      "    Batch 117 / 183\n",
      "    Batch 118 / 183\n",
      "    Batch 119 / 183\n",
      "    Batch 120 / 183\n",
      "    Batch 121 / 183\n",
      "    Batch 122 / 183\n",
      "    Batch 123 / 183\n",
      "    Batch 124 / 183\n",
      "    Batch 125 / 183\n",
      "    Batch 126 / 183\n",
      "    Batch 127 / 183\n",
      "    Batch 128 / 183\n",
      "    Batch 129 / 183\n",
      "    Batch 130 / 183\n",
      "    Batch 131 / 183\n",
      "    Batch 132 / 183\n",
      "    Batch 133 / 183\n",
      "    Batch 134 / 183\n",
      "    Batch 135 / 183\n",
      "    Batch 136 / 183\n",
      "    Batch 137 / 183\n",
      "    Batch 138 / 183\n",
      "    Batch 139 / 183\n",
      "    Batch 140 / 183\n",
      "    Batch 141 / 183\n",
      "    Batch 142 / 183\n",
      "    Batch 143 / 183\n",
      "    Batch 144 / 183\n",
      "    Batch 145 / 183\n",
      "    Batch 146 / 183\n",
      "    Batch 147 / 183\n",
      "    Batch 148 / 183\n",
      "    Batch 149 / 183\n",
      "    Batch 150 / 183\n",
      "    Batch 151 / 183\n",
      "    Batch 152 / 183\n",
      "    Batch 153 / 183\n",
      "    Batch 154 / 183\n",
      "    Batch 155 / 183\n",
      "    Batch 156 / 183\n",
      "    Batch 157 / 183\n",
      "    Batch 158 / 183\n",
      "    Batch 159 / 183\n",
      "    Batch 160 / 183\n",
      "    Batch 161 / 183\n",
      "    Batch 162 / 183\n",
      "    Batch 163 / 183\n",
      "    Batch 164 / 183\n",
      "    Batch 165 / 183\n",
      "    Batch 166 / 183\n",
      "    Batch 167 / 183\n",
      "    Batch 168 / 183\n",
      "    Batch 169 / 183\n",
      "    Batch 170 / 183\n",
      "    Batch 171 / 183\n",
      "    Batch 172 / 183\n",
      "    Batch 173 / 183\n",
      "    Batch 174 / 183\n",
      "    Batch 175 / 183\n",
      "    Batch 176 / 183\n",
      "    Batch 177 / 183\n",
      "    Batch 178 / 183\n",
      "    Batch 179 / 183\n",
      "    Batch 180 / 183\n",
      "    Batch 181 / 183\n",
      "    Batch 182 / 183\n",
      "    Batch 183 / 183\n",
      "ROC-AUC score: 0.8831\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset after training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the true positive rate and true negative rate vs threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    tpr, fpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    tnr = 1 - fpr\n",
    "    plt.plot(thresholds, tpr, label='tpr')\n",
    "    plt.plot(thresholds, tnr, label='tnr')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.title('TPR / TNR vs Threshold')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an appropriate threshold and generate classification report on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 1 / 183\n",
      "    Batch 2 / 183\n",
      "    Batch 3 / 183\n",
      "    Batch 4 / 183\n",
      "    Batch 5 / 183\n",
      "    Batch 6 / 183\n",
      "    Batch 7 / 183\n",
      "    Batch 8 / 183\n",
      "    Batch 9 / 183\n",
      "    Batch 10 / 183\n",
      "    Batch 11 / 183\n",
      "    Batch 12 / 183\n",
      "    Batch 13 / 183\n",
      "    Batch 14 / 183\n",
      "    Batch 15 / 183\n",
      "    Batch 16 / 183\n",
      "    Batch 17 / 183\n",
      "    Batch 18 / 183\n",
      "    Batch 19 / 183\n",
      "    Batch 20 / 183\n",
      "    Batch 21 / 183\n",
      "    Batch 22 / 183\n",
      "    Batch 23 / 183\n",
      "    Batch 24 / 183\n",
      "    Batch 25 / 183\n",
      "    Batch 26 / 183\n",
      "    Batch 27 / 183\n",
      "    Batch 28 / 183\n",
      "    Batch 29 / 183\n",
      "    Batch 30 / 183\n",
      "    Batch 31 / 183\n",
      "    Batch 32 / 183\n",
      "    Batch 33 / 183\n",
      "    Batch 34 / 183\n",
      "    Batch 35 / 183\n",
      "    Batch 36 / 183\n",
      "    Batch 37 / 183\n",
      "    Batch 38 / 183\n",
      "    Batch 39 / 183\n",
      "    Batch 40 / 183\n",
      "    Batch 41 / 183\n",
      "    Batch 42 / 183\n",
      "    Batch 43 / 183\n",
      "    Batch 44 / 183\n",
      "    Batch 45 / 183\n",
      "    Batch 46 / 183\n",
      "    Batch 47 / 183\n",
      "    Batch 48 / 183\n",
      "    Batch 49 / 183\n",
      "    Batch 50 / 183\n",
      "    Batch 51 / 183\n",
      "    Batch 52 / 183\n",
      "    Batch 53 / 183\n",
      "    Batch 54 / 183\n",
      "    Batch 55 / 183\n",
      "    Batch 56 / 183\n",
      "    Batch 57 / 183\n",
      "    Batch 58 / 183\n",
      "    Batch 59 / 183\n",
      "    Batch 60 / 183\n",
      "    Batch 61 / 183\n",
      "    Batch 62 / 183\n",
      "    Batch 63 / 183\n",
      "    Batch 64 / 183\n",
      "    Batch 65 / 183\n",
      "    Batch 66 / 183\n",
      "    Batch 67 / 183\n",
      "    Batch 68 / 183\n",
      "    Batch 69 / 183\n",
      "    Batch 70 / 183\n",
      "    Batch 71 / 183\n",
      "    Batch 72 / 183\n",
      "    Batch 73 / 183\n",
      "    Batch 74 / 183\n",
      "    Batch 75 / 183\n",
      "    Batch 76 / 183\n",
      "    Batch 77 / 183\n",
      "    Batch 78 / 183\n",
      "    Batch 79 / 183\n",
      "    Batch 80 / 183\n",
      "    Batch 81 / 183\n",
      "    Batch 82 / 183\n",
      "    Batch 83 / 183\n",
      "    Batch 84 / 183\n",
      "    Batch 85 / 183\n",
      "    Batch 86 / 183\n",
      "    Batch 87 / 183\n",
      "    Batch 88 / 183\n",
      "    Batch 89 / 183\n",
      "    Batch 90 / 183\n",
      "    Batch 91 / 183\n",
      "    Batch 92 / 183\n",
      "    Batch 93 / 183\n",
      "    Batch 94 / 183\n",
      "    Batch 95 / 183\n",
      "    Batch 96 / 183\n",
      "    Batch 97 / 183\n",
      "    Batch 98 / 183\n",
      "    Batch 99 / 183\n",
      "    Batch 100 / 183\n",
      "    Batch 101 / 183\n",
      "    Batch 102 / 183\n",
      "    Batch 103 / 183\n",
      "    Batch 104 / 183\n",
      "    Batch 105 / 183\n",
      "    Batch 106 / 183\n",
      "    Batch 107 / 183\n",
      "    Batch 108 / 183\n",
      "    Batch 109 / 183\n",
      "    Batch 110 / 183\n",
      "    Batch 111 / 183\n",
      "    Batch 112 / 183\n",
      "    Batch 113 / 183\n",
      "    Batch 114 / 183\n",
      "    Batch 115 / 183\n",
      "    Batch 116 / 183\n",
      "    Batch 117 / 183\n",
      "    Batch 118 / 183\n",
      "    Batch 119 / 183\n",
      "    Batch 120 / 183\n",
      "    Batch 121 / 183\n",
      "    Batch 122 / 183\n",
      "    Batch 123 / 183\n",
      "    Batch 124 / 183\n",
      "    Batch 125 / 183\n",
      "    Batch 126 / 183\n",
      "    Batch 127 / 183\n",
      "    Batch 128 / 183\n",
      "    Batch 129 / 183\n",
      "    Batch 130 / 183\n",
      "    Batch 131 / 183\n",
      "    Batch 132 / 183\n",
      "    Batch 133 / 183\n",
      "    Batch 134 / 183\n",
      "    Batch 135 / 183\n",
      "    Batch 136 / 183\n",
      "    Batch 137 / 183\n",
      "    Batch 138 / 183\n",
      "    Batch 139 / 183\n",
      "    Batch 140 / 183\n",
      "    Batch 141 / 183\n",
      "    Batch 142 / 183\n",
      "    Batch 143 / 183\n",
      "    Batch 144 / 183\n",
      "    Batch 145 / 183\n",
      "    Batch 146 / 183\n",
      "    Batch 147 / 183\n",
      "    Batch 148 / 183\n",
      "    Batch 149 / 183\n",
      "    Batch 150 / 183\n",
      "    Batch 151 / 183\n",
      "    Batch 152 / 183\n",
      "    Batch 153 / 183\n",
      "    Batch 154 / 183\n",
      "    Batch 155 / 183\n",
      "    Batch 156 / 183\n",
      "    Batch 157 / 183\n",
      "    Batch 158 / 183\n",
      "    Batch 159 / 183\n",
      "    Batch 160 / 183\n",
      "    Batch 161 / 183\n",
      "    Batch 162 / 183\n",
      "    Batch 163 / 183\n",
      "    Batch 164 / 183\n",
      "    Batch 165 / 183\n",
      "    Batch 166 / 183\n",
      "    Batch 167 / 183\n",
      "    Batch 168 / 183\n",
      "    Batch 169 / 183\n",
      "    Batch 170 / 183\n",
      "    Batch 171 / 183\n",
      "    Batch 172 / 183\n",
      "    Batch 173 / 183\n",
      "    Batch 174 / 183\n",
      "    Batch 175 / 183\n",
      "    Batch 176 / 183\n",
      "    Batch 177 / 183\n",
      "    Batch 178 / 183\n",
      "    Batch 179 / 183\n",
      "    Batch 180 / 183\n",
      "    Batch 181 / 183\n",
      "    Batch 182 / 183\n",
      "    Batch 183 / 183\n",
      "Threshold: 0.6634, accuracy: 0.8152\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.82      2922\n",
      "         1.0       0.82      0.81      0.82      2922\n",
      "\n",
      "    accuracy                           0.82      5844\n",
      "   macro avg       0.82      0.82      0.82      5844\n",
      "weighted avg       0.82      0.82      0.82      5844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx1 = np.where(tpr <= tnr)[0]\n",
    "idx2 = np.where(tpr >= tnr)[0]\n",
    "t = thresholds[idx1[-1]]\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "with torch.no_grad():\n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        edges, features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        all_pairs = torch.mm(out, out.t())\n",
    "        scores = all_pairs[edges.T]\n",
    "        predictions = (scores >= t).long()\n",
    "        y_true.extend(labels.detach().numpy())\n",
    "        y_pred.extend(predictions.detach().numpy())\n",
    "        total_correct += torch.sum(predictions == labels.long()).item()\n",
    "        total_examples += len(labels) \n",
    "        print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "print('Threshold: {:.4f}, accuracy: {:.4f}'.format(t, total_correct / total_examples))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "print('Classification report\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-radoslaw-email/ia-radoslaw-email.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: test\n",
      "Number of vertices: 167\n",
      "Number of static edges: 5358\n",
      "Number of temporal edges: 62195\n",
      "Number of examples/datapoints: 5504\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the test dataset after training.\n",
      "    Batch 3 / 172: loss 0.5441, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9150\n",
      "    Batch 6 / 172: loss 0.6333, accuracy 0.7708\n",
      "    ROC-AUC score: 0.7098\n",
      "    Batch 9 / 172: loss 0.6178, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 12 / 172: loss 0.5665, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9190\n",
      "    Batch 15 / 172: loss 0.5905, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 18 / 172: loss 0.5562, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8492\n",
      "    Batch 21 / 172: loss 0.6061, accuracy 0.8333\n",
      "    ROC-AUC score: 0.7897\n",
      "    Batch 24 / 172: loss 0.6172, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 27 / 172: loss 0.6615, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7302\n",
      "    Batch 30 / 172: loss 0.6142, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7733\n",
      "    Batch 33 / 172: loss 0.5661, accuracy 0.8854\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 36 / 172: loss 0.5460, accuracy 0.8438\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 39 / 172: loss 0.5815, accuracy 0.8021\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 42 / 172: loss 0.6015, accuracy 0.8229\n",
      "    ROC-AUC score: 0.9258\n",
      "    Batch 45 / 172: loss 0.6116, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8633\n",
      "    Batch 48 / 172: loss 0.5065, accuracy 0.8958\n",
      "    ROC-AUC score: 0.9325\n",
      "    Batch 51 / 172: loss 0.6174, accuracy 0.7708\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 54 / 172: loss 0.6275, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8320\n",
      "    Batch 57 / 172: loss 0.6643, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7659\n",
      "    Batch 60 / 172: loss 0.5677, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9292\n",
      "    Batch 63 / 172: loss 0.5758, accuracy 0.8021\n",
      "    ROC-AUC score: 0.9216\n",
      "    Batch 66 / 172: loss 0.5991, accuracy 0.8021\n",
      "    ROC-AUC score: 0.9474\n",
      "    Batch 69 / 172: loss 0.5455, accuracy 0.8333\n",
      "    ROC-AUC score: 0.8651\n",
      "    Batch 72 / 172: loss 0.5942, accuracy 0.8125\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 75 / 172: loss 0.6449, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8906\n",
      "    Batch 78 / 172: loss 0.6237, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 81 / 172: loss 0.5586, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9125\n",
      "    Batch 84 / 172: loss 0.6132, accuracy 0.8021\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 87 / 172: loss 0.5618, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9569\n",
      "    Batch 90 / 172: loss 0.6199, accuracy 0.7812\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 93 / 172: loss 0.6068, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9474\n",
      "    Batch 96 / 172: loss 0.5642, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9451\n",
      "    Batch 99 / 172: loss 0.5866, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8831\n",
      "    Batch 102 / 172: loss 0.5899, accuracy 0.8438\n",
      "    ROC-AUC score: 0.8941\n",
      "    Batch 105 / 172: loss 0.5844, accuracy 0.8021\n",
      "    ROC-AUC score: 0.9333\n",
      "    Batch 108 / 172: loss 0.5325, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9595\n",
      "    Batch 111 / 172: loss 0.6239, accuracy 0.7500\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 114 / 172: loss 0.5320, accuracy 0.8958\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 117 / 172: loss 0.5455, accuracy 0.8229\n",
      "    ROC-AUC score: 0.8375\n",
      "    Batch 120 / 172: loss 0.6013, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9150\n",
      "    Batch 123 / 172: loss 0.5744, accuracy 0.7917\n",
      "    ROC-AUC score: 0.7375\n",
      "    Batch 126 / 172: loss 0.5895, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 129 / 172: loss 0.5618, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9805\n",
      "    Batch 132 / 172: loss 0.5590, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8042\n",
      "    Batch 135 / 172: loss 0.5666, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9433\n",
      "    Batch 138 / 172: loss 0.6020, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 141 / 172: loss 0.6216, accuracy 0.8021\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 144 / 172: loss 0.5288, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9595\n",
      "    Batch 147 / 172: loss 0.5765, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8824\n",
      "    Batch 150 / 172: loss 0.5978, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8651\n",
      "    Batch 153 / 172: loss 0.6663, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8902\n",
      "    Batch 156 / 172: loss 0.6135, accuracy 0.8021\n",
      "    ROC-AUC score: 0.8549\n",
      "    Batch 159 / 172: loss 0.6357, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8849\n",
      "    Batch 162 / 172: loss 0.6429, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8792\n",
      "    Batch 165 / 172: loss 0.5692, accuracy 0.8125\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 168 / 172: loss 0.5731, accuracy 0.8750\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 171 / 172: loss 0.5643, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9453\n",
      "Loss 0.5905, accuracy 0.8083\n",
      "ROC-AUC score: 0.8798\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.72      0.79      2752\n",
      "         1.0       0.76      0.90      0.82      2752\n",
      "\n",
      "    accuracy                           0.81      5504\n",
      "   macro avg       0.82      0.81      0.81      5504\n",
      "weighted avg       0.82      0.81      0.81      5504\n",
      "\n",
      "Finished testing.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'test',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the test dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished testing.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
