{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import visdom\n",
    "\n",
    "from datasets import link_prediction\n",
    "from layers import MeanAggregator, LSTMAggregator, MaxPoolAggregator, MeanPoolAggregator\n",
    "import models\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up arguments for datasets, models and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"task\" : \"link_prediction\",\n",
    "    \n",
    "    \"dataset\" : \"IAContactsHypertext\",\n",
    "    \"dataset_path\" : \"/Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"generate_neg_examples\" : False,\n",
    "    \n",
    "    \"duplicate_examples\" : True,\n",
    "    \"repeat_examples\" : False,\n",
    "    \n",
    "    \"self_loop\" : True,\n",
    "    \"normalize_adj\" : False,\n",
    "    \n",
    "    \"cuda\" : \"True\",\n",
    "    \"model\" : \"GraphSAGE\",\n",
    "    \"agg_class\" : \"MeanAggregator\",\n",
    "    \"hidden_dims\" : [32],\n",
    "    \"dropout\" : 0,\n",
    "    \"num_samples\" : -1,\n",
    "    \n",
    "    \"epochs\" : 5,\n",
    "    \"batch_size\" : 32,\n",
    "    \"lr\" : 5e-4,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"visdom\" : True,\n",
    "    \n",
    "    \"load\" : False,\n",
    "    \"save\" : False\n",
    "}\n",
    "config = args\n",
    "config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "\n",
    "if config['cuda'] and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "config['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset, dataloader and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: train\n",
      "Number of vertices: 113\n",
      "Number of static edges: 1010\n",
      "Number of temporal edges: 6245\n",
      "Number of examples/datapoints: 7044\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'train',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "input_dim, output_dim = dataset.get_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE(\n",
      "  (aggregators): ModuleList(\n",
      "    (0): MeanAggregator()\n",
      "    (1): MeanAggregator()\n",
      "  )\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=226, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agg_class = utils.get_agg_class(config['agg_class'])\n",
    "model = models.GraphSAGE(input_dim, config['hidden_dims'],\n",
    "                         output_dim, config['dropout'],\n",
    "                         agg_class, config['num_samples'],\n",
    "                         config['device'])\n",
    "model.to(config['device'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score for the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset before training.\n",
      "    Batch 1 / 221\n",
      "    Batch 2 / 221\n",
      "    Batch 3 / 221\n",
      "    Batch 4 / 221\n",
      "    Batch 5 / 221\n",
      "    Batch 6 / 221\n",
      "    Batch 7 / 221\n",
      "    Batch 8 / 221\n",
      "    Batch 9 / 221\n",
      "    Batch 10 / 221\n",
      "    Batch 11 / 221\n",
      "    Batch 12 / 221\n",
      "    Batch 13 / 221\n",
      "    Batch 14 / 221\n",
      "    Batch 15 / 221\n",
      "    Batch 16 / 221\n",
      "    Batch 17 / 221\n",
      "    Batch 18 / 221\n",
      "    Batch 19 / 221\n",
      "    Batch 20 / 221\n",
      "    Batch 21 / 221\n",
      "    Batch 22 / 221\n",
      "    Batch 23 / 221\n",
      "    Batch 24 / 221\n",
      "    Batch 25 / 221\n",
      "    Batch 26 / 221\n",
      "    Batch 27 / 221\n",
      "    Batch 28 / 221\n",
      "    Batch 29 / 221\n",
      "    Batch 30 / 221\n",
      "    Batch 31 / 221\n",
      "    Batch 32 / 221\n",
      "    Batch 33 / 221\n",
      "    Batch 34 / 221\n",
      "    Batch 35 / 221\n",
      "    Batch 36 / 221\n",
      "    Batch 37 / 221\n",
      "    Batch 38 / 221\n",
      "    Batch 39 / 221\n",
      "    Batch 40 / 221\n",
      "    Batch 41 / 221\n",
      "    Batch 42 / 221\n",
      "    Batch 43 / 221\n",
      "    Batch 44 / 221\n",
      "    Batch 45 / 221\n",
      "    Batch 46 / 221\n",
      "    Batch 47 / 221\n",
      "    Batch 48 / 221\n",
      "    Batch 49 / 221\n",
      "    Batch 50 / 221\n",
      "    Batch 51 / 221\n",
      "    Batch 52 / 221\n",
      "    Batch 53 / 221\n",
      "    Batch 54 / 221\n",
      "    Batch 55 / 221\n",
      "    Batch 56 / 221\n",
      "    Batch 57 / 221\n",
      "    Batch 58 / 221\n",
      "    Batch 59 / 221\n",
      "    Batch 60 / 221\n",
      "    Batch 61 / 221\n",
      "    Batch 62 / 221\n",
      "    Batch 63 / 221\n",
      "    Batch 64 / 221\n",
      "    Batch 65 / 221\n",
      "    Batch 66 / 221\n",
      "    Batch 67 / 221\n",
      "    Batch 68 / 221\n",
      "    Batch 69 / 221\n",
      "    Batch 70 / 221\n",
      "    Batch 71 / 221\n",
      "    Batch 72 / 221\n",
      "    Batch 73 / 221\n",
      "    Batch 74 / 221\n",
      "    Batch 75 / 221\n",
      "    Batch 76 / 221\n",
      "    Batch 77 / 221\n",
      "    Batch 78 / 221\n",
      "    Batch 79 / 221\n",
      "    Batch 80 / 221\n",
      "    Batch 81 / 221\n",
      "    Batch 82 / 221\n",
      "    Batch 83 / 221\n",
      "    Batch 84 / 221\n",
      "    Batch 85 / 221\n",
      "    Batch 86 / 221\n",
      "    Batch 87 / 221\n",
      "    Batch 88 / 221\n",
      "    Batch 89 / 221\n",
      "    Batch 90 / 221\n",
      "    Batch 91 / 221\n",
      "    Batch 92 / 221\n",
      "    Batch 93 / 221\n",
      "    Batch 94 / 221\n",
      "    Batch 95 / 221\n",
      "    Batch 96 / 221\n",
      "    Batch 97 / 221\n",
      "    Batch 98 / 221\n",
      "    Batch 99 / 221\n",
      "    Batch 100 / 221\n",
      "    Batch 101 / 221\n",
      "    Batch 102 / 221\n",
      "    Batch 103 / 221\n",
      "    Batch 104 / 221\n",
      "    Batch 105 / 221\n",
      "    Batch 106 / 221\n",
      "    Batch 107 / 221\n",
      "    Batch 108 / 221\n",
      "    Batch 109 / 221\n",
      "    Batch 110 / 221\n",
      "    Batch 111 / 221\n",
      "    Batch 112 / 221\n",
      "    Batch 113 / 221\n",
      "    Batch 114 / 221\n",
      "    Batch 115 / 221\n",
      "    Batch 116 / 221\n",
      "    Batch 117 / 221\n",
      "    Batch 118 / 221\n",
      "    Batch 119 / 221\n",
      "    Batch 120 / 221\n",
      "    Batch 121 / 221\n",
      "    Batch 122 / 221\n",
      "    Batch 123 / 221\n",
      "    Batch 124 / 221\n",
      "    Batch 125 / 221\n",
      "    Batch 126 / 221\n",
      "    Batch 127 / 221\n",
      "    Batch 128 / 221\n",
      "    Batch 129 / 221\n",
      "    Batch 130 / 221\n",
      "    Batch 131 / 221\n",
      "    Batch 132 / 221\n",
      "    Batch 133 / 221\n",
      "    Batch 134 / 221\n",
      "    Batch 135 / 221\n",
      "    Batch 136 / 221\n",
      "    Batch 137 / 221\n",
      "    Batch 138 / 221\n",
      "    Batch 139 / 221\n",
      "    Batch 140 / 221\n",
      "    Batch 141 / 221\n",
      "    Batch 142 / 221\n",
      "    Batch 143 / 221\n",
      "    Batch 144 / 221\n",
      "    Batch 145 / 221\n",
      "    Batch 146 / 221\n",
      "    Batch 147 / 221\n",
      "    Batch 148 / 221\n",
      "    Batch 149 / 221\n",
      "    Batch 150 / 221\n",
      "    Batch 151 / 221\n",
      "    Batch 152 / 221\n",
      "    Batch 153 / 221\n",
      "    Batch 154 / 221\n",
      "    Batch 155 / 221\n",
      "    Batch 156 / 221\n",
      "    Batch 157 / 221\n",
      "    Batch 158 / 221\n",
      "    Batch 159 / 221\n",
      "    Batch 160 / 221\n",
      "    Batch 161 / 221\n",
      "    Batch 162 / 221\n",
      "    Batch 163 / 221\n",
      "    Batch 164 / 221\n",
      "    Batch 165 / 221\n",
      "    Batch 166 / 221\n",
      "    Batch 167 / 221\n",
      "    Batch 168 / 221\n",
      "    Batch 169 / 221\n",
      "    Batch 170 / 221\n",
      "    Batch 171 / 221\n",
      "    Batch 172 / 221\n",
      "    Batch 173 / 221\n",
      "    Batch 174 / 221\n",
      "    Batch 175 / 221\n",
      "    Batch 176 / 221\n",
      "    Batch 177 / 221\n",
      "    Batch 178 / 221\n",
      "    Batch 179 / 221\n",
      "    Batch 180 / 221\n",
      "    Batch 181 / 221\n",
      "    Batch 182 / 221\n",
      "    Batch 183 / 221\n",
      "    Batch 184 / 221\n",
      "    Batch 185 / 221\n",
      "    Batch 186 / 221\n",
      "    Batch 187 / 221\n",
      "    Batch 188 / 221\n",
      "    Batch 189 / 221\n",
      "    Batch 190 / 221\n",
      "    Batch 191 / 221\n",
      "    Batch 192 / 221\n",
      "    Batch 193 / 221\n",
      "    Batch 194 / 221\n",
      "    Batch 195 / 221\n",
      "    Batch 196 / 221\n",
      "    Batch 197 / 221\n",
      "    Batch 198 / 221\n",
      "    Batch 199 / 221\n",
      "    Batch 200 / 221\n",
      "    Batch 201 / 221\n",
      "    Batch 202 / 221\n",
      "    Batch 203 / 221\n",
      "    Batch 204 / 221\n",
      "    Batch 205 / 221\n",
      "    Batch 206 / 221\n",
      "    Batch 207 / 221\n",
      "    Batch 208 / 221\n",
      "    Batch 209 / 221\n",
      "    Batch 210 / 221\n",
      "    Batch 211 / 221\n",
      "    Batch 212 / 221\n",
      "    Batch 213 / 221\n",
      "    Batch 214 / 221\n",
      "    Batch 215 / 221\n",
      "    Batch 216 / 221\n",
      "    Batch 217 / 221\n",
      "    Batch 218 / 221\n",
      "    Batch 219 / 221\n",
      "    Batch 220 / 221\n",
      "    Batch 221 / 221\n",
      "ROC-AUC score: 0.5169\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset before training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Training.\n",
      "Epoch 1 / 5\n",
      "    Batch 3 / 221: loss 0.6928\n",
      "    ROC-AUC score: 0.5961\n",
      "    Batch 6 / 221: loss 0.6931\n",
      "    ROC-AUC score: 0.4762\n",
      "    Batch 9 / 221: loss 0.6919\n",
      "    ROC-AUC score: 0.5977\n",
      "    Batch 12 / 221: loss 0.6918\n",
      "    ROC-AUC score: 0.7098\n",
      "    Batch 15 / 221: loss 0.6903\n",
      "    ROC-AUC score: 0.7935\n",
      "    Batch 18 / 221: loss 0.6909\n",
      "    ROC-AUC score: 0.7148\n",
      "    Batch 21 / 221: loss 0.6908\n",
      "    ROC-AUC score: 0.6431\n",
      "    Batch 24 / 221: loss 0.6885\n",
      "    ROC-AUC score: 0.5917\n",
      "    Batch 27 / 221: loss 0.6908\n",
      "    ROC-AUC score: 0.6508\n",
      "    Batch 30 / 221: loss 0.6857\n",
      "    ROC-AUC score: 0.6500\n",
      "    Batch 33 / 221: loss 0.6875\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 36 / 221: loss 0.6893\n",
      "    ROC-AUC score: 0.6640\n",
      "    Batch 39 / 221: loss 0.6895\n",
      "    ROC-AUC score: 0.7063\n",
      "    Batch 42 / 221: loss 0.6887\n",
      "    ROC-AUC score: 0.6118\n",
      "    Batch 45 / 221: loss 0.6830\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 48 / 221: loss 0.6850\n",
      "    ROC-AUC score: 0.7667\n",
      "    Batch 51 / 221: loss 0.6812\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 54 / 221: loss 0.6867\n",
      "    ROC-AUC score: 0.7667\n",
      "    Batch 57 / 221: loss 0.6755\n",
      "    ROC-AUC score: 0.6758\n",
      "    Batch 60 / 221: loss 0.6825\n",
      "    ROC-AUC score: 0.7103\n",
      "    Batch 63 / 221: loss 0.6683\n",
      "    ROC-AUC score: 0.8867\n",
      "    Batch 66 / 221: loss 0.6743\n",
      "    ROC-AUC score: 0.7708\n",
      "    Batch 69 / 221: loss 0.6842\n",
      "    ROC-AUC score: 0.5992\n",
      "    Batch 72 / 221: loss 0.6700\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 75 / 221: loss 0.6739\n",
      "    ROC-AUC score: 0.7255\n",
      "    Batch 78 / 221: loss 0.6776\n",
      "    ROC-AUC score: 0.7333\n",
      "    Batch 81 / 221: loss 0.6682\n",
      "    ROC-AUC score: 0.6875\n",
      "    Batch 84 / 221: loss 0.6715\n",
      "    ROC-AUC score: 0.6786\n",
      "    Batch 87 / 221: loss 0.6631\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 90 / 221: loss 0.6524\n",
      "    ROC-AUC score: 0.7738\n",
      "    Batch 93 / 221: loss 0.6567\n",
      "    ROC-AUC score: 0.7247\n",
      "    Batch 96 / 221: loss 0.6597\n",
      "    ROC-AUC score: 0.7333\n",
      "    Batch 99 / 221: loss 0.6591\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 102 / 221: loss 0.6522\n",
      "    ROC-AUC score: 0.7529\n",
      "    Batch 105 / 221: loss 0.6572\n",
      "    ROC-AUC score: 0.6194\n",
      "    Batch 108 / 221: loss 0.6472\n",
      "    ROC-AUC score: 0.8208\n",
      "    Batch 111 / 221: loss 0.6504\n",
      "    ROC-AUC score: 0.8138\n",
      "    Batch 114 / 221: loss 0.6439\n",
      "    ROC-AUC score: 0.7024\n",
      "    Batch 117 / 221: loss 0.6485\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 120 / 221: loss 0.6362\n",
      "    ROC-AUC score: 0.7451\n",
      "    Batch 123 / 221: loss 0.6350\n",
      "    ROC-AUC score: 0.8492\n",
      "    Batch 126 / 221: loss 0.6222\n",
      "    ROC-AUC score: 0.9150\n",
      "    Batch 129 / 221: loss 0.6225\n",
      "    ROC-AUC score: 0.7895\n",
      "    Batch 132 / 221: loss 0.6526\n",
      "    ROC-AUC score: 0.8704\n",
      "    Batch 135 / 221: loss 0.6591\n",
      "    ROC-AUC score: 0.7250\n",
      "    Batch 138 / 221: loss 0.6339\n",
      "    ROC-AUC score: 0.7431\n",
      "    Batch 141 / 221: loss 0.6211\n",
      "    ROC-AUC score: 0.7625\n",
      "    Batch 144 / 221: loss 0.6560\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 147 / 221: loss 0.6542\n",
      "    ROC-AUC score: 0.6211\n",
      "    Batch 150 / 221: loss 0.6528\n",
      "    ROC-AUC score: 0.6926\n",
      "    Batch 153 / 221: loss 0.6457\n",
      "    ROC-AUC score: 0.9405\n",
      "    Batch 156 / 221: loss 0.6433\n",
      "    ROC-AUC score: 0.8375\n",
      "    Batch 159 / 221: loss 0.6167\n",
      "    ROC-AUC score: 0.8708\n",
      "    Batch 162 / 221: loss 0.6074\n",
      "    ROC-AUC score: 0.8182\n",
      "    Batch 165 / 221: loss 0.6394\n",
      "    ROC-AUC score: 0.6641\n",
      "    Batch 168 / 221: loss 0.6147\n",
      "    ROC-AUC score: 0.6761\n",
      "    Batch 171 / 221: loss 0.6328\n",
      "    ROC-AUC score: 0.6255\n",
      "    Batch 174 / 221: loss 0.6007\n",
      "    ROC-AUC score: 0.8320\n",
      "    Batch 177 / 221: loss 0.6103\n",
      "    ROC-AUC score: 0.7854\n",
      "    Batch 180 / 221: loss 0.6181\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 183 / 221: loss 0.5900\n",
      "    ROC-AUC score: 0.8941\n",
      "    Batch 186 / 221: loss 0.6363\n",
      "    ROC-AUC score: 0.7891\n",
      "    Batch 189 / 221: loss 0.6380\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 192 / 221: loss 0.5867\n",
      "    ROC-AUC score: 0.7103\n",
      "    Batch 195 / 221: loss 0.6299\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 198 / 221: loss 0.6529\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 201 / 221: loss 0.5979\n",
      "    ROC-AUC score: 0.7381\n",
      "    Batch 204 / 221: loss 0.6310\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 207 / 221: loss 0.5834\n",
      "    ROC-AUC score: 0.8623\n",
      "    Batch 210 / 221: loss 0.6387\n",
      "    ROC-AUC score: 0.7063\n",
      "    Batch 213 / 221: loss 0.6081\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 216 / 221: loss 0.6280\n",
      "    ROC-AUC score: 0.7085\n",
      "    Batch 219 / 221: loss 0.6270\n",
      "    ROC-AUC score: 0.8392\n",
      "Epoch 2 / 5\n",
      "    Batch 3 / 221: loss 0.5878\n",
      "    ROC-AUC score: 0.8214\n",
      "    Batch 6 / 221: loss 0.6224\n",
      "    ROC-AUC score: 0.7817\n",
      "    Batch 9 / 221: loss 0.5819\n",
      "    ROC-AUC score: 0.7412\n",
      "    Batch 12 / 221: loss 0.6426\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 15 / 221: loss 0.6386\n",
      "    ROC-AUC score: 0.8145\n",
      "    Batch 18 / 221: loss 0.6553\n",
      "    ROC-AUC score: 0.7625\n",
      "    Batch 21 / 221: loss 0.6377\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 24 / 221: loss 0.5980\n",
      "    ROC-AUC score: 0.7778\n",
      "    Batch 27 / 221: loss 0.5787\n",
      "    ROC-AUC score: 0.8543\n",
      "    Batch 30 / 221: loss 0.6061\n",
      "    ROC-AUC score: 0.9636\n",
      "    Batch 33 / 221: loss 0.6208\n",
      "    ROC-AUC score: 0.8008\n",
      "    Batch 36 / 221: loss 0.5928\n",
      "    ROC-AUC score: 0.8373\n",
      "    Batch 39 / 221: loss 0.5986\n",
      "    ROC-AUC score: 0.7625\n",
      "    Batch 42 / 221: loss 0.6096\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 45 / 221: loss 0.6059\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 48 / 221: loss 0.6004\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 51 / 221: loss 0.5613\n",
      "    ROC-AUC score: 0.8203\n",
      "    Batch 54 / 221: loss 0.5530\n",
      "    ROC-AUC score: 0.9922\n",
      "    Batch 57 / 221: loss 0.6121\n",
      "    ROC-AUC score: 0.6875\n",
      "    Batch 60 / 221: loss 0.5969\n",
      "    ROC-AUC score: 0.7343\n",
      "    Batch 63 / 221: loss 0.5733\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 66 / 221: loss 0.6105\n",
      "    ROC-AUC score: 0.7222\n",
      "    Batch 69 / 221: loss 0.5405\n",
      "    ROC-AUC score: 0.7958\n",
      "    Batch 72 / 221: loss 0.5846\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 75 / 221: loss 0.6068\n",
      "    ROC-AUC score: 0.7733\n",
      "    Batch 78 / 221: loss 0.5785\n",
      "    ROC-AUC score: 0.8359\n",
      "    Batch 81 / 221: loss 0.5893\n",
      "    ROC-AUC score: 0.8704\n",
      "    Batch 84 / 221: loss 0.5331\n",
      "    ROC-AUC score: 0.9433\n",
      "    Batch 87 / 221: loss 0.6011\n",
      "    ROC-AUC score: 0.8906\n",
      "    Batch 90 / 221: loss 0.5883\n",
      "    ROC-AUC score: 0.7247\n",
      "    Batch 93 / 221: loss 0.6407\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 96 / 221: loss 0.5243\n",
      "    ROC-AUC score: 0.9246\n",
      "    Batch 99 / 221: loss 0.5155\n",
      "    ROC-AUC score: 0.8980\n",
      "    Batch 102 / 221: loss 0.5764\n",
      "    ROC-AUC score: 0.7897\n",
      "    Batch 105 / 221: loss 0.6395\n",
      "    ROC-AUC score: 0.5992\n",
      "    Batch 108 / 221: loss 0.6226\n",
      "    ROC-AUC score: 0.7294\n",
      "    Batch 111 / 221: loss 0.5693\n",
      "    ROC-AUC score: 0.8633\n",
      "    Batch 114 / 221: loss 0.5845\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 117 / 221: loss 0.5931\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 120 / 221: loss 0.5593\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 123 / 221: loss 0.5787\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 126 / 221: loss 0.5394\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 129 / 221: loss 0.5952\n",
      "    ROC-AUC score: 0.7183\n",
      "    Batch 132 / 221: loss 0.6517\n",
      "    ROC-AUC score: 0.6627\n",
      "    Batch 135 / 221: loss 0.5935\n",
      "    ROC-AUC score: 0.7930\n",
      "    Batch 138 / 221: loss 0.5739\n",
      "    ROC-AUC score: 0.7294\n",
      "    Batch 141 / 221: loss 0.6159\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 144 / 221: loss 0.5704\n",
      "    ROC-AUC score: 0.8314\n",
      "    Batch 147 / 221: loss 0.5559\n",
      "    ROC-AUC score: 0.8773\n",
      "    Batch 150 / 221: loss 0.5924\n",
      "    ROC-AUC score: 0.8073\n",
      "    Batch 153 / 221: loss 0.5904\n",
      "    ROC-AUC score: 0.9529\n",
      "    Batch 156 / 221: loss 0.5465\n",
      "    ROC-AUC score: 0.8810\n",
      "    Batch 159 / 221: loss 0.6246\n",
      "    ROC-AUC score: 0.8274\n",
      "    Batch 162 / 221: loss 0.6075\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 165 / 221: loss 0.5919\n",
      "    ROC-AUC score: 0.8826\n",
      "    Batch 168 / 221: loss 0.5743\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 171 / 221: loss 0.5650\n",
      "    ROC-AUC score: 0.8516\n",
      "    Batch 174 / 221: loss 0.5920\n",
      "    ROC-AUC score: 0.7935\n",
      "    Batch 177 / 221: loss 0.5513\n",
      "    ROC-AUC score: 0.9087\n",
      "    Batch 180 / 221: loss 0.5893\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 183 / 221: loss 0.5468\n",
      "    ROC-AUC score: 0.9484\n",
      "    Batch 186 / 221: loss 0.5711\n",
      "    ROC-AUC score: 0.7895\n",
      "    Batch 189 / 221: loss 0.5516\n",
      "    ROC-AUC score: 0.8627\n",
      "    Batch 192 / 221: loss 0.5751\n",
      "    ROC-AUC score: 0.8672\n",
      "    Batch 195 / 221: loss 0.5262\n",
      "    ROC-AUC score: 0.9042\n",
      "    Batch 198 / 221: loss 0.6107\n",
      "    ROC-AUC score: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 201 / 221: loss 0.5829\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 204 / 221: loss 0.6051\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 207 / 221: loss 0.6462\n",
      "    ROC-AUC score: 0.7864\n",
      "    Batch 210 / 221: loss 0.5396\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 213 / 221: loss 0.5205\n",
      "    ROC-AUC score: 0.8073\n",
      "    Batch 216 / 221: loss 0.5981\n",
      "    ROC-AUC score: 0.7222\n",
      "    Batch 219 / 221: loss 0.5962\n",
      "    ROC-AUC score: 0.7287\n",
      "Epoch 3 / 5\n",
      "    Batch 3 / 221: loss 0.5700\n",
      "    ROC-AUC score: 0.8651\n",
      "    Batch 6 / 221: loss 0.5222\n",
      "    ROC-AUC score: 0.8413\n",
      "    Batch 9 / 221: loss 0.5420\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 12 / 221: loss 0.6390\n",
      "    ROC-AUC score: 0.7591\n",
      "    Batch 15 / 221: loss 0.6330\n",
      "    ROC-AUC score: 0.9221\n",
      "    Batch 18 / 221: loss 0.5414\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 21 / 221: loss 0.5660\n",
      "    ROC-AUC score: 0.8968\n",
      "    Batch 24 / 221: loss 0.5625\n",
      "    ROC-AUC score: 0.6275\n",
      "    Batch 27 / 221: loss 0.5643\n",
      "    ROC-AUC score: 0.7583\n",
      "    Batch 30 / 221: loss 0.5606\n",
      "    ROC-AUC score: 0.9167\n",
      "    Batch 33 / 221: loss 0.5750\n",
      "    ROC-AUC score: 0.8824\n",
      "    Batch 36 / 221: loss 0.6406\n",
      "    ROC-AUC score: 0.7004\n",
      "    Batch 39 / 221: loss 0.5948\n",
      "    ROC-AUC score: 0.7451\n",
      "    Batch 42 / 221: loss 0.5794\n",
      "    ROC-AUC score: 0.8138\n",
      "    Batch 45 / 221: loss 0.5586\n",
      "    ROC-AUC score: 0.8947\n",
      "    Batch 48 / 221: loss 0.5670\n",
      "    ROC-AUC score: 0.9648\n",
      "    Batch 51 / 221: loss 0.5744\n",
      "    ROC-AUC score: 0.8788\n",
      "    Batch 54 / 221: loss 0.5339\n",
      "    ROC-AUC score: 0.7725\n",
      "    Batch 57 / 221: loss 0.6006\n",
      "    ROC-AUC score: 0.7686\n",
      "    Batch 60 / 221: loss 0.5309\n",
      "    ROC-AUC score: 0.9137\n",
      "    Batch 63 / 221: loss 0.5795\n",
      "    ROC-AUC score: 0.7373\n",
      "    Batch 66 / 221: loss 0.6263\n",
      "    ROC-AUC score: 0.8057\n",
      "    Batch 69 / 221: loss 0.5112\n",
      "    ROC-AUC score: 0.9882\n",
      "    Batch 72 / 221: loss 0.5433\n",
      "    ROC-AUC score: 0.8203\n",
      "    Batch 75 / 221: loss 0.5721\n",
      "    ROC-AUC score: 0.7897\n",
      "    Batch 78 / 221: loss 0.6542\n",
      "    ROC-AUC score: 0.7183\n",
      "    Batch 81 / 221: loss 0.5407\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 84 / 221: loss 0.5310\n",
      "    ROC-AUC score: 0.8672\n",
      "    Batch 87 / 221: loss 0.5856\n",
      "    ROC-AUC score: 0.8175\n",
      "    Batch 90 / 221: loss 0.5154\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 93 / 221: loss 0.5256\n",
      "    ROC-AUC score: 0.8314\n",
      "    Batch 96 / 221: loss 0.5155\n",
      "    ROC-AUC score: 0.9000\n",
      "    Batch 99 / 221: loss 0.5917\n",
      "    ROC-AUC score: 0.8314\n",
      "    Batch 102 / 221: loss 0.5385\n",
      "    ROC-AUC score: 0.8591\n",
      "    Batch 105 / 221: loss 0.6488\n",
      "    ROC-AUC score: 0.8254\n",
      "    Batch 108 / 221: loss 0.5470\n",
      "    ROC-AUC score: 0.7422\n",
      "    Batch 111 / 221: loss 0.5391\n",
      "    ROC-AUC score: 0.7835\n",
      "    Batch 114 / 221: loss 0.5947\n",
      "    ROC-AUC score: 0.8458\n",
      "    Batch 117 / 221: loss 0.5193\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 120 / 221: loss 0.5618\n",
      "    ROC-AUC score: 0.9492\n",
      "    Batch 123 / 221: loss 0.5714\n",
      "    ROC-AUC score: 0.9294\n",
      "    Batch 126 / 221: loss 0.5676\n",
      "    ROC-AUC score: 0.8320\n",
      "    Batch 129 / 221: loss 0.5280\n",
      "    ROC-AUC score: 0.8968\n",
      "    Batch 132 / 221: loss 0.5892\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 135 / 221: loss 0.5668\n",
      "    ROC-AUC score: 0.8730\n",
      "    Batch 138 / 221: loss 0.6266\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 141 / 221: loss 0.6108\n",
      "    ROC-AUC score: 0.8373\n",
      "    Batch 144 / 221: loss 0.5973\n",
      "    ROC-AUC score: 0.6905\n",
      "    Batch 147 / 221: loss 0.5274\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 150 / 221: loss 0.4866\n",
      "    ROC-AUC score: 0.9565\n",
      "    Batch 153 / 221: loss 0.5478\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 156 / 221: loss 0.5943\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 159 / 221: loss 0.5570\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 162 / 221: loss 0.6129\n",
      "    ROC-AUC score: 0.7255\n",
      "    Batch 165 / 221: loss 0.6228\n",
      "    ROC-AUC score: 0.6270\n",
      "    Batch 168 / 221: loss 0.5682\n",
      "    ROC-AUC score: 0.8711\n",
      "    Batch 171 / 221: loss 0.6138\n",
      "    ROC-AUC score: 0.6818\n",
      "    Batch 174 / 221: loss 0.5129\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 177 / 221: loss 0.5418\n",
      "    ROC-AUC score: 0.7206\n",
      "    Batch 180 / 221: loss 0.5751\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 183 / 221: loss 0.6038\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 186 / 221: loss 0.5744\n",
      "    ROC-AUC score: 0.7333\n",
      "    Batch 189 / 221: loss 0.6259\n",
      "    ROC-AUC score: 0.8413\n",
      "    Batch 192 / 221: loss 0.5843\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 195 / 221: loss 0.5570\n",
      "    ROC-AUC score: 0.9325\n",
      "    Batch 198 / 221: loss 0.5665\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 201 / 221: loss 0.5663\n",
      "    ROC-AUC score: 0.7917\n",
      "    Batch 204 / 221: loss 0.5522\n",
      "    ROC-AUC score: 0.8730\n",
      "    Batch 207 / 221: loss 0.5959\n",
      "    ROC-AUC score: 0.7935\n",
      "    Batch 210 / 221: loss 0.5735\n",
      "    ROC-AUC score: 0.8690\n",
      "    Batch 213 / 221: loss 0.5693\n",
      "    ROC-AUC score: 0.7930\n",
      "    Batch 216 / 221: loss 0.5445\n",
      "    ROC-AUC score: 0.9333\n",
      "    Batch 219 / 221: loss 0.6510\n",
      "    ROC-AUC score: 0.7136\n",
      "Epoch 4 / 5\n",
      "    Batch 3 / 221: loss 0.5632\n",
      "    ROC-AUC score: 0.6941\n",
      "    Batch 6 / 221: loss 0.5228\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 9 / 221: loss 0.5489\n",
      "    ROC-AUC score: 0.8438\n",
      "    Batch 12 / 221: loss 0.5406\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 15 / 221: loss 0.5432\n",
      "    ROC-AUC score: 0.7143\n",
      "    Batch 18 / 221: loss 0.5229\n",
      "    ROC-AUC score: 0.7738\n",
      "    Batch 21 / 221: loss 0.5672\n",
      "    ROC-AUC score: 0.9437\n",
      "    Batch 24 / 221: loss 0.5532\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 27 / 221: loss 0.5157\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 30 / 221: loss 0.5370\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 33 / 221: loss 0.6073\n",
      "    ROC-AUC score: 0.7695\n",
      "    Batch 36 / 221: loss 0.6015\n",
      "    ROC-AUC score: 0.6941\n",
      "    Batch 39 / 221: loss 0.5425\n",
      "    ROC-AUC score: 0.9231\n",
      "    Batch 42 / 221: loss 0.5440\n",
      "    ROC-AUC score: 0.8300\n",
      "    Batch 45 / 221: loss 0.5751\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 48 / 221: loss 0.5963\n",
      "    ROC-AUC score: 0.8340\n",
      "    Batch 51 / 221: loss 0.5286\n",
      "    ROC-AUC score: 0.7792\n",
      "    Batch 54 / 221: loss 0.6203\n",
      "    ROC-AUC score: 0.8175\n",
      "    Batch 57 / 221: loss 0.4425\n",
      "    ROC-AUC score: 0.8792\n",
      "    Batch 60 / 221: loss 0.6181\n",
      "    ROC-AUC score: 0.7227\n",
      "    Batch 63 / 221: loss 0.6207\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 66 / 221: loss 0.6341\n",
      "    ROC-AUC score: 0.7695\n",
      "    Batch 69 / 221: loss 0.5555\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 72 / 221: loss 0.5834\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 75 / 221: loss 0.5284\n",
      "    ROC-AUC score: 0.8068\n",
      "    Batch 78 / 221: loss 0.4930\n",
      "    ROC-AUC score: 0.8708\n",
      "    Batch 81 / 221: loss 0.4877\n",
      "    ROC-AUC score: 0.8438\n",
      "    Batch 84 / 221: loss 0.6081\n",
      "    ROC-AUC score: 0.7812\n",
      "    Batch 87 / 221: loss 0.5921\n",
      "    ROC-AUC score: 0.6863\n",
      "    Batch 90 / 221: loss 0.5919\n",
      "    ROC-AUC score: 0.8492\n",
      "    Batch 93 / 221: loss 0.5755\n",
      "    ROC-AUC score: 0.7109\n",
      "    Batch 96 / 221: loss 0.5884\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 99 / 221: loss 0.5305\n",
      "    ROC-AUC score: 0.8810\n",
      "    Batch 102 / 221: loss 0.5699\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 105 / 221: loss 0.5349\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 108 / 221: loss 0.5878\n",
      "    ROC-AUC score: 0.7412\n",
      "    Batch 111 / 221: loss 0.6148\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 114 / 221: loss 0.5689\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 117 / 221: loss 0.5644\n",
      "    ROC-AUC score: 0.7294\n",
      "    Batch 120 / 221: loss 0.6452\n",
      "    ROC-AUC score: 0.5584\n",
      "    Batch 123 / 221: loss 0.5705\n",
      "    ROC-AUC score: 0.8980\n",
      "    Batch 126 / 221: loss 0.5283\n",
      "    ROC-AUC score: 0.8057\n",
      "    Batch 129 / 221: loss 0.6577\n",
      "    ROC-AUC score: 0.6071\n",
      "    Batch 132 / 221: loss 0.5197\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 135 / 221: loss 0.5693\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 138 / 221: loss 0.5428\n",
      "    ROC-AUC score: 0.8730\n",
      "    Batch 141 / 221: loss 0.5287\n",
      "    ROC-AUC score: 0.9176\n",
      "    Batch 144 / 221: loss 0.5620\n",
      "    ROC-AUC score: 0.7344\n",
      "    Batch 147 / 221: loss 0.6086\n",
      "    ROC-AUC score: 0.7451\n",
      "    Batch 150 / 221: loss 0.6051\n",
      "    ROC-AUC score: 0.8052\n",
      "    Batch 153 / 221: loss 0.6183\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 156 / 221: loss 0.6164\n",
      "    ROC-AUC score: 0.7451\n",
      "    Batch 159 / 221: loss 0.5124\n",
      "    ROC-AUC score: 0.9048\n",
      "    Batch 162 / 221: loss 0.5046\n",
      "    ROC-AUC score: 0.9312\n",
      "    Batch 165 / 221: loss 0.5941\n",
      "    ROC-AUC score: 0.8398\n",
      "    Batch 168 / 221: loss 0.5508\n",
      "    ROC-AUC score: 0.8452\n",
      "    Batch 171 / 221: loss 0.5925\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 174 / 221: loss 0.6063\n",
      "    ROC-AUC score: 0.7804\n",
      "    Batch 177 / 221: loss 0.5966\n",
      "    ROC-AUC score: 0.6316\n",
      "    Batch 180 / 221: loss 0.5315\n",
      "    ROC-AUC score: 0.8828\n",
      "    Batch 183 / 221: loss 0.5801\n",
      "    ROC-AUC score: 0.8543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 186 / 221: loss 0.5623\n",
      "    ROC-AUC score: 0.9318\n",
      "    Batch 189 / 221: loss 0.5304\n",
      "    ROC-AUC score: 0.8300\n",
      "    Batch 192 / 221: loss 0.5390\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 195 / 221: loss 0.5930\n",
      "    ROC-AUC score: 0.5938\n",
      "    Batch 198 / 221: loss 0.6114\n",
      "    ROC-AUC score: 0.8300\n",
      "    Batch 201 / 221: loss 0.5581\n",
      "    ROC-AUC score: 0.8708\n",
      "    Batch 204 / 221: loss 0.5550\n",
      "    ROC-AUC score: 0.8706\n",
      "    Batch 207 / 221: loss 0.5523\n",
      "    ROC-AUC score: 0.8727\n",
      "    Batch 210 / 221: loss 0.5190\n",
      "    ROC-AUC score: 0.8442\n",
      "    Batch 213 / 221: loss 0.5941\n",
      "    ROC-AUC score: 0.9583\n",
      "    Batch 216 / 221: loss 0.5426\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 219 / 221: loss 0.6195\n",
      "    ROC-AUC score: 0.7302\n",
      "Epoch 5 / 5\n",
      "    Batch 3 / 221: loss 0.4902\n",
      "    ROC-AUC score: 0.8532\n",
      "    Batch 6 / 221: loss 0.5997\n",
      "    ROC-AUC score: 0.6627\n",
      "    Batch 9 / 221: loss 0.5849\n",
      "    ROC-AUC score: 0.8196\n",
      "    Batch 12 / 221: loss 0.5402\n",
      "    ROC-AUC score: 0.8135\n",
      "    Batch 15 / 221: loss 0.5620\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 18 / 221: loss 0.5820\n",
      "    ROC-AUC score: 0.8701\n",
      "    Batch 21 / 221: loss 0.5689\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 24 / 221: loss 0.6080\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 27 / 221: loss 0.5711\n",
      "    ROC-AUC score: 0.8292\n",
      "    Batch 30 / 221: loss 0.5623\n",
      "    ROC-AUC score: 0.8532\n",
      "    Batch 33 / 221: loss 0.5461\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 36 / 221: loss 0.6225\n",
      "    ROC-AUC score: 0.4675\n",
      "    Batch 39 / 221: loss 0.5729\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 42 / 221: loss 0.7006\n",
      "    ROC-AUC score: 0.4881\n",
      "    Batch 45 / 221: loss 0.6501\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 48 / 221: loss 0.6051\n",
      "    ROC-AUC score: 0.9433\n",
      "    Batch 51 / 221: loss 0.6725\n",
      "    ROC-AUC score: 0.6397\n",
      "    Batch 54 / 221: loss 0.5571\n",
      "    ROC-AUC score: 0.8492\n",
      "    Batch 57 / 221: loss 0.5717\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 60 / 221: loss 0.5161\n",
      "    ROC-AUC score: 0.9710\n",
      "    Batch 63 / 221: loss 0.5573\n",
      "    ROC-AUC score: 0.9524\n",
      "    Batch 66 / 221: loss 0.5165\n",
      "    ROC-AUC score: 0.6583\n",
      "    Batch 69 / 221: loss 0.4718\n",
      "    ROC-AUC score: 0.9545\n",
      "    Batch 72 / 221: loss 0.5892\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 75 / 221: loss 0.5705\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 78 / 221: loss 0.5668\n",
      "    ROC-AUC score: 0.8314\n",
      "    Batch 81 / 221: loss 0.6128\n",
      "    ROC-AUC score: 0.7817\n",
      "    Batch 84 / 221: loss 0.5608\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 87 / 221: loss 0.6110\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 90 / 221: loss 0.5456\n",
      "    ROC-AUC score: 0.8690\n",
      "    Batch 93 / 221: loss 0.5595\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 96 / 221: loss 0.6309\n",
      "    ROC-AUC score: 0.7857\n",
      "    Batch 99 / 221: loss 0.5332\n",
      "    ROC-AUC score: 0.8462\n",
      "    Batch 102 / 221: loss 0.5467\n",
      "    ROC-AUC score: 0.9137\n",
      "    Batch 105 / 221: loss 0.5356\n",
      "    ROC-AUC score: 0.7176\n",
      "    Batch 108 / 221: loss 0.5011\n",
      "    ROC-AUC score: 0.8824\n",
      "    Batch 111 / 221: loss 0.5418\n",
      "    ROC-AUC score: 0.8785\n",
      "    Batch 114 / 221: loss 0.5501\n",
      "    ROC-AUC score: 0.7659\n",
      "    Batch 117 / 221: loss 0.5976\n",
      "    ROC-AUC score: 0.9297\n",
      "    Batch 120 / 221: loss 0.5570\n",
      "    ROC-AUC score: 0.7422\n",
      "    Batch 123 / 221: loss 0.5484\n",
      "    ROC-AUC score: 0.7085\n",
      "    Batch 126 / 221: loss 0.5692\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 129 / 221: loss 0.6061\n",
      "    ROC-AUC score: 0.7540\n",
      "    Batch 132 / 221: loss 0.4987\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 135 / 221: loss 0.5774\n",
      "    ROC-AUC score: 0.8711\n",
      "    Batch 138 / 221: loss 0.5946\n",
      "    ROC-AUC score: 0.9167\n",
      "    Batch 141 / 221: loss 0.6144\n",
      "    ROC-AUC score: 0.7042\n",
      "    Batch 144 / 221: loss 0.5173\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 147 / 221: loss 0.5832\n",
      "    ROC-AUC score: 0.7891\n",
      "    Batch 150 / 221: loss 0.5400\n",
      "    ROC-AUC score: 0.7287\n",
      "    Batch 153 / 221: loss 0.5468\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 156 / 221: loss 0.5380\n",
      "    ROC-AUC score: 0.8320\n",
      "    Batch 159 / 221: loss 0.5929\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 162 / 221: loss 0.5234\n",
      "    ROC-AUC score: 0.9841\n",
      "    Batch 165 / 221: loss 0.5456\n",
      "    ROC-AUC score: 0.7451\n",
      "    Batch 168 / 221: loss 0.5856\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 171 / 221: loss 0.5080\n",
      "    ROC-AUC score: 0.9717\n",
      "    Batch 174 / 221: loss 0.5386\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 177 / 221: loss 0.5502\n",
      "    ROC-AUC score: 0.8542\n",
      "    Batch 180 / 221: loss 0.5359\n",
      "    ROC-AUC score: 0.8583\n",
      "    Batch 183 / 221: loss 0.5267\n",
      "    ROC-AUC score: 0.8889\n",
      "    Batch 186 / 221: loss 0.5294\n",
      "    ROC-AUC score: 0.9417\n",
      "    Batch 189 / 221: loss 0.5549\n",
      "    ROC-AUC score: 0.8849\n",
      "    Batch 192 / 221: loss 0.5072\n",
      "    ROC-AUC score: 0.9087\n",
      "    Batch 195 / 221: loss 0.5465\n",
      "    ROC-AUC score: 0.9595\n",
      "    Batch 198 / 221: loss 0.6110\n",
      "    ROC-AUC score: 0.9087\n",
      "    Batch 201 / 221: loss 0.5215\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 204 / 221: loss 0.5584\n",
      "    ROC-AUC score: 0.9028\n",
      "    Batch 207 / 221: loss 0.5646\n",
      "    ROC-AUC score: 0.6836\n",
      "    Batch 210 / 221: loss 0.6342\n",
      "    ROC-AUC score: 0.6409\n",
      "    Batch 213 / 221: loss 0.6115\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 216 / 221: loss 0.4419\n",
      "    ROC-AUC score: 0.9603\n",
      "    Batch 219 / 221: loss 0.5155\n",
      "    ROC-AUC score: 0.9048\n",
      "Finished training.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    use_visdom = config['visdom']\n",
    "    if use_visdom:\n",
    "        vis = visdom.Visdom()\n",
    "        loss_window = None\n",
    "    criterion = utils.get_criterion(config['task'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                           weight_decay=config['weight_decay'])\n",
    "    epochs = config['epochs']\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.8)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 400], gamma=0.7)\n",
    "    model.train()\n",
    "    print('--------------------------------')\n",
    "    print('Training.')\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "        running_loss = 0.0\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            loss = criterion(scores, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "            if (idx + 1) % stats_per_batch == 0:\n",
    "                running_loss /= stats_per_batch\n",
    "                print('    Batch {} / {}: loss {:.4f}'.format(\n",
    "                    idx+1, num_batches, running_loss))\n",
    "                if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "                    area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "                    print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "                running_loss = 0.0\n",
    "                num_correct, num_examples = 0, 0\n",
    "            if use_visdom:\n",
    "                if loss_window is None:\n",
    "                    loss_window = vis.line(\n",
    "                        Y=[loss.item()],\n",
    "                        X=[epoch*num_batches+idx],\n",
    "                        opts=dict(xlabel='batch', ylabel='Loss', title='Training Loss', legend=['Loss']))\n",
    "                else:\n",
    "                    vis.line(\n",
    "                        [loss.item()],\n",
    "                        [epoch*num_batches+idx],\n",
    "                        win=loss_window,\n",
    "                        update='append')\n",
    "            scheduler.step()\n",
    "    if use_visdom:\n",
    "        vis.close(win=loss_window)\n",
    "    print('Finished training.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['load']:\n",
    "    if config['save']:\n",
    "        print('--------------------------------')\n",
    "        directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                'trained_models')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        fname = utils.get_fname(config)\n",
    "        path = os.path.join(directory, fname)\n",
    "        print('Saving model at {}'.format(path))\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print('Finished saving model.')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset after training.\n",
      "    Batch 1 / 221\n",
      "    Batch 2 / 221\n",
      "    Batch 3 / 221\n",
      "    Batch 4 / 221\n",
      "    Batch 5 / 221\n",
      "    Batch 6 / 221\n",
      "    Batch 7 / 221\n",
      "    Batch 8 / 221\n",
      "    Batch 9 / 221\n",
      "    Batch 10 / 221\n",
      "    Batch 11 / 221\n",
      "    Batch 12 / 221\n",
      "    Batch 13 / 221\n",
      "    Batch 14 / 221\n",
      "    Batch 15 / 221\n",
      "    Batch 16 / 221\n",
      "    Batch 17 / 221\n",
      "    Batch 18 / 221\n",
      "    Batch 19 / 221\n",
      "    Batch 20 / 221\n",
      "    Batch 21 / 221\n",
      "    Batch 22 / 221\n",
      "    Batch 23 / 221\n",
      "    Batch 24 / 221\n",
      "    Batch 25 / 221\n",
      "    Batch 26 / 221\n",
      "    Batch 27 / 221\n",
      "    Batch 28 / 221\n",
      "    Batch 29 / 221\n",
      "    Batch 30 / 221\n",
      "    Batch 31 / 221\n",
      "    Batch 32 / 221\n",
      "    Batch 33 / 221\n",
      "    Batch 34 / 221\n",
      "    Batch 35 / 221\n",
      "    Batch 36 / 221\n",
      "    Batch 37 / 221\n",
      "    Batch 38 / 221\n",
      "    Batch 39 / 221\n",
      "    Batch 40 / 221\n",
      "    Batch 41 / 221\n",
      "    Batch 42 / 221\n",
      "    Batch 43 / 221\n",
      "    Batch 44 / 221\n",
      "    Batch 45 / 221\n",
      "    Batch 46 / 221\n",
      "    Batch 47 / 221\n",
      "    Batch 48 / 221\n",
      "    Batch 49 / 221\n",
      "    Batch 50 / 221\n",
      "    Batch 51 / 221\n",
      "    Batch 52 / 221\n",
      "    Batch 53 / 221\n",
      "    Batch 54 / 221\n",
      "    Batch 55 / 221\n",
      "    Batch 56 / 221\n",
      "    Batch 57 / 221\n",
      "    Batch 58 / 221\n",
      "    Batch 59 / 221\n",
      "    Batch 60 / 221\n",
      "    Batch 61 / 221\n",
      "    Batch 62 / 221\n",
      "    Batch 63 / 221\n",
      "    Batch 64 / 221\n",
      "    Batch 65 / 221\n",
      "    Batch 66 / 221\n",
      "    Batch 67 / 221\n",
      "    Batch 68 / 221\n",
      "    Batch 69 / 221\n",
      "    Batch 70 / 221\n",
      "    Batch 71 / 221\n",
      "    Batch 72 / 221\n",
      "    Batch 73 / 221\n",
      "    Batch 74 / 221\n",
      "    Batch 75 / 221\n",
      "    Batch 76 / 221\n",
      "    Batch 77 / 221\n",
      "    Batch 78 / 221\n",
      "    Batch 79 / 221\n",
      "    Batch 80 / 221\n",
      "    Batch 81 / 221\n",
      "    Batch 82 / 221\n",
      "    Batch 83 / 221\n",
      "    Batch 84 / 221\n",
      "    Batch 85 / 221\n",
      "    Batch 86 / 221\n",
      "    Batch 87 / 221\n",
      "    Batch 88 / 221\n",
      "    Batch 89 / 221\n",
      "    Batch 90 / 221\n",
      "    Batch 91 / 221\n",
      "    Batch 92 / 221\n",
      "    Batch 93 / 221\n",
      "    Batch 94 / 221\n",
      "    Batch 95 / 221\n",
      "    Batch 96 / 221\n",
      "    Batch 97 / 221\n",
      "    Batch 98 / 221\n",
      "    Batch 99 / 221\n",
      "    Batch 100 / 221\n",
      "    Batch 101 / 221\n",
      "    Batch 102 / 221\n",
      "    Batch 103 / 221\n",
      "    Batch 104 / 221\n",
      "    Batch 105 / 221\n",
      "    Batch 106 / 221\n",
      "    Batch 107 / 221\n",
      "    Batch 108 / 221\n",
      "    Batch 109 / 221\n",
      "    Batch 110 / 221\n",
      "    Batch 111 / 221\n",
      "    Batch 112 / 221\n",
      "    Batch 113 / 221\n",
      "    Batch 114 / 221\n",
      "    Batch 115 / 221\n",
      "    Batch 116 / 221\n",
      "    Batch 117 / 221\n",
      "    Batch 118 / 221\n",
      "    Batch 119 / 221\n",
      "    Batch 120 / 221\n",
      "    Batch 121 / 221\n",
      "    Batch 122 / 221\n",
      "    Batch 123 / 221\n",
      "    Batch 124 / 221\n",
      "    Batch 125 / 221\n",
      "    Batch 126 / 221\n",
      "    Batch 127 / 221\n",
      "    Batch 128 / 221\n",
      "    Batch 129 / 221\n",
      "    Batch 130 / 221\n",
      "    Batch 131 / 221\n",
      "    Batch 132 / 221\n",
      "    Batch 133 / 221\n",
      "    Batch 134 / 221\n",
      "    Batch 135 / 221\n",
      "    Batch 136 / 221\n",
      "    Batch 137 / 221\n",
      "    Batch 138 / 221\n",
      "    Batch 139 / 221\n",
      "    Batch 140 / 221\n",
      "    Batch 141 / 221\n",
      "    Batch 142 / 221\n",
      "    Batch 143 / 221\n",
      "    Batch 144 / 221\n",
      "    Batch 145 / 221\n",
      "    Batch 146 / 221\n",
      "    Batch 147 / 221\n",
      "    Batch 148 / 221\n",
      "    Batch 149 / 221\n",
      "    Batch 150 / 221\n",
      "    Batch 151 / 221\n",
      "    Batch 152 / 221\n",
      "    Batch 153 / 221\n",
      "    Batch 154 / 221\n",
      "    Batch 155 / 221\n",
      "    Batch 156 / 221\n",
      "    Batch 157 / 221\n",
      "    Batch 158 / 221\n",
      "    Batch 159 / 221\n",
      "    Batch 160 / 221\n",
      "    Batch 161 / 221\n",
      "    Batch 162 / 221\n",
      "    Batch 163 / 221\n",
      "    Batch 164 / 221\n",
      "    Batch 165 / 221\n",
      "    Batch 166 / 221\n",
      "    Batch 167 / 221\n",
      "    Batch 168 / 221\n",
      "    Batch 169 / 221\n",
      "    Batch 170 / 221\n",
      "    Batch 171 / 221\n",
      "    Batch 172 / 221\n",
      "    Batch 173 / 221\n",
      "    Batch 174 / 221\n",
      "    Batch 175 / 221\n",
      "    Batch 176 / 221\n",
      "    Batch 177 / 221\n",
      "    Batch 178 / 221\n",
      "    Batch 179 / 221\n",
      "    Batch 180 / 221\n",
      "    Batch 181 / 221\n",
      "    Batch 182 / 221\n",
      "    Batch 183 / 221\n",
      "    Batch 184 / 221\n",
      "    Batch 185 / 221\n",
      "    Batch 186 / 221\n",
      "    Batch 187 / 221\n",
      "    Batch 188 / 221\n",
      "    Batch 189 / 221\n",
      "    Batch 190 / 221\n",
      "    Batch 191 / 221\n",
      "    Batch 192 / 221\n",
      "    Batch 193 / 221\n",
      "    Batch 194 / 221\n",
      "    Batch 195 / 221\n",
      "    Batch 196 / 221\n",
      "    Batch 197 / 221\n",
      "    Batch 198 / 221\n",
      "    Batch 199 / 221\n",
      "    Batch 200 / 221\n",
      "    Batch 201 / 221\n",
      "    Batch 202 / 221\n",
      "    Batch 203 / 221\n",
      "    Batch 204 / 221\n",
      "    Batch 205 / 221\n",
      "    Batch 206 / 221\n",
      "    Batch 207 / 221\n",
      "    Batch 208 / 221\n",
      "    Batch 209 / 221\n",
      "    Batch 210 / 221\n",
      "    Batch 211 / 221\n",
      "    Batch 212 / 221\n",
      "    Batch 213 / 221\n",
      "    Batch 214 / 221\n",
      "    Batch 215 / 221\n",
      "    Batch 216 / 221\n",
      "    Batch 217 / 221\n",
      "    Batch 218 / 221\n",
      "    Batch 219 / 221\n",
      "    Batch 220 / 221\n",
      "    Batch 221 / 221\n",
      "ROC-AUC score: 0.8241\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset after training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the true positive rate and true negative rate vs threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    tpr, fpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    tnr = 1 - fpr\n",
    "    plt.plot(thresholds, tpr, label='tpr')\n",
    "    plt.plot(thresholds, tnr, label='tnr')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.title('TPR / TNR vs Threshold')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an appropriate threshold and generate classification report on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 1 / 221\n",
      "    Batch 2 / 221\n",
      "    Batch 3 / 221\n",
      "    Batch 4 / 221\n",
      "    Batch 5 / 221\n",
      "    Batch 6 / 221\n",
      "    Batch 7 / 221\n",
      "    Batch 8 / 221\n",
      "    Batch 9 / 221\n",
      "    Batch 10 / 221\n",
      "    Batch 11 / 221\n",
      "    Batch 12 / 221\n",
      "    Batch 13 / 221\n",
      "    Batch 14 / 221\n",
      "    Batch 15 / 221\n",
      "    Batch 16 / 221\n",
      "    Batch 17 / 221\n",
      "    Batch 18 / 221\n",
      "    Batch 19 / 221\n",
      "    Batch 20 / 221\n",
      "    Batch 21 / 221\n",
      "    Batch 22 / 221\n",
      "    Batch 23 / 221\n",
      "    Batch 24 / 221\n",
      "    Batch 25 / 221\n",
      "    Batch 26 / 221\n",
      "    Batch 27 / 221\n",
      "    Batch 28 / 221\n",
      "    Batch 29 / 221\n",
      "    Batch 30 / 221\n",
      "    Batch 31 / 221\n",
      "    Batch 32 / 221\n",
      "    Batch 33 / 221\n",
      "    Batch 34 / 221\n",
      "    Batch 35 / 221\n",
      "    Batch 36 / 221\n",
      "    Batch 37 / 221\n",
      "    Batch 38 / 221\n",
      "    Batch 39 / 221\n",
      "    Batch 40 / 221\n",
      "    Batch 41 / 221\n",
      "    Batch 42 / 221\n",
      "    Batch 43 / 221\n",
      "    Batch 44 / 221\n",
      "    Batch 45 / 221\n",
      "    Batch 46 / 221\n",
      "    Batch 47 / 221\n",
      "    Batch 48 / 221\n",
      "    Batch 49 / 221\n",
      "    Batch 50 / 221\n",
      "    Batch 51 / 221\n",
      "    Batch 52 / 221\n",
      "    Batch 53 / 221\n",
      "    Batch 54 / 221\n",
      "    Batch 55 / 221\n",
      "    Batch 56 / 221\n",
      "    Batch 57 / 221\n",
      "    Batch 58 / 221\n",
      "    Batch 59 / 221\n",
      "    Batch 60 / 221\n",
      "    Batch 61 / 221\n",
      "    Batch 62 / 221\n",
      "    Batch 63 / 221\n",
      "    Batch 64 / 221\n",
      "    Batch 65 / 221\n",
      "    Batch 66 / 221\n",
      "    Batch 67 / 221\n",
      "    Batch 68 / 221\n",
      "    Batch 69 / 221\n",
      "    Batch 70 / 221\n",
      "    Batch 71 / 221\n",
      "    Batch 72 / 221\n",
      "    Batch 73 / 221\n",
      "    Batch 74 / 221\n",
      "    Batch 75 / 221\n",
      "    Batch 76 / 221\n",
      "    Batch 77 / 221\n",
      "    Batch 78 / 221\n",
      "    Batch 79 / 221\n",
      "    Batch 80 / 221\n",
      "    Batch 81 / 221\n",
      "    Batch 82 / 221\n",
      "    Batch 83 / 221\n",
      "    Batch 84 / 221\n",
      "    Batch 85 / 221\n",
      "    Batch 86 / 221\n",
      "    Batch 87 / 221\n",
      "    Batch 88 / 221\n",
      "    Batch 89 / 221\n",
      "    Batch 90 / 221\n",
      "    Batch 91 / 221\n",
      "    Batch 92 / 221\n",
      "    Batch 93 / 221\n",
      "    Batch 94 / 221\n",
      "    Batch 95 / 221\n",
      "    Batch 96 / 221\n",
      "    Batch 97 / 221\n",
      "    Batch 98 / 221\n",
      "    Batch 99 / 221\n",
      "    Batch 100 / 221\n",
      "    Batch 101 / 221\n",
      "    Batch 102 / 221\n",
      "    Batch 103 / 221\n",
      "    Batch 104 / 221\n",
      "    Batch 105 / 221\n",
      "    Batch 106 / 221\n",
      "    Batch 107 / 221\n",
      "    Batch 108 / 221\n",
      "    Batch 109 / 221\n",
      "    Batch 110 / 221\n",
      "    Batch 111 / 221\n",
      "    Batch 112 / 221\n",
      "    Batch 113 / 221\n",
      "    Batch 114 / 221\n",
      "    Batch 115 / 221\n",
      "    Batch 116 / 221\n",
      "    Batch 117 / 221\n",
      "    Batch 118 / 221\n",
      "    Batch 119 / 221\n",
      "    Batch 120 / 221\n",
      "    Batch 121 / 221\n",
      "    Batch 122 / 221\n",
      "    Batch 123 / 221\n",
      "    Batch 124 / 221\n",
      "    Batch 125 / 221\n",
      "    Batch 126 / 221\n",
      "    Batch 127 / 221\n",
      "    Batch 128 / 221\n",
      "    Batch 129 / 221\n",
      "    Batch 130 / 221\n",
      "    Batch 131 / 221\n",
      "    Batch 132 / 221\n",
      "    Batch 133 / 221\n",
      "    Batch 134 / 221\n",
      "    Batch 135 / 221\n",
      "    Batch 136 / 221\n",
      "    Batch 137 / 221\n",
      "    Batch 138 / 221\n",
      "    Batch 139 / 221\n",
      "    Batch 140 / 221\n",
      "    Batch 141 / 221\n",
      "    Batch 142 / 221\n",
      "    Batch 143 / 221\n",
      "    Batch 144 / 221\n",
      "    Batch 145 / 221\n",
      "    Batch 146 / 221\n",
      "    Batch 147 / 221\n",
      "    Batch 148 / 221\n",
      "    Batch 149 / 221\n",
      "    Batch 150 / 221\n",
      "    Batch 151 / 221\n",
      "    Batch 152 / 221\n",
      "    Batch 153 / 221\n",
      "    Batch 154 / 221\n",
      "    Batch 155 / 221\n",
      "    Batch 156 / 221\n",
      "    Batch 157 / 221\n",
      "    Batch 158 / 221\n",
      "    Batch 159 / 221\n",
      "    Batch 160 / 221\n",
      "    Batch 161 / 221\n",
      "    Batch 162 / 221\n",
      "    Batch 163 / 221\n",
      "    Batch 164 / 221\n",
      "    Batch 165 / 221\n",
      "    Batch 166 / 221\n",
      "    Batch 167 / 221\n",
      "    Batch 168 / 221\n",
      "    Batch 169 / 221\n",
      "    Batch 170 / 221\n",
      "    Batch 171 / 221\n",
      "    Batch 172 / 221\n",
      "    Batch 173 / 221\n",
      "    Batch 174 / 221\n",
      "    Batch 175 / 221\n",
      "    Batch 176 / 221\n",
      "    Batch 177 / 221\n",
      "    Batch 178 / 221\n",
      "    Batch 179 / 221\n",
      "    Batch 180 / 221\n",
      "    Batch 181 / 221\n",
      "    Batch 182 / 221\n",
      "    Batch 183 / 221\n",
      "    Batch 184 / 221\n",
      "    Batch 185 / 221\n",
      "    Batch 186 / 221\n",
      "    Batch 187 / 221\n",
      "    Batch 188 / 221\n",
      "    Batch 189 / 221\n",
      "    Batch 190 / 221\n",
      "    Batch 191 / 221\n",
      "    Batch 192 / 221\n",
      "    Batch 193 / 221\n",
      "    Batch 194 / 221\n",
      "    Batch 195 / 221\n",
      "    Batch 196 / 221\n",
      "    Batch 197 / 221\n",
      "    Batch 198 / 221\n",
      "    Batch 199 / 221\n",
      "    Batch 200 / 221\n",
      "    Batch 201 / 221\n",
      "    Batch 202 / 221\n",
      "    Batch 203 / 221\n",
      "    Batch 204 / 221\n",
      "    Batch 205 / 221\n",
      "    Batch 206 / 221\n",
      "    Batch 207 / 221\n",
      "    Batch 208 / 221\n",
      "    Batch 209 / 221\n",
      "    Batch 210 / 221\n",
      "    Batch 211 / 221\n",
      "    Batch 212 / 221\n",
      "    Batch 213 / 221\n",
      "    Batch 214 / 221\n",
      "    Batch 215 / 221\n",
      "    Batch 216 / 221\n",
      "    Batch 217 / 221\n",
      "    Batch 218 / 221\n",
      "    Batch 219 / 221\n",
      "    Batch 220 / 221\n",
      "    Batch 221 / 221\n",
      "Threshold: 0.3448, accuracy: 0.7528\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.75      0.75      3522\n",
      "         1.0       0.75      0.75      0.75      3522\n",
      "\n",
      "    accuracy                           0.75      7044\n",
      "   macro avg       0.75      0.75      0.75      7044\n",
      "weighted avg       0.75      0.75      0.75      7044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx1 = np.where(tpr <= tnr)[0]\n",
    "idx2 = np.where(tpr >= tnr)[0]\n",
    "t = thresholds[idx1[-1]]\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "with torch.no_grad():\n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        edges, features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        all_pairs = torch.mm(out, out.t())\n",
    "        scores = all_pairs[edges.T]\n",
    "        predictions = (scores >= t).long()\n",
    "        y_true.extend(labels.detach().numpy())\n",
    "        y_pred.extend(predictions.detach().numpy())\n",
    "        total_correct += torch.sum(predictions == labels.long()).item()\n",
    "        total_examples += len(labels) \n",
    "        print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "print('Threshold: {:.4f}, accuracy: {:.4f}'.format(t, total_correct / total_examples))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "print('Classification report\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: val\n",
      "Number of vertices: 113\n",
      "Number of static edges: 1636\n",
      "Number of temporal edges: 10409\n",
      "Number of examples/datapoints: 8550\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the validation dataset after training.\n",
      "    Batch 3 / 268: loss 0.6548, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7063\n",
      "    Batch 6 / 268: loss 0.6217, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7247\n",
      "    Batch 9 / 268: loss 0.5647, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 12 / 268: loss 0.6604, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 15 / 268: loss 0.5859, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 18 / 268: loss 0.6416, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7431\n",
      "    Batch 21 / 268: loss 0.6768, accuracy 0.5833\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 24 / 268: loss 0.6272, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7686\n",
      "    Batch 27 / 268: loss 0.6399, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 30 / 268: loss 0.6630, accuracy 0.7188\n",
      "    ROC-AUC score: 0.6636\n",
      "    Batch 33 / 268: loss 0.6023, accuracy 0.6875\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 36 / 268: loss 0.6236, accuracy 0.6875\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 39 / 268: loss 0.7217, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7183\n",
      "    Batch 42 / 268: loss 0.6157, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7137\n",
      "    Batch 45 / 268: loss 0.5806, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8532\n",
      "    Batch 48 / 268: loss 0.6010, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 51 / 268: loss 0.6033, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8510\n",
      "    Batch 54 / 268: loss 0.5858, accuracy 0.7917\n",
      "    ROC-AUC score: 0.7540\n",
      "    Batch 57 / 268: loss 0.6537, accuracy 0.5938\n",
      "    ROC-AUC score: 0.6392\n",
      "    Batch 60 / 268: loss 0.6159, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7222\n",
      "    Batch 63 / 268: loss 0.6041, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 66 / 268: loss 0.5879, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 69 / 268: loss 0.6195, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7875\n",
      "    Batch 72 / 268: loss 0.6177, accuracy 0.7396\n",
      "    ROC-AUC score: 0.9414\n",
      "    Batch 75 / 268: loss 0.5777, accuracy 0.6875\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 78 / 268: loss 0.6745, accuracy 0.5729\n",
      "    ROC-AUC score: 0.6588\n",
      "    Batch 81 / 268: loss 0.5734, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7449\n",
      "    Batch 84 / 268: loss 0.6949, accuracy 0.6875\n",
      "    ROC-AUC score: 0.5725\n",
      "    Batch 87 / 268: loss 0.5803, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 90 / 268: loss 0.5850, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8167\n",
      "    Batch 93 / 268: loss 0.6724, accuracy 0.6458\n",
      "    ROC-AUC score: 0.6914\n",
      "    Batch 96 / 268: loss 0.5943, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7143\n",
      "    Batch 99 / 268: loss 0.5863, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 102 / 268: loss 0.6305, accuracy 0.6875\n",
      "    ROC-AUC score: 0.8196\n",
      "    Batch 105 / 268: loss 0.6429, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6706\n",
      "    Batch 108 / 268: loss 0.5547, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8528\n",
      "    Batch 111 / 268: loss 0.5919, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7460\n",
      "    Batch 114 / 268: loss 0.6035, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 117 / 268: loss 0.6447, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8810\n",
      "    Batch 120 / 268: loss 0.6610, accuracy 0.6146\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 123 / 268: loss 0.6199, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7738\n",
      "    Batch 126 / 268: loss 0.7132, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6523\n",
      "    Batch 129 / 268: loss 0.5981, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 132 / 268: loss 0.5489, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 135 / 268: loss 0.6675, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7053\n",
      "    Batch 138 / 268: loss 0.5976, accuracy 0.7292\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 141 / 268: loss 0.6099, accuracy 0.6667\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 144 / 268: loss 0.6130, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6627\n",
      "    Batch 147 / 268: loss 0.6196, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6784\n",
      "    Batch 150 / 268: loss 0.6566, accuracy 0.6667\n",
      "    ROC-AUC score: 0.8574\n",
      "    Batch 153 / 268: loss 0.6527, accuracy 0.6146\n",
      "    ROC-AUC score: 0.4364\n",
      "    Batch 156 / 268: loss 0.6499, accuracy 0.6458\n",
      "    ROC-AUC score: 0.6599\n",
      "    Batch 159 / 268: loss 0.5662, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8765\n",
      "    Batch 162 / 268: loss 0.6213, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8442\n",
      "    Batch 165 / 268: loss 0.6897, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6909\n",
      "    Batch 168 / 268: loss 0.6579, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7109\n",
      "    Batch 171 / 268: loss 0.5783, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8214\n",
      "    Batch 174 / 268: loss 0.5931, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7412\n",
      "    Batch 177 / 268: loss 0.5961, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8135\n",
      "    Batch 180 / 268: loss 0.5665, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8849\n",
      "    Batch 183 / 268: loss 0.6289, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 186 / 268: loss 0.6278, accuracy 0.6667\n",
      "    ROC-AUC score: 0.7659\n",
      "    Batch 189 / 268: loss 0.6016, accuracy 0.6875\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 192 / 268: loss 0.5908, accuracy 0.7604\n",
      "    ROC-AUC score: 0.5830\n",
      "    Batch 195 / 268: loss 0.6267, accuracy 0.6667\n",
      "    ROC-AUC score: 0.8627\n",
      "    Batch 198 / 268: loss 0.6204, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8047\n",
      "    Batch 201 / 268: loss 0.6166, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7422\n",
      "    Batch 204 / 268: loss 0.5959, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7458\n",
      "    Batch 207 / 268: loss 0.6062, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7937\n",
      "    Batch 210 / 268: loss 0.6225, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 213 / 268: loss 0.5922, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6599\n",
      "    Batch 216 / 268: loss 0.6179, accuracy 0.6667\n",
      "    ROC-AUC score: 0.7412\n",
      "    Batch 219 / 268: loss 0.6063, accuracy 0.6979\n",
      "    ROC-AUC score: 0.6636\n",
      "    Batch 222 / 268: loss 0.5734, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8542\n",
      "    Batch 225 / 268: loss 0.6182, accuracy 0.6771\n",
      "    ROC-AUC score: 0.9250\n",
      "    Batch 228 / 268: loss 0.5598, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 231 / 268: loss 0.5882, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8292\n",
      "    Batch 234 / 268: loss 0.6060, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7804\n",
      "    Batch 237 / 268: loss 0.5482, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8941\n",
      "    Batch 240 / 268: loss 0.6474, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6917\n",
      "    Batch 243 / 268: loss 0.7075, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7455\n",
      "    Batch 246 / 268: loss 0.6068, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8785\n",
      "    Batch 249 / 268: loss 0.6186, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8611\n",
      "    Batch 252 / 268: loss 0.6535, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7166\n",
      "    Batch 255 / 268: loss 0.6065, accuracy 0.6667\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 258 / 268: loss 0.6737, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7578\n",
      "    Batch 261 / 268: loss 0.6242, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7529\n",
      "    Batch 264 / 268: loss 0.5992, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8889\n",
      "    Batch 267 / 268: loss 0.5929, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6329\n",
      "Loss 0.6184, accuracy 0.6951\n",
      "ROC-AUC score: 0.7586\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.72      0.70      4275\n",
      "         1.0       0.70      0.67      0.69      4275\n",
      "\n",
      "    accuracy                           0.70      8550\n",
      "   macro avg       0.70      0.70      0.69      8550\n",
      "weighted avg       0.70      0.70      0.69      8550\n",
      "\n",
      "Finished validating.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'val',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the validation dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished validating.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: test\n",
      "Number of vertices: 113\n",
      "Number of static edges: 2096\n",
      "Number of temporal edges: 15613\n",
      "Number of examples/datapoints: 8278\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the test dataset after training.\n",
      "    Batch 3 / 259: loss 0.5884, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 6 / 259: loss 0.6292, accuracy 0.7500\n",
      "    ROC-AUC score: 0.6314\n",
      "    Batch 9 / 259: loss 0.7483, accuracy 0.6458\n",
      "    ROC-AUC score: 0.4000\n",
      "    Batch 12 / 259: loss 0.6740, accuracy 0.6562\n",
      "    ROC-AUC score: 0.7804\n",
      "    Batch 15 / 259: loss 0.6475, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 18 / 259: loss 0.6728, accuracy 0.6562\n",
      "    ROC-AUC score: 0.7143\n",
      "    Batch 21 / 259: loss 0.6266, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 24 / 259: loss 0.6595, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6292\n",
      "    Batch 27 / 259: loss 0.6528, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7318\n",
      "    Batch 30 / 259: loss 0.6081, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6587\n",
      "    Batch 33 / 259: loss 0.7192, accuracy 0.5729\n",
      "    ROC-AUC score: 0.5668\n",
      "    Batch 36 / 259: loss 0.6188, accuracy 0.7708\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 39 / 259: loss 0.5848, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 42 / 259: loss 0.6749, accuracy 0.7396\n",
      "    ROC-AUC score: 0.6289\n",
      "    Batch 45 / 259: loss 0.6636, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8810\n",
      "    Batch 48 / 259: loss 0.6340, accuracy 0.7604\n",
      "    ROC-AUC score: 0.6377\n",
      "    Batch 51 / 259: loss 0.6109, accuracy 0.7292\n",
      "    ROC-AUC score: 0.6458\n",
      "    Batch 54 / 259: loss 0.6434, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7381\n",
      "    Batch 57 / 259: loss 0.7032, accuracy 0.6458\n",
      "    ROC-AUC score: 0.6917\n",
      "    Batch 60 / 259: loss 0.6349, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7625\n",
      "    Batch 63 / 259: loss 0.6780, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7955\n",
      "    Batch 66 / 259: loss 0.6848, accuracy 0.6979\n",
      "    ROC-AUC score: 0.6016\n",
      "    Batch 69 / 259: loss 0.5784, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8664\n",
      "    Batch 72 / 259: loss 0.6621, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7692\n",
      "    Batch 75 / 259: loss 0.6836, accuracy 0.6979\n",
      "    ROC-AUC score: 0.9023\n",
      "    Batch 78 / 259: loss 0.6455, accuracy 0.6667\n",
      "    ROC-AUC score: 0.5292\n",
      "    Batch 81 / 259: loss 0.7045, accuracy 0.5833\n",
      "    ROC-AUC score: 0.5098\n",
      "    Batch 84 / 259: loss 0.7006, accuracy 0.6146\n",
      "    ROC-AUC score: 0.6375\n",
      "    Batch 87 / 259: loss 0.6189, accuracy 0.6979\n",
      "    ROC-AUC score: 0.6083\n",
      "    Batch 90 / 259: loss 0.6096, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 93 / 259: loss 0.6689, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6471\n",
      "    Batch 96 / 259: loss 0.6284, accuracy 0.7917\n",
      "    ROC-AUC score: 0.6636\n",
      "    Batch 99 / 259: loss 0.6450, accuracy 0.7292\n",
      "    ROC-AUC score: 0.6746\n",
      "    Batch 102 / 259: loss 0.6564, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6510\n",
      "    Batch 105 / 259: loss 0.6162, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 108 / 259: loss 0.6286, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7302\n",
      "    Batch 111 / 259: loss 0.6383, accuracy 0.6458\n",
      "    ROC-AUC score: 0.8292\n",
      "    Batch 114 / 259: loss 0.7119, accuracy 0.6458\n",
      "    ROC-AUC score: 0.5714\n",
      "    Batch 117 / 259: loss 0.5945, accuracy 0.7500\n",
      "    ROC-AUC score: 0.6745\n",
      "    Batch 120 / 259: loss 0.6826, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6923\n",
      "    Batch 123 / 259: loss 0.6337, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7458\n",
      "    Batch 126 / 259: loss 0.6517, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6353\n",
      "    Batch 129 / 259: loss 0.5677, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8937\n",
      "    Batch 132 / 259: loss 0.6729, accuracy 0.6042\n",
      "    ROC-AUC score: 0.7294\n",
      "    Batch 135 / 259: loss 0.7061, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6431\n",
      "    Batch 138 / 259: loss 0.6772, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6786\n",
      "    Batch 141 / 259: loss 0.6603, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 144 / 259: loss 0.6057, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8196\n",
      "    Batch 147 / 259: loss 0.6857, accuracy 0.6667\n",
      "    ROC-AUC score: 0.5516\n",
      "    Batch 150 / 259: loss 0.7376, accuracy 0.6458\n",
      "    ROC-AUC score: 0.6151\n",
      "    Batch 153 / 259: loss 0.6187, accuracy 0.7292\n",
      "    ROC-AUC score: 0.6863\n",
      "    Batch 156 / 259: loss 0.6751, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7692\n",
      "    Batch 159 / 259: loss 0.7162, accuracy 0.6042\n",
      "    ROC-AUC score: 0.7328\n",
      "    Batch 162 / 259: loss 0.7006, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6437\n",
      "    Batch 165 / 259: loss 0.6078, accuracy 0.8125\n",
      "    ROC-AUC score: 0.8826\n",
      "    Batch 168 / 259: loss 0.6950, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6914\n",
      "    Batch 171 / 259: loss 0.6194, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7539\n",
      "    Batch 174 / 259: loss 0.6532, accuracy 0.7292\n",
      "    ROC-AUC score: 0.6964\n",
      "    Batch 177 / 259: loss 0.6320, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6190\n",
      "    Batch 180 / 259: loss 0.7200, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6786\n",
      "    Batch 183 / 259: loss 0.7184, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7316\n",
      "    Batch 186 / 259: loss 0.6684, accuracy 0.7396\n",
      "    ROC-AUC score: 0.5476\n",
      "    Batch 189 / 259: loss 0.6659, accuracy 0.6771\n",
      "    ROC-AUC score: 0.5466\n",
      "    Batch 192 / 259: loss 0.6337, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8250\n",
      "    Batch 195 / 259: loss 0.6028, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7656\n",
      "    Batch 198 / 259: loss 0.6426, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7143\n",
      "    Batch 201 / 259: loss 0.6647, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7692\n",
      "    Batch 204 / 259: loss 0.6793, accuracy 0.6042\n",
      "    ROC-AUC score: 0.6392\n",
      "    Batch 207 / 259: loss 0.6989, accuracy 0.6458\n",
      "    ROC-AUC score: 0.6000\n",
      "    Batch 210 / 259: loss 0.6475, accuracy 0.6979\n",
      "    ROC-AUC score: 0.5754\n",
      "    Batch 213 / 259: loss 0.6773, accuracy 0.6771\n",
      "    ROC-AUC score: 0.5804\n",
      "    Batch 216 / 259: loss 0.7468, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6623\n",
      "    Batch 219 / 259: loss 0.6845, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6094\n",
      "    Batch 222 / 259: loss 0.6591, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7031\n",
      "    Batch 225 / 259: loss 0.6254, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8633\n",
      "    Batch 228 / 259: loss 0.6499, accuracy 0.7188\n",
      "    ROC-AUC score: 0.4336\n",
      "    Batch 231 / 259: loss 0.6898, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7024\n",
      "    Batch 234 / 259: loss 0.7358, accuracy 0.5938\n",
      "    ROC-AUC score: 0.4941\n",
      "    Batch 237 / 259: loss 0.6846, accuracy 0.6562\n",
      "    ROC-AUC score: 0.5159\n",
      "    Batch 240 / 259: loss 0.6200, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8549\n",
      "    Batch 243 / 259: loss 0.6089, accuracy 0.7604\n",
      "    ROC-AUC score: 0.6992\n",
      "    Batch 246 / 259: loss 0.6699, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7287\n",
      "    Batch 249 / 259: loss 0.6896, accuracy 0.6979\n",
      "    ROC-AUC score: 0.6333\n",
      "    Batch 252 / 259: loss 0.6322, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7103\n",
      "    Batch 255 / 259: loss 0.6871, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 258 / 259: loss 0.6870, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7100\n",
      "Loss 0.6585, accuracy 0.7017\n",
      "ROC-AUC score: 0.6952\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.73      0.71      4139\n",
      "         1.0       0.71      0.68      0.69      4139\n",
      "\n",
      "    accuracy                           0.70      8278\n",
      "   macro avg       0.70      0.70      0.70      8278\n",
      "weighted avg       0.70      0.70      0.70      8278\n",
      "\n",
      "Finished testing.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'test',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the test dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished testing.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
