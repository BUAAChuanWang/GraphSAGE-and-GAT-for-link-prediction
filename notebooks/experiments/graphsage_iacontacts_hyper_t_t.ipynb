{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import visdom\n",
    "\n",
    "from datasets import link_prediction\n",
    "from layers import MeanAggregator, LSTMAggregator, MaxPoolAggregator, MeanPoolAggregator\n",
    "import models\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up arguments for datasets, models and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"task\" : \"link_prediction\",\n",
    "    \n",
    "    \"dataset\" : \"IAContactsHypertext\",\n",
    "    \"dataset_path\" : \"/Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"generate_neg_examples\" : True,\n",
    "    \n",
    "    \"duplicate_examples\" : True,\n",
    "    \"repeat_examples\" : True,\n",
    "    \n",
    "    \"self_loop\" : True,\n",
    "    \"normalize_adj\" : False,\n",
    "    \n",
    "    \"cuda\" : \"True\",\n",
    "    \"model\" : \"GraphSAGE\",\n",
    "    \"agg_class\" : \"MaxPoolAggregator\",\n",
    "    \"hidden_dims\" : [64],\n",
    "    \"dropout\" : 0,\n",
    "    \"num_samples\" : -1,\n",
    "    \n",
    "    \"epochs\" : 3,\n",
    "    \"batch_size\" : 32,\n",
    "    \"lr\" : 5e-4,\n",
    "    \"weight_decay\" : 1e-3,\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"visdom\" : True,\n",
    "    \n",
    "    \"load\" : False,\n",
    "    \"save\" : False\n",
    "}\n",
    "config = args\n",
    "config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "\n",
    "if config['cuda'] and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "config['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset, dataloader and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: train\n",
      "Number of vertices: 113\n",
      "Number of static edges: 1010\n",
      "Number of temporal edges: 6245\n",
      "Number of examples/datapoints: 7886\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'train',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "input_dim, output_dim = dataset.get_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE(\n",
      "  (aggregators): ModuleList(\n",
      "    (0): MaxPoolAggregator(\n",
      "      (fc1): Linear(in_features=113, out_features=113, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): MaxPoolAggregator(\n",
      "      (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=226, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agg_class = utils.get_agg_class(config['agg_class'])\n",
    "model = models.GraphSAGE(input_dim, config['hidden_dims'],\n",
    "                         output_dim, config['dropout'],\n",
    "                         agg_class, config['num_samples'],\n",
    "                         config['device'])\n",
    "model.to(config['device'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score for the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset before training.\n",
      "    Batch 1 / 247\n",
      "    Batch 2 / 247\n",
      "    Batch 3 / 247\n",
      "    Batch 4 / 247\n",
      "    Batch 5 / 247\n",
      "    Batch 6 / 247\n",
      "    Batch 7 / 247\n",
      "    Batch 8 / 247\n",
      "    Batch 9 / 247\n",
      "    Batch 10 / 247\n",
      "    Batch 11 / 247\n",
      "    Batch 12 / 247\n",
      "    Batch 13 / 247\n",
      "    Batch 14 / 247\n",
      "    Batch 15 / 247\n",
      "    Batch 16 / 247\n",
      "    Batch 17 / 247\n",
      "    Batch 18 / 247\n",
      "    Batch 19 / 247\n",
      "    Batch 20 / 247\n",
      "    Batch 21 / 247\n",
      "    Batch 22 / 247\n",
      "    Batch 23 / 247\n",
      "    Batch 24 / 247\n",
      "    Batch 25 / 247\n",
      "    Batch 26 / 247\n",
      "    Batch 27 / 247\n",
      "    Batch 28 / 247\n",
      "    Batch 29 / 247\n",
      "    Batch 30 / 247\n",
      "    Batch 31 / 247\n",
      "    Batch 32 / 247\n",
      "    Batch 33 / 247\n",
      "    Batch 34 / 247\n",
      "    Batch 35 / 247\n",
      "    Batch 36 / 247\n",
      "    Batch 37 / 247\n",
      "    Batch 38 / 247\n",
      "    Batch 39 / 247\n",
      "    Batch 40 / 247\n",
      "    Batch 41 / 247\n",
      "    Batch 42 / 247\n",
      "    Batch 43 / 247\n",
      "    Batch 44 / 247\n",
      "    Batch 45 / 247\n",
      "    Batch 46 / 247\n",
      "    Batch 47 / 247\n",
      "    Batch 48 / 247\n",
      "    Batch 49 / 247\n",
      "    Batch 50 / 247\n",
      "    Batch 51 / 247\n",
      "    Batch 52 / 247\n",
      "    Batch 53 / 247\n",
      "    Batch 54 / 247\n",
      "    Batch 55 / 247\n",
      "    Batch 56 / 247\n",
      "    Batch 57 / 247\n",
      "    Batch 58 / 247\n",
      "    Batch 59 / 247\n",
      "    Batch 60 / 247\n",
      "    Batch 61 / 247\n",
      "    Batch 62 / 247\n",
      "    Batch 63 / 247\n",
      "    Batch 64 / 247\n",
      "    Batch 65 / 247\n",
      "    Batch 66 / 247\n",
      "    Batch 67 / 247\n",
      "    Batch 68 / 247\n",
      "    Batch 69 / 247\n",
      "    Batch 70 / 247\n",
      "    Batch 71 / 247\n",
      "    Batch 72 / 247\n",
      "    Batch 73 / 247\n",
      "    Batch 74 / 247\n",
      "    Batch 75 / 247\n",
      "    Batch 76 / 247\n",
      "    Batch 77 / 247\n",
      "    Batch 78 / 247\n",
      "    Batch 79 / 247\n",
      "    Batch 80 / 247\n",
      "    Batch 81 / 247\n",
      "    Batch 82 / 247\n",
      "    Batch 83 / 247\n",
      "    Batch 84 / 247\n",
      "    Batch 85 / 247\n",
      "    Batch 86 / 247\n",
      "    Batch 87 / 247\n",
      "    Batch 88 / 247\n",
      "    Batch 89 / 247\n",
      "    Batch 90 / 247\n",
      "    Batch 91 / 247\n",
      "    Batch 92 / 247\n",
      "    Batch 93 / 247\n",
      "    Batch 94 / 247\n",
      "    Batch 95 / 247\n",
      "    Batch 96 / 247\n",
      "    Batch 97 / 247\n",
      "    Batch 98 / 247\n",
      "    Batch 99 / 247\n",
      "    Batch 100 / 247\n",
      "    Batch 101 / 247\n",
      "    Batch 102 / 247\n",
      "    Batch 103 / 247\n",
      "    Batch 104 / 247\n",
      "    Batch 105 / 247\n",
      "    Batch 106 / 247\n",
      "    Batch 107 / 247\n",
      "    Batch 108 / 247\n",
      "    Batch 109 / 247\n",
      "    Batch 110 / 247\n",
      "    Batch 111 / 247\n",
      "    Batch 112 / 247\n",
      "    Batch 113 / 247\n",
      "    Batch 114 / 247\n",
      "    Batch 115 / 247\n",
      "    Batch 116 / 247\n",
      "    Batch 117 / 247\n",
      "    Batch 118 / 247\n",
      "    Batch 119 / 247\n",
      "    Batch 120 / 247\n",
      "    Batch 121 / 247\n",
      "    Batch 122 / 247\n",
      "    Batch 123 / 247\n",
      "    Batch 124 / 247\n",
      "    Batch 125 / 247\n",
      "    Batch 126 / 247\n",
      "    Batch 127 / 247\n",
      "    Batch 128 / 247\n",
      "    Batch 129 / 247\n",
      "    Batch 130 / 247\n",
      "    Batch 131 / 247\n",
      "    Batch 132 / 247\n",
      "    Batch 133 / 247\n",
      "    Batch 134 / 247\n",
      "    Batch 135 / 247\n",
      "    Batch 136 / 247\n",
      "    Batch 137 / 247\n",
      "    Batch 138 / 247\n",
      "    Batch 139 / 247\n",
      "    Batch 140 / 247\n",
      "    Batch 141 / 247\n",
      "    Batch 142 / 247\n",
      "    Batch 143 / 247\n",
      "    Batch 144 / 247\n",
      "    Batch 145 / 247\n",
      "    Batch 146 / 247\n",
      "    Batch 147 / 247\n",
      "    Batch 148 / 247\n",
      "    Batch 149 / 247\n",
      "    Batch 150 / 247\n",
      "    Batch 151 / 247\n",
      "    Batch 152 / 247\n",
      "    Batch 153 / 247\n",
      "    Batch 154 / 247\n",
      "    Batch 155 / 247\n",
      "    Batch 156 / 247\n",
      "    Batch 157 / 247\n",
      "    Batch 158 / 247\n",
      "    Batch 159 / 247\n",
      "    Batch 160 / 247\n",
      "    Batch 161 / 247\n",
      "    Batch 162 / 247\n",
      "    Batch 163 / 247\n",
      "    Batch 164 / 247\n",
      "    Batch 165 / 247\n",
      "    Batch 166 / 247\n",
      "    Batch 167 / 247\n",
      "    Batch 168 / 247\n",
      "    Batch 169 / 247\n",
      "    Batch 170 / 247\n",
      "    Batch 171 / 247\n",
      "    Batch 172 / 247\n",
      "    Batch 173 / 247\n",
      "    Batch 174 / 247\n",
      "    Batch 175 / 247\n",
      "    Batch 176 / 247\n",
      "    Batch 177 / 247\n",
      "    Batch 178 / 247\n",
      "    Batch 179 / 247\n",
      "    Batch 180 / 247\n",
      "    Batch 181 / 247\n",
      "    Batch 182 / 247\n",
      "    Batch 183 / 247\n",
      "    Batch 184 / 247\n",
      "    Batch 185 / 247\n",
      "    Batch 186 / 247\n",
      "    Batch 187 / 247\n",
      "    Batch 188 / 247\n",
      "    Batch 189 / 247\n",
      "    Batch 190 / 247\n",
      "    Batch 191 / 247\n",
      "    Batch 192 / 247\n",
      "    Batch 193 / 247\n",
      "    Batch 194 / 247\n",
      "    Batch 195 / 247\n",
      "    Batch 196 / 247\n",
      "    Batch 197 / 247\n",
      "    Batch 198 / 247\n",
      "    Batch 199 / 247\n",
      "    Batch 200 / 247\n",
      "    Batch 201 / 247\n",
      "    Batch 202 / 247\n",
      "    Batch 203 / 247\n",
      "    Batch 204 / 247\n",
      "    Batch 205 / 247\n",
      "    Batch 206 / 247\n",
      "    Batch 207 / 247\n",
      "    Batch 208 / 247\n",
      "    Batch 209 / 247\n",
      "    Batch 210 / 247\n",
      "    Batch 211 / 247\n",
      "    Batch 212 / 247\n",
      "    Batch 213 / 247\n",
      "    Batch 214 / 247\n",
      "    Batch 215 / 247\n",
      "    Batch 216 / 247\n",
      "    Batch 217 / 247\n",
      "    Batch 218 / 247\n",
      "    Batch 219 / 247\n",
      "    Batch 220 / 247\n",
      "    Batch 221 / 247\n",
      "    Batch 222 / 247\n",
      "    Batch 223 / 247\n",
      "    Batch 224 / 247\n",
      "    Batch 225 / 247\n",
      "    Batch 226 / 247\n",
      "    Batch 227 / 247\n",
      "    Batch 228 / 247\n",
      "    Batch 229 / 247\n",
      "    Batch 230 / 247\n",
      "    Batch 231 / 247\n",
      "    Batch 232 / 247\n",
      "    Batch 233 / 247\n",
      "    Batch 234 / 247\n",
      "    Batch 235 / 247\n",
      "    Batch 236 / 247\n",
      "    Batch 237 / 247\n",
      "    Batch 238 / 247\n",
      "    Batch 239 / 247\n",
      "    Batch 240 / 247\n",
      "    Batch 241 / 247\n",
      "    Batch 242 / 247\n",
      "    Batch 243 / 247\n",
      "    Batch 244 / 247\n",
      "    Batch 245 / 247\n",
      "    Batch 246 / 247\n",
      "    Batch 247 / 247\n",
      "ROC-AUC score: 0.5091\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset before training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Training.\n",
      "Epoch 1 / 3\n",
      "    Batch 3 / 247: loss 0.6930\n",
      "    ROC-AUC score: 0.6250\n",
      "    Batch 6 / 247: loss 0.6934\n",
      "    ROC-AUC score: 0.4023\n",
      "    Batch 9 / 247: loss 0.6916\n",
      "    ROC-AUC score: 0.6914\n",
      "    Batch 12 / 247: loss 0.6919\n",
      "    ROC-AUC score: 0.6625\n",
      "    Batch 15 / 247: loss 0.6927\n",
      "    ROC-AUC score: 0.5750\n",
      "    Batch 18 / 247: loss 0.6910\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 21 / 247: loss 0.6919\n",
      "    ROC-AUC score: 0.6627\n",
      "    Batch 24 / 247: loss 0.6886\n",
      "    ROC-AUC score: 0.7571\n",
      "    Batch 27 / 247: loss 0.6876\n",
      "    ROC-AUC score: 0.6549\n",
      "    Batch 30 / 247: loss 0.6807\n",
      "    ROC-AUC score: 0.8545\n",
      "    Batch 33 / 247: loss 0.6872\n",
      "    ROC-AUC score: 0.7852\n",
      "    Batch 36 / 247: loss 0.6816\n",
      "    ROC-AUC score: 0.8175\n",
      "    Batch 39 / 247: loss 0.6836\n",
      "    ROC-AUC score: 0.5714\n",
      "    Batch 42 / 247: loss 0.6755\n",
      "    ROC-AUC score: 0.7344\n",
      "    Batch 45 / 247: loss 0.6826\n",
      "    ROC-AUC score: 0.5569\n",
      "    Batch 48 / 247: loss 0.6835\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 51 / 247: loss 0.6705\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 54 / 247: loss 0.6690\n",
      "    ROC-AUC score: 0.6510\n",
      "    Batch 57 / 247: loss 0.6705\n",
      "    ROC-AUC score: 0.8409\n",
      "    Batch 60 / 247: loss 0.6758\n",
      "    ROC-AUC score: 0.6349\n",
      "    Batch 63 / 247: loss 0.6680\n",
      "    ROC-AUC score: 0.7294\n",
      "    Batch 66 / 247: loss 0.6530\n",
      "    ROC-AUC score: 0.7013\n",
      "    Batch 69 / 247: loss 0.6699\n",
      "    ROC-AUC score: 0.6587\n",
      "    Batch 72 / 247: loss 0.6845\n",
      "    ROC-AUC score: 0.5913\n",
      "    Batch 75 / 247: loss 0.6517\n",
      "    ROC-AUC score: 0.4627\n",
      "    Batch 78 / 247: loss 0.6639\n",
      "    ROC-AUC score: 0.6719\n",
      "    Batch 81 / 247: loss 0.6676\n",
      "    ROC-AUC score: 0.7103\n",
      "    Batch 84 / 247: loss 0.6423\n",
      "    ROC-AUC score: 0.8314\n",
      "    Batch 87 / 247: loss 0.6657\n",
      "    ROC-AUC score: 0.7578\n",
      "    Batch 90 / 247: loss 0.6538\n",
      "    ROC-AUC score: 0.8052\n",
      "    Batch 93 / 247: loss 0.6556\n",
      "    ROC-AUC score: 0.7571\n",
      "    Batch 96 / 247: loss 0.6507\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 99 / 247: loss 0.6349\n",
      "    ROC-AUC score: 0.8789\n",
      "    Batch 102 / 247: loss 0.6100\n",
      "    ROC-AUC score: 0.7652\n",
      "    Batch 105 / 247: loss 0.6569\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 108 / 247: loss 0.6087\n",
      "    ROC-AUC score: 0.8208\n",
      "    Batch 111 / 247: loss 0.6475\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 114 / 247: loss 0.6180\n",
      "    ROC-AUC score: 0.6865\n",
      "    Batch 117 / 247: loss 0.6308\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 120 / 247: loss 0.6236\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 123 / 247: loss 0.6657\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 126 / 247: loss 0.6287\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 129 / 247: loss 0.6327\n",
      "    ROC-AUC score: 0.8941\n",
      "    Batch 132 / 247: loss 0.6447\n",
      "    ROC-AUC score: 0.8831\n",
      "    Batch 135 / 247: loss 0.5953\n",
      "    ROC-AUC score: 0.6955\n",
      "    Batch 138 / 247: loss 0.6707\n",
      "    ROC-AUC score: 0.6941\n",
      "    Batch 141 / 247: loss 0.6330\n",
      "    ROC-AUC score: 0.8056\n",
      "    Batch 144 / 247: loss 0.6201\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 147 / 247: loss 0.6527\n",
      "    ROC-AUC score: 0.6548\n",
      "    Batch 150 / 247: loss 0.6253\n",
      "    ROC-AUC score: 0.9127\n",
      "    Batch 153 / 247: loss 0.6282\n",
      "    ROC-AUC score: 0.8988\n",
      "    Batch 156 / 247: loss 0.5999\n",
      "    ROC-AUC score: 0.8672\n",
      "    Batch 159 / 247: loss 0.6386\n",
      "    ROC-AUC score: 0.7530\n",
      "    Batch 162 / 247: loss 0.6219\n",
      "    ROC-AUC score: 0.8961\n",
      "    Batch 165 / 247: loss 0.6681\n",
      "    ROC-AUC score: 0.8042\n",
      "    Batch 168 / 247: loss 0.6087\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 171 / 247: loss 0.6116\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 174 / 247: loss 0.6357\n",
      "    ROC-AUC score: 0.8672\n",
      "    Batch 177 / 247: loss 0.5678\n",
      "    ROC-AUC score: 0.7917\n",
      "    Batch 180 / 247: loss 0.5909\n",
      "    ROC-AUC score: 0.8555\n",
      "    Batch 183 / 247: loss 0.6189\n",
      "    ROC-AUC score: 0.7692\n",
      "    Batch 186 / 247: loss 0.6170\n",
      "    ROC-AUC score: 0.9137\n",
      "    Batch 189 / 247: loss 0.5802\n",
      "    ROC-AUC score: 0.7458\n",
      "    Batch 192 / 247: loss 0.5842\n",
      "    ROC-AUC score: 0.8866\n",
      "    Batch 195 / 247: loss 0.6191\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 198 / 247: loss 0.5919\n",
      "    ROC-AUC score: 0.8532\n",
      "    Batch 201 / 247: loss 0.6462\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 204 / 247: loss 0.5737\n",
      "    ROC-AUC score: 0.9804\n",
      "    Batch 207 / 247: loss 0.5744\n",
      "    ROC-AUC score: 0.8542\n",
      "    Batch 210 / 247: loss 0.6006\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 213 / 247: loss 0.5987\n",
      "    ROC-AUC score: 0.7103\n",
      "    Batch 216 / 247: loss 0.5527\n",
      "    ROC-AUC score: 0.8968\n",
      "    Batch 219 / 247: loss 0.6123\n",
      "    ROC-AUC score: 0.7098\n",
      "    Batch 222 / 247: loss 0.6385\n",
      "    ROC-AUC score: 0.8477\n",
      "    Batch 225 / 247: loss 0.5524\n",
      "    ROC-AUC score: 0.8492\n",
      "    Batch 228 / 247: loss 0.5732\n",
      "    ROC-AUC score: 0.9414\n",
      "    Batch 231 / 247: loss 0.6542\n",
      "    ROC-AUC score: 0.8452\n",
      "    Batch 234 / 247: loss 0.5549\n",
      "    ROC-AUC score: 0.9137\n",
      "    Batch 237 / 247: loss 0.6284\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 240 / 247: loss 0.5667\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 243 / 247: loss 0.5650\n",
      "    ROC-AUC score: 0.9333\n",
      "    Batch 246 / 247: loss 0.6193\n",
      "    ROC-AUC score: 0.8039\n",
      "Epoch 2 / 3\n",
      "    Batch 3 / 247: loss 0.5415\n",
      "    ROC-AUC score: 0.9490\n",
      "    Batch 6 / 247: loss 0.5938\n",
      "    ROC-AUC score: 0.7409\n",
      "    Batch 9 / 247: loss 0.5593\n",
      "    ROC-AUC score: 0.8831\n",
      "    Batch 12 / 247: loss 0.5365\n",
      "    ROC-AUC score: 0.9667\n",
      "    Batch 15 / 247: loss 0.5972\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 18 / 247: loss 0.5972\n",
      "    ROC-AUC score: 0.6510\n",
      "    Batch 21 / 247: loss 0.6165\n",
      "    ROC-AUC score: 0.7835\n",
      "    Batch 24 / 247: loss 0.5901\n",
      "    ROC-AUC score: 0.9608\n",
      "    Batch 27 / 247: loss 0.5435\n",
      "    ROC-AUC score: 0.8138\n",
      "    Batch 30 / 247: loss 0.6014\n",
      "    ROC-AUC score: 0.8516\n",
      "    Batch 33 / 247: loss 0.5587\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 36 / 247: loss 0.5717\n",
      "    ROC-AUC score: 0.6627\n",
      "    Batch 39 / 247: loss 0.6238\n",
      "    ROC-AUC score: 0.7611\n",
      "    Batch 42 / 247: loss 0.4972\n",
      "    ROC-AUC score: 0.9221\n",
      "    Batch 45 / 247: loss 0.6171\n",
      "    ROC-AUC score: 0.6055\n",
      "    Batch 48 / 247: loss 0.6258\n",
      "    ROC-AUC score: 0.7409\n",
      "    Batch 51 / 247: loss 0.5717\n",
      "    ROC-AUC score: 0.7879\n",
      "    Batch 54 / 247: loss 0.5635\n",
      "    ROC-AUC score: 0.8988\n",
      "    Batch 57 / 247: loss 0.5332\n",
      "    ROC-AUC score: 0.7222\n",
      "    Batch 60 / 247: loss 0.5477\n",
      "    ROC-AUC score: 0.9458\n",
      "    Batch 63 / 247: loss 0.6343\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 66 / 247: loss 0.5588\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 69 / 247: loss 0.6504\n",
      "    ROC-AUC score: 0.7421\n",
      "    Batch 72 / 247: loss 0.5894\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 75 / 247: loss 0.5647\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 78 / 247: loss 0.5616\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 81 / 247: loss 0.5943\n",
      "    ROC-AUC score: 0.7814\n",
      "    Batch 84 / 247: loss 0.5637\n",
      "    ROC-AUC score: 0.9045\n",
      "    Batch 87 / 247: loss 0.6067\n",
      "    ROC-AUC score: 0.8824\n",
      "    Batch 90 / 247: loss 0.5714\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 93 / 247: loss 0.5775\n",
      "    ROC-AUC score: 0.7611\n",
      "    Batch 96 / 247: loss 0.5988\n",
      "    ROC-AUC score: 0.8980\n",
      "    Batch 99 / 247: loss 0.6171\n",
      "    ROC-AUC score: 0.9004\n",
      "    Batch 102 / 247: loss 0.5574\n",
      "    ROC-AUC score: 0.7930\n",
      "    Batch 105 / 247: loss 0.5404\n",
      "    ROC-AUC score: 0.8135\n",
      "    Batch 108 / 247: loss 0.6305\n",
      "    ROC-AUC score: 0.7137\n",
      "    Batch 111 / 247: loss 0.6190\n",
      "    ROC-AUC score: 0.7852\n",
      "    Batch 114 / 247: loss 0.5546\n",
      "    ROC-AUC score: 0.8583\n",
      "    Batch 117 / 247: loss 0.6189\n",
      "    ROC-AUC score: 0.7804\n",
      "    Batch 120 / 247: loss 0.6209\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 123 / 247: loss 0.5834\n",
      "    ROC-AUC score: 0.8373\n",
      "    Batch 126 / 247: loss 0.6280\n",
      "    ROC-AUC score: 0.8009\n",
      "    Batch 129 / 247: loss 0.5478\n",
      "    ROC-AUC score: 0.8381\n",
      "    Batch 132 / 247: loss 0.5516\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 135 / 247: loss 0.6093\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 138 / 247: loss 0.6545\n",
      "    ROC-AUC score: 0.8196\n",
      "    Batch 141 / 247: loss 0.5343\n",
      "    ROC-AUC score: 0.9008\n",
      "    Batch 144 / 247: loss 0.5694\n",
      "    ROC-AUC score: 0.8875\n",
      "    Batch 147 / 247: loss 0.5082\n",
      "    ROC-AUC score: 0.9000\n",
      "    Batch 150 / 247: loss 0.6047\n",
      "    ROC-AUC score: 0.9490\n",
      "    Batch 153 / 247: loss 0.5469\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 156 / 247: loss 0.5474\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 159 / 247: loss 0.5576\n",
      "    ROC-AUC score: 0.8008\n",
      "    Batch 162 / 247: loss 0.5998\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 165 / 247: loss 0.5881\n",
      "    ROC-AUC score: 0.9484\n",
      "    Batch 168 / 247: loss 0.5294\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 171 / 247: loss 0.5655\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 174 / 247: loss 0.5354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ROC-AUC score: 0.8727\n",
      "    Batch 177 / 247: loss 0.5589\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 180 / 247: loss 0.5123\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 183 / 247: loss 0.5860\n",
      "    ROC-AUC score: 0.6706\n",
      "    Batch 186 / 247: loss 0.5156\n",
      "    ROC-AUC score: 0.7257\n",
      "    Batch 189 / 247: loss 0.6402\n",
      "    ROC-AUC score: 0.8594\n",
      "    Batch 192 / 247: loss 0.4666\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 195 / 247: loss 0.5583\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 198 / 247: loss 0.6135\n",
      "    ROC-AUC score: 0.7381\n",
      "    Batch 201 / 247: loss 0.5825\n",
      "    ROC-AUC score: 0.7247\n",
      "    Batch 204 / 247: loss 0.6371\n",
      "    ROC-AUC score: 0.7302\n",
      "    Batch 207 / 247: loss 0.6053\n",
      "    ROC-AUC score: 0.8611\n",
      "    Batch 210 / 247: loss 0.5424\n",
      "    ROC-AUC score: 0.8078\n",
      "    Batch 213 / 247: loss 0.5264\n",
      "    ROC-AUC score: 0.8611\n",
      "    Batch 216 / 247: loss 0.5407\n",
      "    ROC-AUC score: 0.9206\n",
      "    Batch 219 / 247: loss 0.5119\n",
      "    ROC-AUC score: 0.8438\n",
      "    Batch 222 / 247: loss 0.5678\n",
      "    ROC-AUC score: 0.6392\n",
      "    Batch 225 / 247: loss 0.5558\n",
      "    ROC-AUC score: 0.8770\n",
      "    Batch 228 / 247: loss 0.5670\n",
      "    ROC-AUC score: 0.9255\n",
      "    Batch 231 / 247: loss 0.5916\n",
      "    ROC-AUC score: 0.7571\n",
      "    Batch 234 / 247: loss 0.4954\n",
      "    ROC-AUC score: 0.8633\n",
      "    Batch 237 / 247: loss 0.5279\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 240 / 247: loss 0.5626\n",
      "    ROC-AUC score: 0.7812\n",
      "    Batch 243 / 247: loss 0.5640\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 246 / 247: loss 0.5594\n",
      "    ROC-AUC score: 0.8275\n",
      "Epoch 3 / 3\n",
      "    Batch 3 / 247: loss 0.6318\n",
      "    ROC-AUC score: 0.6353\n",
      "    Batch 6 / 247: loss 0.5369\n",
      "    ROC-AUC score: 0.8555\n",
      "    Batch 9 / 247: loss 0.4412\n",
      "    ROC-AUC score: 0.8988\n",
      "    Batch 12 / 247: loss 0.6640\n",
      "    ROC-AUC score: 0.6863\n",
      "    Batch 15 / 247: loss 0.5793\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 18 / 247: loss 0.5274\n",
      "    ROC-AUC score: 0.8918\n",
      "    Batch 21 / 247: loss 0.4709\n",
      "    ROC-AUC score: 0.9180\n",
      "    Batch 24 / 247: loss 0.5703\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 27 / 247: loss 0.5557\n",
      "    ROC-AUC score: 0.8770\n",
      "    Batch 30 / 247: loss 0.5224\n",
      "    ROC-AUC score: 0.7611\n",
      "    Batch 33 / 247: loss 0.6166\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 36 / 247: loss 0.6003\n",
      "    ROC-AUC score: 0.6923\n",
      "    Batch 39 / 247: loss 0.5089\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 42 / 247: loss 0.5571\n",
      "    ROC-AUC score: 0.8889\n",
      "    Batch 45 / 247: loss 0.5711\n",
      "    ROC-AUC score: 0.5922\n",
      "    Batch 48 / 247: loss 0.5922\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 51 / 247: loss 0.6411\n",
      "    ROC-AUC score: 0.8750\n",
      "    Batch 54 / 247: loss 0.5052\n",
      "    ROC-AUC score: 0.8744\n",
      "    Batch 57 / 247: loss 0.6179\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 60 / 247: loss 0.6010\n",
      "    ROC-AUC score: 0.7083\n",
      "    Batch 63 / 247: loss 0.5756\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 66 / 247: loss 0.5555\n",
      "    ROC-AUC score: 0.8867\n",
      "    Batch 69 / 247: loss 0.6139\n",
      "    ROC-AUC score: 0.8398\n",
      "    Batch 72 / 247: loss 0.5503\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 75 / 247: loss 0.5693\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 78 / 247: loss 0.5015\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 81 / 247: loss 0.5451\n",
      "    ROC-AUC score: 0.9264\n",
      "    Batch 84 / 247: loss 0.5865\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 87 / 247: loss 0.5926\n",
      "    ROC-AUC score: 0.6510\n",
      "    Batch 90 / 247: loss 0.6127\n",
      "    ROC-AUC score: 0.7109\n",
      "    Batch 93 / 247: loss 0.6250\n",
      "    ROC-AUC score: 0.6468\n",
      "    Batch 96 / 247: loss 0.5957\n",
      "    ROC-AUC score: 0.8139\n",
      "    Batch 99 / 247: loss 0.5093\n",
      "    ROC-AUC score: 0.8532\n",
      "    Batch 102 / 247: loss 0.4927\n",
      "    ROC-AUC score: 0.7488\n",
      "    Batch 105 / 247: loss 0.5456\n",
      "    ROC-AUC score: 0.8947\n",
      "    Batch 108 / 247: loss 0.5515\n",
      "    ROC-AUC score: 0.9762\n",
      "    Batch 111 / 247: loss 0.5556\n",
      "    ROC-AUC score: 0.8219\n",
      "    Batch 114 / 247: loss 0.5035\n",
      "    ROC-AUC score: 0.8225\n",
      "    Batch 117 / 247: loss 0.6186\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 120 / 247: loss 0.5455\n",
      "    ROC-AUC score: 0.8792\n",
      "    Batch 123 / 247: loss 0.5121\n",
      "    ROC-AUC score: 0.8313\n",
      "    Batch 126 / 247: loss 0.5055\n",
      "    ROC-AUC score: 0.6758\n",
      "    Batch 129 / 247: loss 0.6012\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 132 / 247: loss 0.5271\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 135 / 247: loss 0.5934\n",
      "    ROC-AUC score: 0.8664\n",
      "    Batch 138 / 247: loss 0.5975\n",
      "    ROC-AUC score: 0.8831\n",
      "    Batch 141 / 247: loss 0.5863\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 144 / 247: loss 0.5443\n",
      "    ROC-AUC score: 0.9023\n",
      "    Batch 147 / 247: loss 0.5386\n",
      "    ROC-AUC score: 0.9137\n",
      "    Batch 150 / 247: loss 0.6185\n",
      "    ROC-AUC score: 0.6825\n",
      "    Batch 153 / 247: loss 0.5261\n",
      "    ROC-AUC score: 0.9412\n",
      "    Batch 156 / 247: loss 0.6279\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 159 / 247: loss 0.5141\n",
      "    ROC-AUC score: 0.9255\n",
      "    Batch 162 / 247: loss 0.5368\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 165 / 247: loss 0.6145\n",
      "    ROC-AUC score: 0.8828\n",
      "    Batch 168 / 247: loss 0.5606\n",
      "    ROC-AUC score: 0.7085\n",
      "    Batch 171 / 247: loss 0.5423\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 174 / 247: loss 0.5595\n",
      "    ROC-AUC score: 0.8438\n",
      "    Batch 177 / 247: loss 0.6459\n",
      "    ROC-AUC score: 0.6353\n",
      "    Batch 180 / 247: loss 0.5620\n",
      "    ROC-AUC score: 0.7585\n",
      "    Batch 183 / 247: loss 0.5197\n",
      "    ROC-AUC score: 0.8078\n",
      "    Batch 186 / 247: loss 0.5790\n",
      "    ROC-AUC score: 0.8770\n",
      "    Batch 189 / 247: loss 0.5389\n",
      "    ROC-AUC score: 0.8164\n",
      "    Batch 192 / 247: loss 0.5575\n",
      "    ROC-AUC score: 0.8438\n",
      "    Batch 195 / 247: loss 0.5932\n",
      "    ROC-AUC score: 0.7611\n",
      "    Batch 198 / 247: loss 0.5577\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 201 / 247: loss 0.5323\n",
      "    ROC-AUC score: 0.6833\n",
      "    Batch 204 / 247: loss 0.5380\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 207 / 247: loss 0.5591\n",
      "    ROC-AUC score: 0.8770\n",
      "    Batch 210 / 247: loss 0.5242\n",
      "    ROC-AUC score: 0.8196\n",
      "    Batch 213 / 247: loss 0.5628\n",
      "    ROC-AUC score: 0.7540\n",
      "    Batch 216 / 247: loss 0.5782\n",
      "    ROC-AUC score: 0.7344\n",
      "    Batch 219 / 247: loss 0.5400\n",
      "    ROC-AUC score: 0.8373\n",
      "    Batch 222 / 247: loss 0.4622\n",
      "    ROC-AUC score: 0.9098\n",
      "    Batch 225 / 247: loss 0.5894\n",
      "    ROC-AUC score: 0.7935\n",
      "    Batch 228 / 247: loss 0.5977\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 231 / 247: loss 0.6190\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 234 / 247: loss 0.5446\n",
      "    ROC-AUC score: 0.8355\n",
      "    Batch 237 / 247: loss 0.5753\n",
      "    ROC-AUC score: 0.8543\n",
      "    Batch 240 / 247: loss 0.4765\n",
      "    ROC-AUC score: 0.9569\n",
      "    Batch 243 / 247: loss 0.5564\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 246 / 247: loss 0.6341\n",
      "    ROC-AUC score: 0.6471\n",
      "Finished training.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    use_visdom = config['visdom']\n",
    "    if use_visdom:\n",
    "        vis = visdom.Visdom()\n",
    "        loss_window = None\n",
    "    criterion = utils.get_criterion(config['task'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                           weight_decay=config['weight_decay'])\n",
    "    epochs = config['epochs']\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.8)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200, 400], gamma=0.7)\n",
    "    model.train()\n",
    "    print('--------------------------------')\n",
    "    print('Training.')\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "        running_loss = 0.0\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            loss = criterion(scores, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "            if (idx + 1) % stats_per_batch == 0:\n",
    "                running_loss /= stats_per_batch\n",
    "                print('    Batch {} / {}: loss {:.4f}'.format(\n",
    "                    idx+1, num_batches, running_loss))\n",
    "                if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "                    area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "                    print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "                running_loss = 0.0\n",
    "                num_correct, num_examples = 0, 0\n",
    "            if use_visdom:\n",
    "                if loss_window is None:\n",
    "                    loss_window = vis.line(\n",
    "                        Y=[loss.item()],\n",
    "                        X=[epoch*num_batches+idx],\n",
    "                        opts=dict(xlabel='batch', ylabel='Loss', title='Training Loss', legend=['Loss']))\n",
    "                else:\n",
    "                    vis.line(\n",
    "                        [loss.item()],\n",
    "                        [epoch*num_batches+idx],\n",
    "                        win=loss_window,\n",
    "                        update='append')\n",
    "            scheduler.step()\n",
    "    if use_visdom:\n",
    "        vis.close(win=loss_window)\n",
    "    print('Finished training.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['load']:\n",
    "    if config['save']:\n",
    "        print('--------------------------------')\n",
    "        directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                'trained_models')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        fname = utils.get_fname(config)\n",
    "        path = os.path.join(directory, fname)\n",
    "        print('Saving model at {}'.format(path))\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print('Finished saving model.')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset after training.\n",
      "    Batch 1 / 247\n",
      "    Batch 2 / 247\n",
      "    Batch 3 / 247\n",
      "    Batch 4 / 247\n",
      "    Batch 5 / 247\n",
      "    Batch 6 / 247\n",
      "    Batch 7 / 247\n",
      "    Batch 8 / 247\n",
      "    Batch 9 / 247\n",
      "    Batch 10 / 247\n",
      "    Batch 11 / 247\n",
      "    Batch 12 / 247\n",
      "    Batch 13 / 247\n",
      "    Batch 14 / 247\n",
      "    Batch 15 / 247\n",
      "    Batch 16 / 247\n",
      "    Batch 17 / 247\n",
      "    Batch 18 / 247\n",
      "    Batch 19 / 247\n",
      "    Batch 20 / 247\n",
      "    Batch 21 / 247\n",
      "    Batch 22 / 247\n",
      "    Batch 23 / 247\n",
      "    Batch 24 / 247\n",
      "    Batch 25 / 247\n",
      "    Batch 26 / 247\n",
      "    Batch 27 / 247\n",
      "    Batch 28 / 247\n",
      "    Batch 29 / 247\n",
      "    Batch 30 / 247\n",
      "    Batch 31 / 247\n",
      "    Batch 32 / 247\n",
      "    Batch 33 / 247\n",
      "    Batch 34 / 247\n",
      "    Batch 35 / 247\n",
      "    Batch 36 / 247\n",
      "    Batch 37 / 247\n",
      "    Batch 38 / 247\n",
      "    Batch 39 / 247\n",
      "    Batch 40 / 247\n",
      "    Batch 41 / 247\n",
      "    Batch 42 / 247\n",
      "    Batch 43 / 247\n",
      "    Batch 44 / 247\n",
      "    Batch 45 / 247\n",
      "    Batch 46 / 247\n",
      "    Batch 47 / 247\n",
      "    Batch 48 / 247\n",
      "    Batch 49 / 247\n",
      "    Batch 50 / 247\n",
      "    Batch 51 / 247\n",
      "    Batch 52 / 247\n",
      "    Batch 53 / 247\n",
      "    Batch 54 / 247\n",
      "    Batch 55 / 247\n",
      "    Batch 56 / 247\n",
      "    Batch 57 / 247\n",
      "    Batch 58 / 247\n",
      "    Batch 59 / 247\n",
      "    Batch 60 / 247\n",
      "    Batch 61 / 247\n",
      "    Batch 62 / 247\n",
      "    Batch 63 / 247\n",
      "    Batch 64 / 247\n",
      "    Batch 65 / 247\n",
      "    Batch 66 / 247\n",
      "    Batch 67 / 247\n",
      "    Batch 68 / 247\n",
      "    Batch 69 / 247\n",
      "    Batch 70 / 247\n",
      "    Batch 71 / 247\n",
      "    Batch 72 / 247\n",
      "    Batch 73 / 247\n",
      "    Batch 74 / 247\n",
      "    Batch 75 / 247\n",
      "    Batch 76 / 247\n",
      "    Batch 77 / 247\n",
      "    Batch 78 / 247\n",
      "    Batch 79 / 247\n",
      "    Batch 80 / 247\n",
      "    Batch 81 / 247\n",
      "    Batch 82 / 247\n",
      "    Batch 83 / 247\n",
      "    Batch 84 / 247\n",
      "    Batch 85 / 247\n",
      "    Batch 86 / 247\n",
      "    Batch 87 / 247\n",
      "    Batch 88 / 247\n",
      "    Batch 89 / 247\n",
      "    Batch 90 / 247\n",
      "    Batch 91 / 247\n",
      "    Batch 92 / 247\n",
      "    Batch 93 / 247\n",
      "    Batch 94 / 247\n",
      "    Batch 95 / 247\n",
      "    Batch 96 / 247\n",
      "    Batch 97 / 247\n",
      "    Batch 98 / 247\n",
      "    Batch 99 / 247\n",
      "    Batch 100 / 247\n",
      "    Batch 101 / 247\n",
      "    Batch 102 / 247\n",
      "    Batch 103 / 247\n",
      "    Batch 104 / 247\n",
      "    Batch 105 / 247\n",
      "    Batch 106 / 247\n",
      "    Batch 107 / 247\n",
      "    Batch 108 / 247\n",
      "    Batch 109 / 247\n",
      "    Batch 110 / 247\n",
      "    Batch 111 / 247\n",
      "    Batch 112 / 247\n",
      "    Batch 113 / 247\n",
      "    Batch 114 / 247\n",
      "    Batch 115 / 247\n",
      "    Batch 116 / 247\n",
      "    Batch 117 / 247\n",
      "    Batch 118 / 247\n",
      "    Batch 119 / 247\n",
      "    Batch 120 / 247\n",
      "    Batch 121 / 247\n",
      "    Batch 122 / 247\n",
      "    Batch 123 / 247\n",
      "    Batch 124 / 247\n",
      "    Batch 125 / 247\n",
      "    Batch 126 / 247\n",
      "    Batch 127 / 247\n",
      "    Batch 128 / 247\n",
      "    Batch 129 / 247\n",
      "    Batch 130 / 247\n",
      "    Batch 131 / 247\n",
      "    Batch 132 / 247\n",
      "    Batch 133 / 247\n",
      "    Batch 134 / 247\n",
      "    Batch 135 / 247\n",
      "    Batch 136 / 247\n",
      "    Batch 137 / 247\n",
      "    Batch 138 / 247\n",
      "    Batch 139 / 247\n",
      "    Batch 140 / 247\n",
      "    Batch 141 / 247\n",
      "    Batch 142 / 247\n",
      "    Batch 143 / 247\n",
      "    Batch 144 / 247\n",
      "    Batch 145 / 247\n",
      "    Batch 146 / 247\n",
      "    Batch 147 / 247\n",
      "    Batch 148 / 247\n",
      "    Batch 149 / 247\n",
      "    Batch 150 / 247\n",
      "    Batch 151 / 247\n",
      "    Batch 152 / 247\n",
      "    Batch 153 / 247\n",
      "    Batch 154 / 247\n",
      "    Batch 155 / 247\n",
      "    Batch 156 / 247\n",
      "    Batch 157 / 247\n",
      "    Batch 158 / 247\n",
      "    Batch 159 / 247\n",
      "    Batch 160 / 247\n",
      "    Batch 161 / 247\n",
      "    Batch 162 / 247\n",
      "    Batch 163 / 247\n",
      "    Batch 164 / 247\n",
      "    Batch 165 / 247\n",
      "    Batch 166 / 247\n",
      "    Batch 167 / 247\n",
      "    Batch 168 / 247\n",
      "    Batch 169 / 247\n",
      "    Batch 170 / 247\n",
      "    Batch 171 / 247\n",
      "    Batch 172 / 247\n",
      "    Batch 173 / 247\n",
      "    Batch 174 / 247\n",
      "    Batch 175 / 247\n",
      "    Batch 176 / 247\n",
      "    Batch 177 / 247\n",
      "    Batch 178 / 247\n",
      "    Batch 179 / 247\n",
      "    Batch 180 / 247\n",
      "    Batch 181 / 247\n",
      "    Batch 182 / 247\n",
      "    Batch 183 / 247\n",
      "    Batch 184 / 247\n",
      "    Batch 185 / 247\n",
      "    Batch 186 / 247\n",
      "    Batch 187 / 247\n",
      "    Batch 188 / 247\n",
      "    Batch 189 / 247\n",
      "    Batch 190 / 247\n",
      "    Batch 191 / 247\n",
      "    Batch 192 / 247\n",
      "    Batch 193 / 247\n",
      "    Batch 194 / 247\n",
      "    Batch 195 / 247\n",
      "    Batch 196 / 247\n",
      "    Batch 197 / 247\n",
      "    Batch 198 / 247\n",
      "    Batch 199 / 247\n",
      "    Batch 200 / 247\n",
      "    Batch 201 / 247\n",
      "    Batch 202 / 247\n",
      "    Batch 203 / 247\n",
      "    Batch 204 / 247\n",
      "    Batch 205 / 247\n",
      "    Batch 206 / 247\n",
      "    Batch 207 / 247\n",
      "    Batch 208 / 247\n",
      "    Batch 209 / 247\n",
      "    Batch 210 / 247\n",
      "    Batch 211 / 247\n",
      "    Batch 212 / 247\n",
      "    Batch 213 / 247\n",
      "    Batch 214 / 247\n",
      "    Batch 215 / 247\n",
      "    Batch 216 / 247\n",
      "    Batch 217 / 247\n",
      "    Batch 218 / 247\n",
      "    Batch 219 / 247\n",
      "    Batch 220 / 247\n",
      "    Batch 221 / 247\n",
      "    Batch 222 / 247\n",
      "    Batch 223 / 247\n",
      "    Batch 224 / 247\n",
      "    Batch 225 / 247\n",
      "    Batch 226 / 247\n",
      "    Batch 227 / 247\n",
      "    Batch 228 / 247\n",
      "    Batch 229 / 247\n",
      "    Batch 230 / 247\n",
      "    Batch 231 / 247\n",
      "    Batch 232 / 247\n",
      "    Batch 233 / 247\n",
      "    Batch 234 / 247\n",
      "    Batch 235 / 247\n",
      "    Batch 236 / 247\n",
      "    Batch 237 / 247\n",
      "    Batch 238 / 247\n",
      "    Batch 239 / 247\n",
      "    Batch 240 / 247\n",
      "    Batch 241 / 247\n",
      "    Batch 242 / 247\n",
      "    Batch 243 / 247\n",
      "    Batch 244 / 247\n",
      "    Batch 245 / 247\n",
      "    Batch 246 / 247\n",
      "    Batch 247 / 247\n",
      "ROC-AUC score: 0.8278\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset after training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the true positive rate and true negative rate vs threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXxU1fnH8c+TyU5CAllYEiDsqxggILuoqICiVVFBRbQqrVVbi7Vqa63S6s+61KUu1K3uiAutqCgqiiyCEPYdEggQdhISSELWOb8/7oBDSGCSTHIzM8/79corM/eeufc7ITy5c+6954gxBqWUUr4vyO4ASimlvEMLulJK+Qkt6Eop5Se0oCullJ/Qgq6UUn5CC7pSSvkJLehKKeUntKArVY9EZISIZDfQvrJEZGQtX2tEpFM1624SkYV1S6caghb0ACciBW5fThE55vb8ehF5WETKXM/zRORHERnkeu1NIlLhWndERFaLyKUe7PNPIvJYFcuO77fYbbsFIrLe1caIyFoRCXJ73d9F5E3X4xRXm+OvyxKR+736Azv1vXzptr8yESl1ez6tPvetVGVa0AOcMSbq+BewExjrtuw9V7MZrvUJwEJgpoiIa91i17pY4CXgAxGJPcNuxwCzK+V4zC3Hr49v1/XV061pa2D8GbYf69rOOOAvInLhGdrXmjFmtFvu94An3HL/uqbbExGH91OqQKEFXXnMGFMGvAW0BOIqrXMC7wBNgM7VbUNEmgFdgMW1jPEE8IiIBHuQNx1YD6RWk2WaiDxVadmnIjLF9fg+EdktIkdFZLOIXFDLzIjIPSJyQET2isjNbsvfFJGXRWS2iBQC54lImIg8JSI7RWS/K2eEq328iHzu+rSUKyIL3D+xAKkiskZE8kVkhoiEu+3rNhHJcL1uloi0riZrnGv9ERFZCnSs7ftWDUsLuvKYiIQBNwHZxphDldY5gJuBMmDHaTZzMTDXGFNRyxgzgSOuHGfKOxDoBWRU0+R94NrjnzZcf2wuwvqU0RW4E+hvjIl25c6qZeaWQAyQBNwCvOja13HXAY8C0VifgP6B9UcvFejket1Drrb3ANlYn5ZaAH8C3AdkugYYBbQHeuP6OYnI+cD/uda3wvo3+qCavC8Cxa52v3R9KR+gBV154hoRyQN2Af2AX7itG+haVww8BdxgjDlwmm1dQqXulhoywF+Ah1x/YKpySESOYX0KeAn4XzXtFri2N8z1fBxWV88eoAIIA3qISIgxJssYk1nLzGXAVGNMmTFmNlAAdHVb/6kxZpHrU04JcBvwe2NMrjHmKPAYP3czlWEV2nau7S0wJ4+w97wxZo8xJhf4jJ8/nVwPvGGMWWGMKQEeAAaJSIp7UNcf5quAh4wxhcaYdVifypQP0IKuPPGhMSbWGJNojDnfGLPcbd0SY0ws0AyYxc/F8RSuroELga/qEsZVFHcCk6tpEg9EAX8ARgAh1WzHYB2lTnAtug6rHxxjTAZwN/AwcEBEPqiui8IDOcaYcrfnRa58x+1ye5wARALLXd0qeVg/rwTX+iexPnF8LSLbqjjpu6+a/bTG7ZOTMaYAyME6+neXAARXynS6T1yqEdGCrrzCVSB+A0wUkT7VNOsPZBljDnphlw8Cf8YqflXlqTDGPI31yeE3p9nOdGCciLQDzgE+cdvG+8aYoUA7rCP5f3ghd5Vx3R4fAo4BPV1/RGONMTGuk64YY44aY+4xxnQAxgJTPOzb34P1PgAQkSZY50F2V2p3ECgH2rgta1vjd6RsoQVdeY0xJgd4jZ/7eyura3eL+77mAWuBSWdo+jjwR/eTg5W2sxKriL0GzDHG5AGISFcROd/VrVOMVWRr2+/vMVe3y6vAMyKS6MqSJCIXux5fKiKdXP3+R1yZPMn1PnCziKS63tNjwE/GmKxK+6/AOk/xsIhEikgPzvwzVo2EFnTlbc8CY0SkdxXrTrlcsY4eBJqfoc0XwGGsfunqTAdGYhW948Kw/hgcwurGSMQ6AdkQ7sPqVlkiIkeAb/m5z72z63kBrnMErj9up2WMmYt17uETYC/WlSvVXf55J1ZXzT7gTeA/tXwfqoGJzlikGoKItABWAa2N/tIpVS/0CF01lBhgihZzpeqPHqErpZSf0CN0pZTyE2e8fbq+xMfHm5SUFLt2r5RSPmn58uWHjDEJVa2zraCnpKSQnp5u1+6VUsoniUi1N3ppl4tSSvkJLehKKeUntKArpZSf0IKulFJ+Qgu6Ukr5iTMWdBF5wzXTyrpq1ouIPO+aCWWNiPT1fkyllFJn4skR+ptYM6BUZzTWgEGdscanfrnusZRSStWUJ/Myzq88q0kllwNvu8boWCIisSLSyhiz10sZT5KelcuijBxaxYTTIiaclk3DaRkTTtPwYH6et1gp5fOO5cGaGVB2zG1hpaFKThm6pIqhTM7U5pSX1HAfVQ6fcoY2XUdBUr8qXlc33rixKImTZzfJdi07paCLyGRcs8y0bVu7MfOX7zjMM99uOWV5RIjDKvKuAt/Srdi3bBpOq5hw4qLCcARp0Veq0Ss+Am9eCvvX2p3Ei9xqT3TLRlvQq6qQVY74ZYx5BXgFIC0trVajgv3q3I7cNCSFA0dK2HekmH35rq8jP39fuj2X/UeKKXeevAtHkJAYHUanxCiu6pvMqF4tCQ9x1CaGUqo+pb9hFfMJM6D98JPXnfJJvIoSdKY2Xl9fzbIG5o2Cns3J01UlY013VW/Cgh20aR5Jm+ZVzj4GgNNpyCks/bnYHylmX/4x9uWXsCwrl7tnrCJmVghX9EliwoC2dG0ZXZ+RlVLunE7YtwYKD0H5MatbpewYlBdDyVFY+KxVyLue7vSdqswbBX0WcKeIfIA1J2N+ffWf10RQkJAQHUZCdBhnEXPSOqfTsHhbDtOX7uT9n3by5o9Z9Gkby4T+bbn07FZEhto2xI1S/u3gFqtffO2HkLez+nbhsXDBXxsul58443joIjIda+b0eGA/8Fdcs6gbY6a55jZ8AetKmCLgZmPMGUfdSktLM41hcK7cwlJmrshm+tKdZB4sJCosmAt7tOD2ER3p0kKP2pWqtQMbYctXUJQDBzbBgQ1wZDdIEHQYAWddA3GdICQcgiMgxO0rOLxRdGE0RiKy3BiTVuU6uya4aCwF/ThjDOk7DvPB0l18tW4vRWUV/CI1ibtHdqZdXBO74ynlWw5ugddHQnE+BIVAQldI7AFJfaHnFdZJQVUrWtBr6HBhKdPmZ/LWj1mUVRiuSUvmrvM70zo2wu5oSjV+Rbnw2gXWkflt30Oz9hCkN6V7ixb0WjpwpJiX5mXy/k9WX9/1A9ty1/mdad4k1OZkSjVSFWXw7lWwczFM+hzanmN3Ir9zuoKufzZPI7FpOA9f1pPv7x3BlX2TeOvHLEY9O5/Za20/56tU41N8BGZOhu0/wNjntJjbQAu6B5JiI3j8qt7MunMoTgO/eW8Fj36xAadTJ9hWCrC6Wd4YBRv+Z12dknqd3YkCkhb0GuiVFMOP95/PxIHteHXBdqZ8uIrScqfdsZSy17HD8PblkJsJ138Mw6bYnShg6QXXNRQaHMTUy3vSMiacJ+ds5lBBKdMm9iMqTH+UKgDlZMInt8DBTTB+OnS6wO5EAU2P0GtBRLjjvE48Oa43i7flMP6VxRw4Wmx3LKUaTt4ueP0i+Fdfq6hf8zZ0Hml3qoCnBb0Ork5rw2uT0sg8UMilzy/ks9X1OuKBUo3HW5daNw6d9yDcsRS6jrY7kUILep2d1zWR6ZMHEh7i4K7pK7nj/RUUlZbbHUup+pOTCYezoO+NcO690LSV3YmUixZ0L0htE8v3fxjBzUNS+GLNXn4/YxWFJVrUlZ+a/Qfr7s+Bt9udRFWiBd1LHEHCX8f25MFLuvPNhv2Mfm4BHyzdiV03bilVL7b9AJnfwQV/gZhku9OoSrSge9mtwzow/baBRIUFc//MtXy0PNvuSEp5hzEw9xFomgQDfmV3GlUFLej14JwOcXx211AGd4zjL/9bx5d6Z6nyZcbArqXwxT2wezmce581QqJqdLSg1xNHkPDs+FQ6t4ji9vdW8PiXm/TOUuVbDmfBqvetqeBevxCWvwl9boDU6+1Opqqhd8PUo8TocD65fTCPfLaBaT9kEhXm4M7zO9sdS6nTK8qFj38J2763njdNggunwtnXQVSCvdnUaWlBr2dhwQ4e/UUvCorL+ec3W2gb14TLzm5tdyylqvf53ZC1EPrfBmeNgzbn6GQTPkILegMQEf7vyrPYl1/MHz5aTdvmkaS2ibU7llKn2rUUNnwKI/4EI+6zO42qIe1DbyBNwoKZNrEfidFh/Obd5RSXVdgdSalTffMQRLWAwXfanUTVghb0BtS8SShPXX02e/KLeW7uVrvjKHWyvautiSmG/h5CddpFX6QFvYEN7BDH+P5tmPZDpo79ohqXle+CIwzOHm93ElVL2odug7+O7UnmwQJ++8FKosODGdE10e5IKtCVFsGaD6H7WIhoZncaVUt6hG6DiFAH027oR2xECI9+sZGyCp0kQ9ns6wehOA8GTLY7iaoDLeg2iYsK4/GrerP1QAFvL95hdxwVyHK3Q/obkPZLnQfUx2lBt9FFPVpwbpcE/vb5BpZl5dodRwWq+U9CcBgM/6PdSVQdaUG3kYjw91/0IiYihCkfrqKkXC9lVA0sJxNWT4e0W3Rccz+gBd1mbZpH8uJ1fdmVe4yXvs+0O44KNHMfsa5sGXq33UmUF2hBbwSGdo5nVM+WPDd3K99u2G93HBUoMuZad4UOuwei9Eorf6AFvZF45tpUurdqyl9nrdeuF1X/ykvgyz9C8w4w+C670ygv0YLeSESEOvjTmG7szjvGf1fstjuO8me52+HDGyEnA8Y8qWOb+xG9sagRGdopnvbxTZi5YjfjB7S1O47yF84KyE6HzLmw9RvYsxKCHNZoip1G2p1OeZEW9EZERLhxUDse+WwDn67azeWpSXZHUr4udxvMuBH2rwUEkvvDeX+G1Ak6J6gf8qigi8go4DnAAbxmjHm80vq2wFtArKvN/caY2V7OGhAmDmzHrNV7+OPHaxjUIY7EpvpxWNVS3k74zxgoL4ZfvAxdRkFkc7tTqXp0xj50EXEALwKjgR7ABBHpUanZg8CHxpg+wHjgJW8HDRTBjiCeuKo3JeVOXlu43e44ylcdOwzTr4NjeTDpM0i9Tot5APDkpOgAIMMYs80YUwp8AFxeqY0BmroexwA6jGAddG4RzeWprXl3yQ5yCkrsjqN8TcEBax7QQ5vh2neg5Vl2J1INxJOCngTscnue7Vrm7mHgBhHJBmYDVV4HJSKTRSRdRNIPHjxYi7iB467zO1Fa7uSprzfbHUX5mo9/afWdXzcDOl9odxrVgDwp6FVNJlh5+voJwJvGmGRgDPCOiJyybWPMK8aYNGNMWkKCTjZ7Op0SoxnXL5npS3exZFuO3XGUrziyF7IWwNAp0PF8u9OoBuZJQc8G2rg9T+bULpVbgA8BjDGLgXAg3hsBA9k9F3WlXVwkd01fyYGjxXbHUb5g2zzre5eLbY2h7OFJQV8GdBaR9iISinXSc1alNjuBCwBEpDtWQdc+lTpKiA7jxev6crS4jKunLdY7SNWZ7VgE4THQopfdSZQNzljQjTHlwJ3AHGAj1tUs60Vkqohc5mp2D3CbiKwGpgM3GWMqd8uoWuiVFMM/rurNjpwiPl6ebXcc1djtXAJtB0GQ3gQeiDy6Dt11TfnsSssecnu8ARji3WjquMvObs1rC7bzr7kZjOzeghZ6bbqqSn425Gy1LlFUAUn/jPuA4+Om5x8r4/czVuF06ocfVYVV71vfe/7C3hzKNlrQfcTZbWKZcmEXfszM4eUfdNx0VUl2OvzwBHQdY42gqAKSFnQfcuuw9pzXNYEn52zmGx03XR2XkwnvXAnRLWHsc3anUTbSgu5DRIRnr+1D7+QY7pq+gt15x+yOpOxWUQ6f3AoicNMXOlFFgNOC7mNiIkN4YlxvSsud3P/JGu1PD3Q//AP2rIBLn4Fm7exOo2ymBd0HdWvZlL9c2oMFWw/xz2+22B1H2WXXUljwFKReD72utDuNagS0oPuomwancGnvVvx7fibbDhbYHUfZ4asHIDIORv/D7iSqkdCC7qNEhL+O7Ul4sIO/zlqP3scVYLLTYXc69BoHYdF2p1GNhBZ0H5YQHcaUi7qwYOsh3l68w+44qqGUl8B/fw1NEuH8P9udRjUiWtB93MSB7RjeJYHHZm8k44B2vQSEn/5t3RE69jk9Olcn0YLu44IdQTw1rjcRoQ696iUQbP0W5k61ppPrNsbuNKqR0YLuBxKbhvOnMd1J33GYz9boZFF+a89KmHEDJHaHK1+xO41qhLSg+4mr+yXTLi6S6Ut32h1F1YeSAmsmosjmcMNMa4hcpSrRgu4nRISr+yWzZFsuO3IK7Y6jvO3L+yB3O1z5KkTpbF+qalrQ/ciVfZMJEvhEx033H8bAkmmw6l0Y/gdI0VGqVfW0oPuR1rERDO+SwBuLsigsKbc7jvKGuY/AV/dByjA49z6706hGTgu6n5k0KIWCknK+23TA7iiqrnYshoXPWrf2T/wfOELsTqQaOS3ofmZ4lwQSosOYtVqvdvFpe1bBjOshti2MfgIcHk0upgKcFnQ/4wgSruiTxLcb97NhzxG746jayMmEt8ZCSBOY+F8Ii7I7kfIRWtD90OThHYgKC+bprzfbHUXVlDHw+e/BOOHmLyCuo92JlA/Rgu6H4qPCuHVoB+ZuOsCmfXqU7jOMga8fhO0/wNDfW90tStWAFnQ/NWlwOyJDHUybp/OP+ozvH4PFL0DaLTB0it1plA/Sgu6nYiNDuW5AWz5bs5dduUV2x1FnkpMJC5+Bs66BS56GIP2vqWpOf2v82K3DOhAk8Mr8bXZHUWfyzUMQHAYX/c2aH1SpWtCC7sdaxoRzVd9kPkzfxcGjJXbHUdU5sBE2fQ5D7obolnanUT5MC7qf+9W5HSmrcPJ/szfaHUVVxRir7zw4AvpNsjuN8nFa0P1c+/gmTBqcwqer97A3/5jdcZQ7Y6x5QTfOgmH3QFSi3YmUj9OCHgAmDmxHhdPw4vcZdkdRx5WXwKw74aeXYeBvrIG3lKojLegBoENCFGPOaslH6dnkFZXaHSewGQML/gnP9IKV71oDbl38mJ4IVV6hBT1A3H5uJ0rKnXy1bp/dUQJX4SF4/xprBMXYtjDpMzjvT1rMldd4VNBFZJSIbBaRDBG5v5o214jIBhFZLyLvezemqqteSU1pFROug3bZZfNX8PJg2DYPhv0BbvkG2g+3O5XyM2ccwk1EHMCLwIVANrBMRGYZYza4tekMPAAMMcYcFhE9u9PIiFiDdr00L5M9ecdoHRthd6TAcHQ//O/XkPkdJPaEq9+EdoPtTqX8lCdH6AOADGPMNmNMKfABcHmlNrcBLxpjDgMYY3Qw7kboop7WNc4Ltx6yOUkAKDsG6f+BaUNg+wIYfi/c+o0Wc1WvPBlkOQnY5fY8GzinUpsuACKyCHAADxtjvqq8IRGZDEwGaNtWBx5qaGcnx9CyaTjfbz7ANf3b2B3H/xgDaz6ELV9CxndQkg9Nk+C276BVb7vTqQDgSUGv6oyNqWI7nYERQDKwQER6GWPyTnqRMa8ArwCkpaVV3oaqZyLCyB6JfLJ8N0Wl5USG6qQJXlNeAp/eAWs/sop4l4ugz0Srn1xPeqoG4kmXSzbgfjiXDFQ+s5YNfGqMKTPGbAc2YxV41ciM6dWKY2UVzN+i3S5eteRlq5if9yDcvQ6ueg06nKvFXDUoTwr6MqCziLQXkVBgPDCrUpv/AecBiEg8VheMjgjVCPVv35yosGB+2KKnObxm3zqY9zh0GQXn3qsjJSrbnPEztzGmXETuBOZg9Y+/YYxZLyJTgXRjzCzXuotEZANQAdxrjMmpz+CqdkIcQQzrHM93mw5gjEH0CLJuivPhw4kQHgNjn7c7jXIpKysjOzub4uJiu6PUWnh4OMnJyYSEeD45uEedqMaY2cDsSssecntsgCmuL9XIndctkS/X7WN1dj6pbWLtjuO7yoph5mQ4vANu+hyiW9idSLlkZ2cTHR1NSkqKTx60GGPIyckhOzub9u3be/w6/WwYgIZ0igdgyTb9EFVrRbkw/VrY8pU1hrlejtioFBcXExcX55PFHKwLGOLi4mr8CUMLegBKio2gc2IUizL0xGiN7VgMs++Ff/WDbT/Apc/AoDvsTqWq4KvF/Lja5NeCHqCGdIpnWVYuJeUVdkfxDZnfwfQJ8J/R1qBabQbALV9D2i/tTqYaoby8PF566aUG368W9AA1uGMcxWVOVuzIO3PjQFZ8BD65Fd65ArLTYfBdcG8GXDfDKupKVaE2Bb2iou4HV3pnSYA6p0McQQI/Zh5iUMc4u+M0TjmZ8O5VkLcDBv8WRjwAoZF2p1I+4P777yczM5PU1FRCQkKIiIggLi6OzZs3M3z4cF566SWCgoKIiopiypQpzJkzh6effpqhQ4fWab9a0ANUTEQIvZNjWZRxiHsu6mp3nMbnWB68MQqc5XDTbGg3yO5EqpYe+Ww9G/Yc8eo2e7Ruyl/H9qx2/eOPP866detYtWoV8+bNY9SoUWzYsIF27doxatQoZs6cybhx4ygsLKRXr15MnTrVK7m0yyWADekUx+rsfPKLyuyO0riUl8J746DwAFz9Hy3mqs4GDBhAhw4dcDgcTJgwgYULFwLgcDi46qqrvLYfPUIPYOd3a8GL32cyb8sBLk9NsjtO45H+BmQvgytegQ4j7E6j6uh0R9INpfIVK8efh4eH43A4vLYfPUIPYKltYomNDNFxXdwZA8tehTYDofc1dqdRPio6OpqjR4+eeL506VK2b9+O0+lkxowZde4rr44eoQcwR5AwtFM8C7Ye1GEAjtv4GeRkwNApOrCWqrW4uDiGDBlCr169iIiIYNCgQdx///2sXbuW4cOHc8UVV9TLfrWgB7jhXRL4fM1eNu07SvdWTe2OY6/SIpjzJ2jRC3pfa3ca5ePef9+aiXPevHk89dRTzJgx45Q2BQUFXt2ndrkEuOGdEwCYv+WgzUkagYX/hPxdMOZJcOixjvI9WtADXMuYcLq1jGb+1gAv6DmZsOg568hcx2VRXjRixAg+//zzBtmXFnTFOe2bsygjh7IKp91R7GEMfHU/OMLgQu9cD6yUHbSgK7q5+s7X7s63OYlNVrwNW7+G8x6A6JZ2p1Gq1rSgKy7olgjAyp0BOK5L/m7rRGhyfxgw2e40StWJFnRFYtNwEqLDWL8nwI7Qy0ussVqcFXDV6+DwfGYYpRojLegKgJ6tm3p9vItG76sH4OBGa4KKZu3sTqP8iA6fq2zVOymGrQcKOFIcAOO6GAPfPATpr0OfiTDgNrsTKT9j1/C5WtAVYE14UeE0LMn082npnE749mHrEsXUG2Dsc3YnUn7Iffjc/v37M2LECMaNG0e3bt24/vrrsaZhhpSUFKZOncrQoUP56KOP6rxfvXtCAXB2m1hCHUEsy8rlop5+eqVHSQG8cTHsXwd9b4RLn4MgPabxe1/eD/vWenebLc+C0Y9Xu7ry8LmXX34569evp3Xr1gwZMoRFixadGM8lPDz8xOiLdaW/zQqA8BAHqW1i+Wh5tt1R6kd5KUwfbxXzQXfC2Oe1mKsGM2DAAJKTkwkKCiI1NZWsrKwT66691nvDTOgRujqhT9tYlmblsifvGK1jI+yO411f/xmyFsDIR2Do3XanUQ3pNEfSDSUsLOzEY4fDQXl5+YnnTZo08dp+9BBFnXBFX2tM9G837rc5iZfl74alr1gnQLWYqwZQefjchqJH6OqEri2i6ZXUlPeW7GTiwHb+M5zuTy9b3wfdYW8OFTAqD5/bokWLBtmvFnR1gohwwzntuH/mWpZlHWZA++Z2R6q7wkPw07+h6xhI7G53GhVAjg+fW9kLL7xw4rF7X7o3aJeLOsllqa2JCgvmg2U77Y7iHcv/AxWlcO59didRqt5pQVcniQwNZmT3ROZtPkiF09gdp26cTmvgreQB0DrV7jRK1Tst6OoUF/VsSW5hKYt9/Sajbd9D3k4451d2J1GqQWhBV6c4v1siYcFBzN3kw1e7VJTDwmcgojl0H2t3GmWD43dj+qra5NeCrk4RHuKgf0pzPlu91+4otbfybeu68/P+BMFhZ26v/Ep4eDg5OTk+W9SNMeTk5BAeHl6j13l0lYuIjAKeAxzAa8aYKq/UF5FxwEdAf2NMeo2SqEalV1IMCzMOsf9IMS2a1uyXynblpTD/aWuM8/632p1G2SA5OZns7GwOHvTdqRXDw8NJTk6u0WvOWNBFxAG8CFwIZAPLRGSWMWZDpXbRwG+Bn2qUQDVKF/VswbQfMpm9di83D2lvd5ya+WkaHMmGy54Hf7mWXtVISEgI7dv72O+tF3jS5TIAyDDGbDPGlAIfAJdX0e5vwBNAsRfzKZv0bG1NSzd34wGbk9TQwc3w3d+g6yXQ8Xy70yjVoDwp6EnALrfn2a5lJ4hIH6CNMea0U1uLyGQRSReRdF/+KBQIwoId3DioHQszDpFTUGJ3HM8U5cK74yC0CVz6jB6dq4DjSUGv6n/FiTMNIhIEPAPcc6YNGWNeMcakGWPSEhISPE+pbHFBd+t25fQdh21O4oHyUvjwRsjfCRf+DaIb5lZrpRoTTwp6NtDG7XkysMfteTTQC5gnIlnAQGCWiKR5K6Syx8AOzYkMdbBgayP/NFV8BN6/BrIWwuUvQt+JdidSyhaeFPRlQGcRaS8iocB4YNbxlcaYfGNMvDEmxRiTAiwBLtOrXHxfWLCDoZ3i+Sg9G2djvWs0axH8exhsn28V8z432J1IKducsaAbY8qBO4E5wEbgQ2PMehGZKiKX1XdAZa9hXRIoKXc2vm4XY2DJNHhrLFSUwfUfQp/r7U6llK08ug7dGDMbmF1p2UPVtB1R91iqsbiyTxKPfrGBtxZnNZ7RF/N2wmd3Q+ZcaxTFK6ZBeIzdqZSynd4pqk6rSVgwgzvG88WaveQVldobxlkBy16HV0bAziVw4VS49j0t5kq5aEFXZ3T9OW0BmLfZxpOjWQvh38PhiykQGQ+/mg9DfsoSDgwAABWwSURBVKfzgirlRv83qDM6r2si8VGhfLR815kbe5sxsOg5q6/88A4Y8xTc8RPEd2r4LEo1clrQ1RkFBQnXn9OORRk5rNud33A7djph1l3wzUPQaSTcswkG3KY3DClVDS3oyiOTBqcQ4hD+u3J3w+30m7/AynesyZ0nzICwqIbbt1I+SAu68kjzJqGc2yWRt37MoqzCWf873PApLH4B+k6Cy/6lfeVKeUD/lyiPjeyeSLnT8NO23Prd0dF9MOu30OIsuPgx7WJRykNa0JXHLk9NIiLEwScrsutvJ8bAJ7dC2TG4+j/azaJUDWhBVx6LCHUw9uxWzF67l+KyivrZyar3rJmGRtwH8Z3rZx9K+Skt6KpGLundmpJyJ2/9mOX9ja+eAZ/9Dlr3gUF3en/7Svk5LeiqRoZ1igdg8bYc7200dzt8fAv8dzK0OQcm/k/nAVWqFrSgqxoJChImDmzHjxk5HC0uq/sGl7wM04bCuo9hwGSrmEfE1n27SgUgLeiqxi7p3YrSCidz1u+v/UaMgbl/g6/uh7hOcMcyGPMkBId6L6hSAUYLuqqx/inN6dYymlfnb6vdBopy4YPrYcFT0Hs83PotJHTxbkilApAWdFVjjiDh2v5t2Lz/KBv3HvHsRcbAptnw6R3wRHvY/AUMvssa+tYRUr+BlQoQWtBVrVzRJ4mw4CDe+2nHmRvvWwuvng8fTICV78FZ18Ckz+Civ+tNQ0p5kUcTXChVWWxkKJf2bs27S3Zy78XdiImo5ih79wp45xfWrEIX/R3SfgmhTRo2rFIBQo/QVa2N65cMwNfr91XdoLwUPrwRgoLh5tlWF4sWc6XqjRZ0VWsDOzQnJS6Sd5fswJgqJpFe8Rbk74IrX7FuFlJK1Sst6KrWRITJwzuyOjufRRmVbjQqyoXv/gbJ/aHjBfYEVCrAaEFXdXJVvyRaNA3jxe8zTl4xdyoU58Oox/XEp1INRAu6qpOwYAe3DevA4m05LN9x2Fq4ZQ4s/w+kXg/JafYGVCqAaEFXdTZhQFuaRYbw8rwMyNtpjcsS38U6OldKNRgt6KrOmoQFc/OQ9qzZuJnSNy4BZzlc/SaEN7U7mlIBRQu68oqbusMHYY8iR/fCdTOgRU+7IykVcLSgq7rbvYKmb19AB9nDrSVTWBZ0lt2JlApIWtBV7RXlwqy74NXzACi47gvWRvTnwf+uw+ms4rp0pVS90oKuaufgFniqM6x4G9oOgl/NJ6rLUB4Y3Y3N+4/y79qOxKiUqjUt6Kp2PrnFOvl55Wvwy6+gWQpgDdqVEhfJc3O3cKigxN6MSgUYLeiq5jK+hX1rrLHMe1990qpgRxCvTepPWYXh6a+32BRQqcDkUUEXkVEisllEMkTk/irWTxGRDSKyRkTmikg770dVjcKRvfC/O6B5R7jk6SqbdEqMYsKANnyUvosl3px7VCl1Wmcs6CLiAF4ERgM9gAki0qNSs5VAmjGmN/Ax8IS3g6pGoKIcPpoEBfvhFy9BWFS1Te+9qBtJzSKYMmMVhSXlDRhSqcDlyRH6ACDDGLPNGFMKfABc7t7AGPO9MabI9XQJkOzdmKpRWPEW7PrJmvuz7cDTNo2JDOGf15zNnvxinv9uawMFVCqweVLQk4Bdbs+zXcuqcwvwZVUrRGSyiKSLSPrBgwc9T6ns53TCj89DUhr0v9Wjl/Rr15xx/ZJ5fcF2Nu87Ws8BlVKeFPSqhsqr8iJjEbkBSAOerGq9MeYVY0yaMSYtISHB85TKfjsWwuEsGDC5RqMnPjC6G9HhwfzxkzWUVzjrL59SyqOCng20cXueDOyp3EhERgJ/Bi4zxuj1av5myxxr5qFul9ToZXFRYTw0tgerd+Vxy1vp5B8rq6eASilPCvoyoLOItBeRUGA8MMu9gYj0Af6NVcwPeD+mstWxw7D4Begw4rQnQqvzi9QkHrmsJz9mHuKKFxexalee1yMqpTwo6MaYcuBOYA6wEfjQGLNeRKaKyGWuZk8CUcBHIrJKRGZVsznli1a8bX0feHutXi4iTBqcwju3nENBSTnjXv6RT1ft9mJApRSAVDkXZANIS0sz6enptuxb1UDhIXh5MMS2g1u/qfPm9uYfY/Lby9mw9wj/d+VZjOubTFCQzmiklKdEZLkxpsqZY/ROUVU9Y+DrB63rzkf+1SubbBUTwfu3nUOvpBj++PEa/vHVJq9sVymlBV2dzk/TYPV0SL0BUoZ6bbPR4SHMvH0wV/ZN4pUF21i6Pddr21YqkGlBV1Xbuxq+ecgaSfGy572+eUeQ8LfLe9GmWSS/+2Al+48Ue30fSgUaLejqVMbAF3+AkEi49l0IctTLbpqEBfPs+FRyCkv55ZvLyDpUWC/7USpQaEFXp1ryEmQvhZEPQ5P4et1V37bNeH58H3bkFDH2hYV8v0mvelWqtrSgq5NVlMOSlyGhG/Sd1CC7HNWrJV/+bhhJsRHc/OYy7pq+ksOFpQ2yb6X8iRZ0dbJNn0P+Ljj/QQhquF+PNs0j+eT2wfzq3A58tnoPgx//jt/PWMW+fO1bV8pTWtDVz0oKYO4jENcZuoxu8N03CQvmgdHd+fjXg2gdG85/V+7mwn/+wOdrThlpQilVBS3o6mcLnobcbXDJU+AIti1GWkpz5t4zgjl3D6dN80jufH8lE1//iYNHdYggpU5HC7qy7FxijdfS8wprzJZGoGvLaD741UBuHdqeBVsPMeLJ7/l2w367YynVaGlBV3BwM7x1GYQ2gfMetDvNSZqGh/DgpT346NeDSGoWwa1vp/Pb6SvZul/HV1eqMi3oga4oF96/BjBwy7cQ38nuRFXqn9Kcmb8Zwk2DU/h8zR4uenY+t7+7nMyDBXZHU6rR0IIeyIyBT++AwzusCZ8baTE/LiosmIcv68mSBy5g0qAUvly3j0ueX8A/vtrEsdIKu+MpZTsdbTGQff0Xa1q5C6fCkN/ZnabGsg8X8eSczXy6ag8dEpowoX9bRvZoQUpcJFKDWZWU8iWnG21RC3ogKi+FRc/C949Cl1EwfnqDXnPubTOW7eS5b7eyx3XNeohDuCatDZ0ToxjeJYEOCTWflEOpxkoLurIU5sDMW2HbPDBOSBkG130IoZF2J/OKjANHmbN+P5+u2s2W/T/3rXdv1ZTbhrXngu4tiIkIsTGhUnWnBV1Z3r0KMr61jsp7XwM9rvDpI/PTKatwsu1gIe8syeKz1XvJP1ZGZKiDO87rxM1DUogMte86e6XqQgu6si5NfHEA9LsZxj5rd5oG5XQaftqey7++28qPmTm0aR7BxIHt6J0cy8AOcXbHU6pGTlfQ9TAlEBy/NDE0GoZNsTtNgwsKEgZ1jGNQxzgWZ+bw6OwNPDbbmilpYIfmTLmwKwPaN7c5pVJ1p0fo/qgo17p6Zft8OLIHju61lo9/H7pdYm+2RsAYw87cIt5YuJ2ZK3ZztKSca9Pa8KdLumsfu2r0tMslUOxdA1/cY41lDhDRHFr2gpa9oesYSBlib75G6FBBCY/N3sjMFbsB6NYymjvP78SlvVvbnEypqmlB93d7VsLseyF7mfW8dV+4+FFoN9jeXD7khy0HmbtxP7NW7yGvqIxuLaM5t2sCKXFN6NU6hi4towgLrp+Zm5SqCS3o/mzB0zB3KoRGQcfzoP9t0OFcu1P5rGOlFfx7fiZfrNnLtkOFVDit/x+tY8K5eUh7erRuSpcW0cRHherNS8oWWtD9Ue52+PpBa0KKpDSYMB2iEu1O5VdKy53syClkTXY+byzazvo9R06sS4wO44LuibRt3oTBHeNIiWtCTKT2v6v6p1e5+JuCA/Cf0VCUA0OnwHl/tnX8cn8VGhxE5xbRdG4RzVX9ktmbf4yVO/PYm1/M8h25TF+666T2jiChZdNw2sc3oX9Kc24Z1p6oMP13UQ1Hj9B9zdH98OYlcHg73DhLT3TayBjD1gMFrNhxmC37C8jKKUSA7TmFbDtYSJDA6LNaMWlQil4WqbxGj9D9QU6m1cWycwmUFsCVr2oxt5mI0KVFNF1aRJ+yLj0rl9lr9/HR8l18sWYv8VFhnNO+OWclxzC6V0vaxTWxIbHyd3qE3tgd2QOvjYQju39eduOnjWZWIXV6xWUVfLw8m0UZh1i9K+/EAGI9WzdlYIc4+rVrRuvYCNprH7zykJ4U9UVH9sCP/4JV70F5iTX+ytnjodOF2l/uo4wxZBwoYObK3SzdnsvyHYdPWt85MYqhneNpH9+Es5Nj6ZDQhOhwLfLqZNrl4it2LIZ5j1kTT2QtBIw1IuLFj0Krs+1Op+pIROjcIpr7RnUDoLCknE37jrD/SAmb9h3lx4xDvL14x4lLJY/r0aoprWPDCQt20Do2nNDgIJzGugmqTfNIYiJCSIwOIyosWC+lDHB6hN7Qyoqh8KB1O/6OH2F3OhQchJyt1lUrAJHx0H4YDL4LkvrZm1c1qLIKJztzi1ibnc/SrFyKSys4WFDCmux8wLqU8lhZ1bMzNQl10DExiu4tm9I+ocmJQh8TEUKr2Ahax4RrwfcDde5yEZFRwHOAA3jNGPN4pfVhwNtAPyAHuNYYk3W6bfp9Qa8og8NZsG8trHgbDmyEgn2ntovrBFEtIbYNxHe2RkOM1Csi1OkdLixlZ24RhwpKyCsqY//RYnblFrF531FW7Myr9nXR4cFEhjo4KymWs5NjCAsJIiYihD5tm9EpIYqgIC34jV2dulxExAG8CFwIZAPLRGSWMWaDW7NbgMPGmE4iMh74B3Bt3aM3Ek4nVJRAaaF1dF2w33pcUQoV5VBWBMV5Vr935vfWuiPZP78+uhXEtrWuSoloBondIbo1tDzLKuRK1VCzJqE0axJa5boKp6GotJzDhWXsyC0k/1gZ+/KLOVRQSnFZBQePlrB8x2G+3bj/lNdGhwcTHRZMVHgwFU5DQnQYbZtHkhQbSWxkCGHBQYQ4gggJDiI6LJhgh+AQIShIcAQJQQJBcvyxEB8VRmJ0mP6haCCe9KEPADKMMdsAROQD4HLAvaBfDjzsevwx8IKIiKmP/pwV71gjCRoDmJ+/w6nLDJWeu38/3Tq37xVlVsH2iEBcR2jeHnpfDc07QHxXSE6DIB0HRDUMR5AQHR5CdHgIbeOqno3KGENZhaGwpJx1e/LZur+AXYeLKKtwUlRSQU5hKXvyjpF/rJyvN+wnr6is1nnEVeQBQh1BhDjEbZ2caHNiWaV1Jy87acunea37surbVd6Pe+aabKeqrJym3e8u6MzYs70/AJwnBT0JcL8lLhs4p7o2xphyEckH4oBD7o1EZDIwGaBt27a1SxwZB4k9XD9VqfSdKpa5f6+03gp1htcIhDW1pmkLDoegYKtoRzQDRxg4QsERAhGxVjvto1Q+QEQIDRZCg0MZ1jmBYZ0TTtu+uKyCgpJyyiqclJUbCkvLKSqtwGkMFU6D02moMAansSYUqXAayp2GgwUlHDhSfOIY6lhZxSknfd2P+04cark1Ma6lJy87tR1VtTt+rIepYtmp7U5qW+X+zpS1+nbu26uvYZo9KehVVajKR96etMEY8wrwClh96B7s+1TdxlhfSqkGEx7iIDxEP2U2dp5MKJkNuHf0JgN7qmsjIsFADJDrjYBKKaU840lBXwZ0FpH2IhIKjAdmVWozC5jkejwO+K5e+s+VUkpV64xdLq4+8TuBOViXLb5hjFkvIlOBdGPMLOB14B0RycA6Mh9fn6GVUkqdyqM7RY0xs4HZlZY95Pa4GLjau9GUUkrVhCddLkoppXyAFnSllPITWtCVUspPaEFXSik/YdtoiyJyENhhy87rLp5Kd8H6CX98X/qefIM/vieon/fVzhhT5a29thV0XyYi6dWNdubL/PF96XvyDf74nqDh35d2uSillJ/Qgq6UUn5CC3rtvGJ3gHrij+9L35Nv8Mf3BA38vrQPXSml/IQeoSullJ/Qgq6UUn5CC3oticjVIrJeRJwi4tOXW4nIKBHZLCIZInK/3Xm8QUTeEJEDIrLO7izeIiJtROR7Edno+t37nd2Z6kpEwkVkqYisdr2nR+zO5C0i4hCRlSLyeUPtUwt67a0DrgTm2x2kLtwmAR8N9AAmiEgPe1N5xZvAKLtDeFk5cI8xpjswELjDD/6tSoDzjTFnA6nAKBEZaHMmb/kdsLEhd6gFvZaMMRuNMZvtzuEFJyYBN8aUAscnAfdpxpj5+NmsWcaYvcaYFa7HR7GKRZK9qerGWApcT0NcXz5/pYaIJAOXAK815H61oKuqJgH36SIRCEQkBegD/GRvkrpzdU2sAg4A3xhjfP49Ac8CfwScDblTLeinISLfisi6Kr58/gjWjUcTfKvGQ0SigE+Au40xR+zOU1fGmApjTCrWfMUDRKSX3ZnqQkQuBQ4YY5Y39L49mrEoUBljRtqdoQF4Mgm4aiREJASrmL9njJlpdx5vMsbkicg8rHMfvnwyewhwmYiMAcKBpiLyrjHmhvresR6hK08mAVeNgIgI1vy9G40x/7Q7jzeISIKIxLoeRwAjgU32pqobY8wDxphkY0wK1v+n7xqimIMW9FoTkStEJBsYBHwhInPszlQbxphy4Pgk4BuBD40x6+1NVXciMh1YDHQVkWwRucXuTF4wBJgInC8iq1xfY+wOVUetgO9FZA3WwcU3xpgGu8zP3+it/0op5Sf0CF0ppfyEFnSllPITWtCVUspPaEFXSik/oQVdKaX8hBZ05XNEJM7tsr19IrLb9ThPRDbUw/5G1HTEPBGZV9UonCJyk4i84L10Sv1MC7ryOcaYHGNMqut28WnAM67HqXgwdoaI6B3Syi9pQVf+xiEir7rG1v7adffh8SPmx0TkB+B3rjsUPxGRZa6vIa5257od/a8UkWjXdqNE5GMR2SQi77nu2kRELnC1W+sagz2sciARuVlEtrj2PaSBfg4qAGlBV/6mM/CiMaYnkAdc5bYu1hhzrjHmaeA5rCP7/q42x4c5/QNwh+uIfxhwzLW8D3A31pjxHYAhIhKONe76tcaYs7DGRrrdPYyItAIewSrkF7per1S90IKu/M12Y8wq1+PlQIrbuhluj0cCL7iGbZ2FNYBSNLAI+KeI/BbrD0C5q/1SY0y2McYJrHJtt6trf1tcbd4ChlfKcw4wzxhz0DXe/AyUqifal6j8TYnb4wogwu15odvjIGCQMeYYJ3tcRL4AxgBLROT4iJuVtxtM1UMPV0XH11ANQo/QVaD6GmtQMgBEJNX1vaMxZq0x5h9AOtDtNNvYBKSISCfX84nAD5Xa/ASMcF2ZEwJc7a03oFRlWtBVoPotkCYia1yXOv7atfxu1yQmq7H6z7+sbgPGmGLgZuAjEVmLdYXNtEpt9gIPY438+C2wwttvRKnjdLRFpZTyE3qErpRSfkILulJK+Qkt6Eop5Se0oCullJ/Qgq6UUn5CC7pSSvkJLehKKeUn/h/zYdRqLa1UhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    tpr, fpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    tnr = 1 - fpr\n",
    "    plt.plot(thresholds, tpr, label='tpr')\n",
    "    plt.plot(thresholds, tnr, label='tnr')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.title('TPR / TNR vs Threshold')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an appropriate threshold and generate classification report on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 1 / 247\n",
      "    Batch 2 / 247\n",
      "    Batch 3 / 247\n",
      "    Batch 4 / 247\n",
      "    Batch 5 / 247\n",
      "    Batch 6 / 247\n",
      "    Batch 7 / 247\n",
      "    Batch 8 / 247\n",
      "    Batch 9 / 247\n",
      "    Batch 10 / 247\n",
      "    Batch 11 / 247\n",
      "    Batch 12 / 247\n",
      "    Batch 13 / 247\n",
      "    Batch 14 / 247\n",
      "    Batch 15 / 247\n",
      "    Batch 16 / 247\n",
      "    Batch 17 / 247\n",
      "    Batch 18 / 247\n",
      "    Batch 19 / 247\n",
      "    Batch 20 / 247\n",
      "    Batch 21 / 247\n",
      "    Batch 22 / 247\n",
      "    Batch 23 / 247\n",
      "    Batch 24 / 247\n",
      "    Batch 25 / 247\n",
      "    Batch 26 / 247\n",
      "    Batch 27 / 247\n",
      "    Batch 28 / 247\n",
      "    Batch 29 / 247\n",
      "    Batch 30 / 247\n",
      "    Batch 31 / 247\n",
      "    Batch 32 / 247\n",
      "    Batch 33 / 247\n",
      "    Batch 34 / 247\n",
      "    Batch 35 / 247\n",
      "    Batch 36 / 247\n",
      "    Batch 37 / 247\n",
      "    Batch 38 / 247\n",
      "    Batch 39 / 247\n",
      "    Batch 40 / 247\n",
      "    Batch 41 / 247\n",
      "    Batch 42 / 247\n",
      "    Batch 43 / 247\n",
      "    Batch 44 / 247\n",
      "    Batch 45 / 247\n",
      "    Batch 46 / 247\n",
      "    Batch 47 / 247\n",
      "    Batch 48 / 247\n",
      "    Batch 49 / 247\n",
      "    Batch 50 / 247\n",
      "    Batch 51 / 247\n",
      "    Batch 52 / 247\n",
      "    Batch 53 / 247\n",
      "    Batch 54 / 247\n",
      "    Batch 55 / 247\n",
      "    Batch 56 / 247\n",
      "    Batch 57 / 247\n",
      "    Batch 58 / 247\n",
      "    Batch 59 / 247\n",
      "    Batch 60 / 247\n",
      "    Batch 61 / 247\n",
      "    Batch 62 / 247\n",
      "    Batch 63 / 247\n",
      "    Batch 64 / 247\n",
      "    Batch 65 / 247\n",
      "    Batch 66 / 247\n",
      "    Batch 67 / 247\n",
      "    Batch 68 / 247\n",
      "    Batch 69 / 247\n",
      "    Batch 70 / 247\n",
      "    Batch 71 / 247\n",
      "    Batch 72 / 247\n",
      "    Batch 73 / 247\n",
      "    Batch 74 / 247\n",
      "    Batch 75 / 247\n",
      "    Batch 76 / 247\n",
      "    Batch 77 / 247\n",
      "    Batch 78 / 247\n",
      "    Batch 79 / 247\n",
      "    Batch 80 / 247\n",
      "    Batch 81 / 247\n",
      "    Batch 82 / 247\n",
      "    Batch 83 / 247\n",
      "    Batch 84 / 247\n",
      "    Batch 85 / 247\n",
      "    Batch 86 / 247\n",
      "    Batch 87 / 247\n",
      "    Batch 88 / 247\n",
      "    Batch 89 / 247\n",
      "    Batch 90 / 247\n",
      "    Batch 91 / 247\n",
      "    Batch 92 / 247\n",
      "    Batch 93 / 247\n",
      "    Batch 94 / 247\n",
      "    Batch 95 / 247\n",
      "    Batch 96 / 247\n",
      "    Batch 97 / 247\n",
      "    Batch 98 / 247\n",
      "    Batch 99 / 247\n",
      "    Batch 100 / 247\n",
      "    Batch 101 / 247\n",
      "    Batch 102 / 247\n",
      "    Batch 103 / 247\n",
      "    Batch 104 / 247\n",
      "    Batch 105 / 247\n",
      "    Batch 106 / 247\n",
      "    Batch 107 / 247\n",
      "    Batch 108 / 247\n",
      "    Batch 109 / 247\n",
      "    Batch 110 / 247\n",
      "    Batch 111 / 247\n",
      "    Batch 112 / 247\n",
      "    Batch 113 / 247\n",
      "    Batch 114 / 247\n",
      "    Batch 115 / 247\n",
      "    Batch 116 / 247\n",
      "    Batch 117 / 247\n",
      "    Batch 118 / 247\n",
      "    Batch 119 / 247\n",
      "    Batch 120 / 247\n",
      "    Batch 121 / 247\n",
      "    Batch 122 / 247\n",
      "    Batch 123 / 247\n",
      "    Batch 124 / 247\n",
      "    Batch 125 / 247\n",
      "    Batch 126 / 247\n",
      "    Batch 127 / 247\n",
      "    Batch 128 / 247\n",
      "    Batch 129 / 247\n",
      "    Batch 130 / 247\n",
      "    Batch 131 / 247\n",
      "    Batch 132 / 247\n",
      "    Batch 133 / 247\n",
      "    Batch 134 / 247\n",
      "    Batch 135 / 247\n",
      "    Batch 136 / 247\n",
      "    Batch 137 / 247\n",
      "    Batch 138 / 247\n",
      "    Batch 139 / 247\n",
      "    Batch 140 / 247\n",
      "    Batch 141 / 247\n",
      "    Batch 142 / 247\n",
      "    Batch 143 / 247\n",
      "    Batch 144 / 247\n",
      "    Batch 145 / 247\n",
      "    Batch 146 / 247\n",
      "    Batch 147 / 247\n",
      "    Batch 148 / 247\n",
      "    Batch 149 / 247\n",
      "    Batch 150 / 247\n",
      "    Batch 151 / 247\n",
      "    Batch 152 / 247\n",
      "    Batch 153 / 247\n",
      "    Batch 154 / 247\n",
      "    Batch 155 / 247\n",
      "    Batch 156 / 247\n",
      "    Batch 157 / 247\n",
      "    Batch 158 / 247\n",
      "    Batch 159 / 247\n",
      "    Batch 160 / 247\n",
      "    Batch 161 / 247\n",
      "    Batch 162 / 247\n",
      "    Batch 163 / 247\n",
      "    Batch 164 / 247\n",
      "    Batch 165 / 247\n",
      "    Batch 166 / 247\n",
      "    Batch 167 / 247\n",
      "    Batch 168 / 247\n",
      "    Batch 169 / 247\n",
      "    Batch 170 / 247\n",
      "    Batch 171 / 247\n",
      "    Batch 172 / 247\n",
      "    Batch 173 / 247\n",
      "    Batch 174 / 247\n",
      "    Batch 175 / 247\n",
      "    Batch 176 / 247\n",
      "    Batch 177 / 247\n",
      "    Batch 178 / 247\n",
      "    Batch 179 / 247\n",
      "    Batch 180 / 247\n",
      "    Batch 181 / 247\n",
      "    Batch 182 / 247\n",
      "    Batch 183 / 247\n",
      "    Batch 184 / 247\n",
      "    Batch 185 / 247\n",
      "    Batch 186 / 247\n",
      "    Batch 187 / 247\n",
      "    Batch 188 / 247\n",
      "    Batch 189 / 247\n",
      "    Batch 190 / 247\n",
      "    Batch 191 / 247\n",
      "    Batch 192 / 247\n",
      "    Batch 193 / 247\n",
      "    Batch 194 / 247\n",
      "    Batch 195 / 247\n",
      "    Batch 196 / 247\n",
      "    Batch 197 / 247\n",
      "    Batch 198 / 247\n",
      "    Batch 199 / 247\n",
      "    Batch 200 / 247\n",
      "    Batch 201 / 247\n",
      "    Batch 202 / 247\n",
      "    Batch 203 / 247\n",
      "    Batch 204 / 247\n",
      "    Batch 205 / 247\n",
      "    Batch 206 / 247\n",
      "    Batch 207 / 247\n",
      "    Batch 208 / 247\n",
      "    Batch 209 / 247\n",
      "    Batch 210 / 247\n",
      "    Batch 211 / 247\n",
      "    Batch 212 / 247\n",
      "    Batch 213 / 247\n",
      "    Batch 214 / 247\n",
      "    Batch 215 / 247\n",
      "    Batch 216 / 247\n",
      "    Batch 217 / 247\n",
      "    Batch 218 / 247\n",
      "    Batch 219 / 247\n",
      "    Batch 220 / 247\n",
      "    Batch 221 / 247\n",
      "    Batch 222 / 247\n",
      "    Batch 223 / 247\n",
      "    Batch 224 / 247\n",
      "    Batch 225 / 247\n",
      "    Batch 226 / 247\n",
      "    Batch 227 / 247\n",
      "    Batch 228 / 247\n",
      "    Batch 229 / 247\n",
      "    Batch 230 / 247\n",
      "    Batch 231 / 247\n",
      "    Batch 232 / 247\n",
      "    Batch 233 / 247\n",
      "    Batch 234 / 247\n",
      "    Batch 235 / 247\n",
      "    Batch 236 / 247\n",
      "    Batch 237 / 247\n",
      "    Batch 238 / 247\n",
      "    Batch 239 / 247\n",
      "    Batch 240 / 247\n",
      "    Batch 241 / 247\n",
      "    Batch 242 / 247\n",
      "    Batch 243 / 247\n",
      "    Batch 244 / 247\n",
      "    Batch 245 / 247\n",
      "    Batch 246 / 247\n",
      "    Batch 247 / 247\n",
      "Threshold: 0.3234, accuracy: 0.7542\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.75      0.75      3943\n",
      "         1.0       0.75      0.76      0.75      3943\n",
      "\n",
      "    accuracy                           0.75      7886\n",
      "   macro avg       0.75      0.75      0.75      7886\n",
      "weighted avg       0.75      0.75      0.75      7886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx1 = np.where(tpr <= tnr)[0]\n",
    "idx2 = np.where(tpr >= tnr)[0]\n",
    "t = thresholds[idx1[-1]]\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "with torch.no_grad():\n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        edges, features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        all_pairs = torch.mm(out, out.t())\n",
    "        scores = all_pairs[edges.T]\n",
    "        predictions = (scores >= t).long()\n",
    "        y_true.extend(labels.detach().numpy())\n",
    "        y_pred.extend(predictions.detach().numpy())\n",
    "        total_correct += torch.sum(predictions == labels.long()).item()\n",
    "        total_examples += len(labels) \n",
    "        print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "print('Threshold: {:.4f}, accuracy: {:.4f}'.format(t, total_correct / total_examples))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "print('Classification report\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: val\n",
      "Number of vertices: 113\n",
      "Number of static edges: 1636\n",
      "Number of temporal edges: 10409\n",
      "Number of examples/datapoints: 9928\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the validation dataset after training.\n",
      "    Batch 3 / 311: loss 0.5689, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8907\n",
      "    Batch 6 / 311: loss 0.6064, accuracy 0.7396\n",
      "    ROC-AUC score: 0.9405\n",
      "    Batch 9 / 311: loss 0.5910, accuracy 0.6354\n",
      "    ROC-AUC score: 0.6792\n",
      "    Batch 12 / 311: loss 0.6178, accuracy 0.7708\n",
      "    ROC-AUC score: 0.6750\n",
      "    Batch 15 / 311: loss 0.6431, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7778\n",
      "    Batch 18 / 311: loss 0.5571, accuracy 0.7500\n",
      "    ROC-AUC score: 0.9286\n",
      "    Batch 21 / 311: loss 0.5988, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8138\n",
      "    Batch 24 / 311: loss 0.7216, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7157\n",
      "    Batch 27 / 311: loss 0.6481, accuracy 0.6458\n",
      "    ROC-AUC score: 0.5820\n",
      "    Batch 30 / 311: loss 0.5766, accuracy 0.7396\n",
      "    ROC-AUC score: 0.6548\n",
      "    Batch 33 / 311: loss 0.7100, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 36 / 311: loss 0.5908, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7458\n",
      "    Batch 39 / 311: loss 0.5986, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 42 / 311: loss 0.6631, accuracy 0.6562\n",
      "    ROC-AUC score: 0.8718\n",
      "    Batch 45 / 311: loss 0.6089, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8091\n",
      "    Batch 48 / 311: loss 0.5811, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8672\n",
      "    Batch 51 / 311: loss 0.6334, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8135\n",
      "    Batch 54 / 311: loss 0.5335, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8516\n",
      "    Batch 57 / 311: loss 0.5627, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 60 / 311: loss 0.5890, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8175\n",
      "    Batch 63 / 311: loss 0.5810, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7489\n",
      "    Batch 66 / 311: loss 0.6704, accuracy 0.6875\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 69 / 311: loss 0.6037, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8167\n",
      "    Batch 72 / 311: loss 0.6157, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 75 / 311: loss 0.5781, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7750\n",
      "    Batch 78 / 311: loss 0.5772, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 81 / 311: loss 0.5770, accuracy 0.7708\n",
      "    ROC-AUC score: 0.9514\n",
      "    Batch 84 / 311: loss 0.6479, accuracy 0.6667\n",
      "    ROC-AUC score: 0.7539\n",
      "    Batch 87 / 311: loss 0.6518, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 90 / 311: loss 0.6506, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8196\n",
      "    Batch 93 / 311: loss 0.5728, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 96 / 311: loss 0.5958, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6000\n",
      "    Batch 99 / 311: loss 0.5385, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8947\n",
      "    Batch 102 / 311: loss 0.6304, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7208\n",
      "    Batch 105 / 311: loss 0.5626, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8704\n",
      "    Batch 108 / 311: loss 0.5755, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7864\n",
      "    Batch 111 / 311: loss 0.6081, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7000\n",
      "    Batch 114 / 311: loss 0.6417, accuracy 0.6458\n",
      "    ROC-AUC score: 0.6549\n",
      "    Batch 117 / 311: loss 0.6162, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6964\n",
      "    Batch 120 / 311: loss 0.6546, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6640\n",
      "    Batch 123 / 311: loss 0.6318, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7328\n",
      "    Batch 126 / 311: loss 0.6399, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 129 / 311: loss 0.6134, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8458\n",
      "    Batch 132 / 311: loss 0.5898, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7103\n",
      "    Batch 135 / 311: loss 0.6369, accuracy 0.7292\n",
      "    ROC-AUC score: 0.6417\n",
      "    Batch 138 / 311: loss 0.6157, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7706\n",
      "    Batch 141 / 311: loss 0.5849, accuracy 0.6667\n",
      "    ROC-AUC score: 0.7530\n",
      "    Batch 144 / 311: loss 0.5760, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8788\n",
      "    Batch 147 / 311: loss 0.6258, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7902\n",
      "    Batch 150 / 311: loss 0.6029, accuracy 0.7396\n",
      "    ROC-AUC score: 0.5818\n",
      "    Batch 153 / 311: loss 0.6097, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7652\n",
      "    Batch 156 / 311: loss 0.6145, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6721\n",
      "    Batch 159 / 311: loss 0.5903, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 162 / 311: loss 0.6058, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6883\n",
      "    Batch 165 / 311: loss 0.6687, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7266\n",
      "    Batch 168 / 311: loss 0.6169, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7166\n",
      "    Batch 171 / 311: loss 0.6495, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7625\n",
      "    Batch 174 / 311: loss 0.6366, accuracy 0.6458\n",
      "    ROC-AUC score: 0.7045\n",
      "    Batch 177 / 311: loss 0.6450, accuracy 0.6458\n",
      "    ROC-AUC score: 0.6941\n",
      "    Batch 180 / 311: loss 0.6820, accuracy 0.6354\n",
      "    ROC-AUC score: 0.8594\n",
      "    Batch 183 / 311: loss 0.6087, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 186 / 311: loss 0.5980, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 189 / 311: loss 0.6395, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8135\n",
      "    Batch 192 / 311: loss 0.5993, accuracy 0.6875\n",
      "    ROC-AUC score: 0.8175\n",
      "    Batch 195 / 311: loss 0.6648, accuracy 0.6979\n",
      "    ROC-AUC score: 0.6275\n",
      "    Batch 198 / 311: loss 0.5918, accuracy 0.6667\n",
      "    ROC-AUC score: 0.5873\n",
      "    Batch 201 / 311: loss 0.5979, accuracy 0.7604\n",
      "    ROC-AUC score: 0.6548\n",
      "    Batch 204 / 311: loss 0.6142, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8615\n",
      "    Batch 207 / 311: loss 0.6638, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7578\n",
      "    Batch 210 / 311: loss 0.6063, accuracy 0.6771\n",
      "    ROC-AUC score: 0.9219\n",
      "    Batch 213 / 311: loss 0.5624, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8083\n",
      "    Batch 216 / 311: loss 0.6043, accuracy 0.6875\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 219 / 311: loss 0.6271, accuracy 0.6667\n",
      "    ROC-AUC score: 0.7368\n",
      "    Batch 222 / 311: loss 0.6281, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7652\n",
      "    Batch 225 / 311: loss 0.6655, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8138\n",
      "    Batch 228 / 311: loss 0.5904, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 231 / 311: loss 0.6377, accuracy 0.7708\n",
      "    ROC-AUC score: 0.7854\n",
      "    Batch 234 / 311: loss 0.6428, accuracy 0.6250\n",
      "    ROC-AUC score: 0.7167\n",
      "    Batch 237 / 311: loss 0.5596, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7792\n",
      "    Batch 240 / 311: loss 0.6593, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 243 / 311: loss 0.6013, accuracy 0.6562\n",
      "    ROC-AUC score: 0.9667\n",
      "    Batch 246 / 311: loss 0.5625, accuracy 0.7812\n",
      "    ROC-AUC score: 0.9258\n",
      "    Batch 249 / 311: loss 0.5696, accuracy 0.7604\n",
      "    ROC-AUC score: 0.9177\n",
      "    Batch 252 / 311: loss 0.6090, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8611\n",
      "    Batch 255 / 311: loss 0.5551, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8555\n",
      "    Batch 258 / 311: loss 0.6031, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7381\n",
      "    Batch 261 / 311: loss 0.6698, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8854\n",
      "    Batch 264 / 311: loss 0.5898, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 267 / 311: loss 0.5656, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 270 / 311: loss 0.6513, accuracy 0.6458\n",
      "    ROC-AUC score: 0.8078\n",
      "    Batch 273 / 311: loss 0.6802, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6523\n",
      "    Batch 276 / 311: loss 0.5792, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8381\n",
      "    Batch 279 / 311: loss 0.6500, accuracy 0.7708\n",
      "    ROC-AUC score: 0.7063\n",
      "    Batch 282 / 311: loss 0.6390, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6708\n",
      "    Batch 285 / 311: loss 0.6006, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7262\n",
      "    Batch 288 / 311: loss 0.6226, accuracy 0.7812\n",
      "    ROC-AUC score: 0.7530\n",
      "    Batch 291 / 311: loss 0.6003, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 294 / 311: loss 0.5925, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8770\n",
      "    Batch 297 / 311: loss 0.6169, accuracy 0.6667\n",
      "    ROC-AUC score: 0.7412\n",
      "    Batch 300 / 311: loss 0.7103, accuracy 0.6146\n",
      "    ROC-AUC score: 0.5458\n",
      "    Batch 303 / 311: loss 0.5777, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 306 / 311: loss 0.5679, accuracy 0.7188\n",
      "    ROC-AUC score: 0.6721\n",
      "    Batch 309 / 311: loss 0.6617, accuracy 0.6979\n",
      "    ROC-AUC score: 0.5604\n",
      "Loss 0.6145, accuracy 0.7087\n",
      "ROC-AUC score: 0.7713\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.73      0.71      4964\n",
      "         1.0       0.72      0.69      0.70      4964\n",
      "\n",
      "    accuracy                           0.71      9928\n",
      "   macro avg       0.71      0.71      0.71      9928\n",
      "weighted avg       0.71      0.71      0.71      9928\n",
      "\n",
      "Finished validating.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'val',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the validation dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished validating.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: test\n",
      "Number of vertices: 113\n",
      "Number of static edges: 2096\n",
      "Number of temporal edges: 15613\n",
      "Number of examples/datapoints: 10362\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the test dataset after training.\n",
      "    Batch 3 / 324: loss 0.5995, accuracy 0.6458\n",
      "    ROC-AUC score: 0.8789\n",
      "    Batch 6 / 324: loss 0.6663, accuracy 0.6562\n",
      "    ROC-AUC score: 0.7812\n",
      "    Batch 9 / 324: loss 0.7384, accuracy 0.5729\n",
      "    ROC-AUC score: 0.5952\n",
      "    Batch 12 / 324: loss 0.6753, accuracy 0.6354\n",
      "    ROC-AUC score: 0.6071\n",
      "    Batch 15 / 324: loss 0.7339, accuracy 0.5625\n",
      "    ROC-AUC score: 0.5627\n",
      "    Batch 18 / 324: loss 0.6222, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7617\n",
      "    Batch 21 / 324: loss 0.6779, accuracy 0.6042\n",
      "    ROC-AUC score: 0.6588\n",
      "    Batch 24 / 324: loss 0.6487, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 27 / 324: loss 0.7401, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7895\n",
      "    Batch 30 / 324: loss 0.7316, accuracy 0.5729\n",
      "    ROC-AUC score: 0.6039\n",
      "    Batch 33 / 324: loss 0.6726, accuracy 0.6667\n",
      "    ROC-AUC score: 0.5913\n",
      "    Batch 36 / 324: loss 0.6085, accuracy 0.6250\n",
      "    ROC-AUC score: 0.7874\n",
      "    Batch 39 / 324: loss 0.6856, accuracy 0.5833\n",
      "    ROC-AUC score: 0.5466\n",
      "    Batch 42 / 324: loss 0.6985, accuracy 0.6562\n",
      "    ROC-AUC score: 0.7031\n",
      "    Batch 45 / 324: loss 0.6963, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7206\n",
      "    Batch 48 / 324: loss 0.7081, accuracy 0.5208\n",
      "    ROC-AUC score: 0.6349\n",
      "    Batch 51 / 324: loss 0.5790, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7216\n",
      "    Batch 54 / 324: loss 0.6887, accuracy 0.5729\n",
      "    ROC-AUC score: 0.6784\n",
      "    Batch 57 / 324: loss 0.6352, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 60 / 324: loss 0.7371, accuracy 0.6250\n",
      "    ROC-AUC score: 0.5709\n",
      "    Batch 63 / 324: loss 0.6734, accuracy 0.5625\n",
      "    ROC-AUC score: 0.7373\n",
      "    Batch 66 / 324: loss 0.6776, accuracy 0.5729\n",
      "    ROC-AUC score: 0.6863\n",
      "    Batch 69 / 324: loss 0.7030, accuracy 0.5938\n",
      "    ROC-AUC score: 0.5397\n",
      "    Batch 72 / 324: loss 0.6713, accuracy 0.6458\n",
      "    ROC-AUC score: 0.7792\n",
      "    Batch 75 / 324: loss 0.6851, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7686\n",
      "    Batch 78 / 324: loss 0.6674, accuracy 0.5417\n",
      "    ROC-AUC score: 0.6333\n",
      "    Batch 81 / 324: loss 0.6858, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6125\n",
      "    Batch 84 / 324: loss 0.6662, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6508\n",
      "    Batch 87 / 324: loss 0.6168, accuracy 0.6354\n",
      "    ROC-AUC score: 0.8214\n",
      "    Batch 90 / 324: loss 0.6090, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7545\n",
      "    Batch 93 / 324: loss 0.6053, accuracy 0.6250\n",
      "    ROC-AUC score: 0.7530\n",
      "    Batch 96 / 324: loss 0.6862, accuracy 0.6250\n",
      "    ROC-AUC score: 0.7091\n",
      "    Batch 99 / 324: loss 0.6618, accuracy 0.5938\n",
      "    ROC-AUC score: 0.6314\n",
      "    Batch 102 / 324: loss 0.6117, accuracy 0.6667\n",
      "    ROC-AUC score: 0.8242\n",
      "    Batch 105 / 324: loss 0.7309, accuracy 0.6042\n",
      "    ROC-AUC score: 0.5451\n",
      "    Batch 108 / 324: loss 0.7411, accuracy 0.5625\n",
      "    ROC-AUC score: 0.4708\n",
      "    Batch 111 / 324: loss 0.6702, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6389\n",
      "    Batch 114 / 324: loss 0.6691, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6316\n",
      "    Batch 117 / 324: loss 0.6097, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7891\n",
      "    Batch 120 / 324: loss 0.6274, accuracy 0.6042\n",
      "    ROC-AUC score: 0.6314\n",
      "    Batch 123 / 324: loss 0.6853, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6235\n",
      "    Batch 126 / 324: loss 0.6634, accuracy 0.6458\n",
      "    ROC-AUC score: 0.5833\n",
      "    Batch 129 / 324: loss 0.7134, accuracy 0.5729\n",
      "    ROC-AUC score: 0.6745\n",
      "    Batch 132 / 324: loss 0.6376, accuracy 0.5833\n",
      "    ROC-AUC score: 0.6518\n",
      "    Batch 135 / 324: loss 0.6123, accuracy 0.6458\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 138 / 324: loss 0.7656, accuracy 0.5833\n",
      "    ROC-AUC score: 0.5833\n",
      "    Batch 141 / 324: loss 0.6331, accuracy 0.6458\n",
      "    ROC-AUC score: 0.5714\n",
      "    Batch 144 / 324: loss 0.6743, accuracy 0.6458\n",
      "    ROC-AUC score: 0.7579\n",
      "    Batch 147 / 324: loss 0.7218, accuracy 0.5833\n",
      "    ROC-AUC score: 0.4083\n",
      "    Batch 150 / 324: loss 0.6529, accuracy 0.5312\n",
      "    ROC-AUC score: 0.5750\n",
      "    Batch 153 / 324: loss 0.5899, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8770\n",
      "    Batch 156 / 324: loss 0.7776, accuracy 0.4792\n",
      "    ROC-AUC score: 0.5547\n",
      "    Batch 159 / 324: loss 0.6748, accuracy 0.5729\n",
      "    ROC-AUC score: 0.7024\n",
      "    Batch 162 / 324: loss 0.7138, accuracy 0.5833\n",
      "    ROC-AUC score: 0.6333\n",
      "    Batch 165 / 324: loss 0.6471, accuracy 0.6354\n",
      "    ROC-AUC score: 0.5664\n",
      "    Batch 168 / 324: loss 0.6813, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6310\n",
      "    Batch 171 / 324: loss 0.7632, accuracy 0.5521\n",
      "    ROC-AUC score: 0.5083\n",
      "    Batch 174 / 324: loss 0.6328, accuracy 0.6562\n",
      "    ROC-AUC score: 0.7109\n",
      "    Batch 177 / 324: loss 0.6653, accuracy 0.6042\n",
      "    ROC-AUC score: 0.6761\n",
      "    Batch 180 / 324: loss 0.6499, accuracy 0.6458\n",
      "    ROC-AUC score: 0.7857\n",
      "    Batch 183 / 324: loss 0.6278, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7143\n",
      "    Batch 186 / 324: loss 0.6497, accuracy 0.7083\n",
      "    ROC-AUC score: 0.5294\n",
      "    Batch 189 / 324: loss 0.6950, accuracy 0.5833\n",
      "    ROC-AUC score: 0.7460\n",
      "    Batch 192 / 324: loss 0.7042, accuracy 0.6562\n",
      "    ROC-AUC score: 0.7381\n",
      "    Batch 195 / 324: loss 0.6527, accuracy 0.6354\n",
      "    ROC-AUC score: 0.5098\n",
      "    Batch 198 / 324: loss 0.6092, accuracy 0.6667\n",
      "    ROC-AUC score: 0.7937\n",
      "    Batch 201 / 324: loss 0.6930, accuracy 0.6562\n",
      "    ROC-AUC score: 0.4246\n",
      "    Batch 204 / 324: loss 0.6339, accuracy 0.6354\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 207 / 324: loss 0.7206, accuracy 0.6042\n",
      "    ROC-AUC score: 0.5992\n",
      "    Batch 210 / 324: loss 0.6297, accuracy 0.6562\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 213 / 324: loss 0.7020, accuracy 0.6354\n",
      "    ROC-AUC score: 0.6758\n",
      "    Batch 216 / 324: loss 0.6496, accuracy 0.6146\n",
      "    ROC-AUC score: 0.8047\n",
      "    Batch 219 / 324: loss 0.6349, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6508\n",
      "    Batch 222 / 324: loss 0.6961, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6864\n",
      "    Batch 225 / 324: loss 0.6908, accuracy 0.6042\n",
      "    ROC-AUC score: 0.8135\n",
      "    Batch 228 / 324: loss 0.7272, accuracy 0.5521\n",
      "    ROC-AUC score: 0.4588\n",
      "    Batch 231 / 324: loss 0.6277, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8651\n",
      "    Batch 234 / 324: loss 0.6618, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7255\n",
      "    Batch 237 / 324: loss 0.6257, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8810\n",
      "    Batch 240 / 324: loss 0.6792, accuracy 0.6771\n",
      "    ROC-AUC score: 0.5059\n",
      "    Batch 243 / 324: loss 0.6496, accuracy 0.6146\n",
      "    ROC-AUC score: 0.5797\n",
      "    Batch 246 / 324: loss 0.6897, accuracy 0.6042\n",
      "    ROC-AUC score: 0.5951\n",
      "    Batch 249 / 324: loss 0.6716, accuracy 0.5938\n",
      "    ROC-AUC score: 0.5952\n",
      "    Batch 252 / 324: loss 0.7073, accuracy 0.5521\n",
      "    ROC-AUC score: 0.5425\n",
      "    Batch 255 / 324: loss 0.6018, accuracy 0.6458\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 258 / 324: loss 0.5816, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8355\n",
      "    Batch 261 / 324: loss 0.6570, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7738\n",
      "    Batch 264 / 324: loss 0.7010, accuracy 0.6458\n",
      "    ROC-AUC score: 0.7167\n",
      "    Batch 267 / 324: loss 0.7638, accuracy 0.5417\n",
      "    ROC-AUC score: 0.4417\n",
      "    Batch 270 / 324: loss 0.7150, accuracy 0.6042\n",
      "    ROC-AUC score: 0.5516\n",
      "    Batch 273 / 324: loss 0.7046, accuracy 0.6146\n",
      "    ROC-AUC score: 0.7235\n",
      "    Batch 276 / 324: loss 0.6760, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6548\n",
      "    Batch 279 / 324: loss 0.6347, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7246\n",
      "    Batch 282 / 324: loss 0.6154, accuracy 0.6667\n",
      "    ROC-AUC score: 0.8138\n",
      "    Batch 285 / 324: loss 0.6852, accuracy 0.6146\n",
      "    ROC-AUC score: 0.7292\n",
      "    Batch 288 / 324: loss 0.6844, accuracy 0.6042\n",
      "    ROC-AUC score: 0.6992\n",
      "    Batch 291 / 324: loss 0.6475, accuracy 0.6250\n",
      "    ROC-AUC score: 0.7000\n",
      "    Batch 294 / 324: loss 0.6932, accuracy 0.6250\n",
      "    ROC-AUC score: 0.7446\n",
      "    Batch 297 / 324: loss 0.6734, accuracy 0.6146\n",
      "    ROC-AUC score: 0.5595\n",
      "    Batch 300 / 324: loss 0.6848, accuracy 0.6146\n",
      "    ROC-AUC score: 0.6389\n",
      "    Batch 303 / 324: loss 0.6519, accuracy 0.6458\n",
      "    ROC-AUC score: 0.7864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 306 / 324: loss 0.6418, accuracy 0.6354\n",
      "    ROC-AUC score: 0.8086\n",
      "    Batch 309 / 324: loss 0.7025, accuracy 0.6250\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 312 / 324: loss 0.6665, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6863\n",
      "    Batch 315 / 324: loss 0.6769, accuracy 0.6354\n",
      "    ROC-AUC score: 0.5159\n",
      "    Batch 318 / 324: loss 0.6761, accuracy 0.5833\n",
      "    ROC-AUC score: 0.6580\n",
      "    Batch 321 / 324: loss 0.6918, accuracy 0.6979\n",
      "    ROC-AUC score: 0.5591\n",
      "    Batch 324 / 324: loss 0.6636, accuracy 0.6111\n",
      "    ROC-AUC score: 0.5875\n",
      "Loss 0.6712, accuracy 0.6243\n",
      "ROC-AUC score: 0.6743\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.74      0.66      5181\n",
      "         1.0       0.66      0.51      0.57      5181\n",
      "\n",
      "    accuracy                           0.62     10362\n",
      "   macro avg       0.63      0.62      0.62     10362\n",
      "weighted avg       0.63      0.62      0.62     10362\n",
      "\n",
      "Finished testing.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'test',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the test dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished testing.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
