{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import visdom\n",
    "\n",
    "from datasets import link_prediction\n",
    "from layers import MeanAggregator, LSTMAggregator, MaxPoolAggregator, MeanPoolAggregator\n",
    "import models\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up arguments for datasets, models and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"task\" : \"link_prediction\",\n",
    "    \n",
    "    \"dataset\" : \"IAContactsHypertext\",\n",
    "    \"dataset_path\" : \"/Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"generate_neg_examples\" : False,\n",
    "    \n",
    "    \"duplicate_examples\" : True,\n",
    "    \"repeat_examples\" : True,\n",
    "    \n",
    "    \"self_loop\" : True,\n",
    "    \"normalize_adj\" : False,\n",
    "    \n",
    "    \"cuda\" : \"True\",\n",
    "    \"model\" : \"GraphSAGE\",\n",
    "    \"agg_class\" : \"MeanAggregator\",\n",
    "    \"hidden_dims\" : [32],\n",
    "    \"dropout\" : 0,\n",
    "    \"num_samples\" : -1,\n",
    "    \n",
    "    \"epochs\" : 5,\n",
    "    \"batch_size\" : 32,\n",
    "    \"lr\" : 1e-3,\n",
    "    \"weight_decay\" : 1e-3,\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"visdom\" : True,\n",
    "    \n",
    "    \"load\" : False,\n",
    "    \"save\" : False\n",
    "}\n",
    "config = args\n",
    "config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "\n",
    "if config['cuda'] and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "config['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset, dataloader and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: train\n",
      "Number of vertices: 113\n",
      "Number of static edges: 1010\n",
      "Number of temporal edges: 6245\n",
      "Number of examples/datapoints: 7886\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'train',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "input_dim, output_dim = dataset.get_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE(\n",
      "  (aggregators): ModuleList(\n",
      "    (0): MeanAggregator()\n",
      "    (1): MeanAggregator()\n",
      "  )\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=226, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agg_class = utils.get_agg_class(config['agg_class'])\n",
    "model = models.GraphSAGE(input_dim, config['hidden_dims'],\n",
    "                         output_dim, config['dropout'],\n",
    "                         agg_class, config['num_samples'],\n",
    "                         config['device'])\n",
    "model.to(config['device'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score for the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset before training.\n",
      "    Batch 1 / 247\n",
      "    Batch 2 / 247\n",
      "    Batch 3 / 247\n",
      "    Batch 4 / 247\n",
      "    Batch 5 / 247\n",
      "    Batch 6 / 247\n",
      "    Batch 7 / 247\n",
      "    Batch 8 / 247\n",
      "    Batch 9 / 247\n",
      "    Batch 10 / 247\n",
      "    Batch 11 / 247\n",
      "    Batch 12 / 247\n",
      "    Batch 13 / 247\n",
      "    Batch 14 / 247\n",
      "    Batch 15 / 247\n",
      "    Batch 16 / 247\n",
      "    Batch 17 / 247\n",
      "    Batch 18 / 247\n",
      "    Batch 19 / 247\n",
      "    Batch 20 / 247\n",
      "    Batch 21 / 247\n",
      "    Batch 22 / 247\n",
      "    Batch 23 / 247\n",
      "    Batch 24 / 247\n",
      "    Batch 25 / 247\n",
      "    Batch 26 / 247\n",
      "    Batch 27 / 247\n",
      "    Batch 28 / 247\n",
      "    Batch 29 / 247\n",
      "    Batch 30 / 247\n",
      "    Batch 31 / 247\n",
      "    Batch 32 / 247\n",
      "    Batch 33 / 247\n",
      "    Batch 34 / 247\n",
      "    Batch 35 / 247\n",
      "    Batch 36 / 247\n",
      "    Batch 37 / 247\n",
      "    Batch 38 / 247\n",
      "    Batch 39 / 247\n",
      "    Batch 40 / 247\n",
      "    Batch 41 / 247\n",
      "    Batch 42 / 247\n",
      "    Batch 43 / 247\n",
      "    Batch 44 / 247\n",
      "    Batch 45 / 247\n",
      "    Batch 46 / 247\n",
      "    Batch 47 / 247\n",
      "    Batch 48 / 247\n",
      "    Batch 49 / 247\n",
      "    Batch 50 / 247\n",
      "    Batch 51 / 247\n",
      "    Batch 52 / 247\n",
      "    Batch 53 / 247\n",
      "    Batch 54 / 247\n",
      "    Batch 55 / 247\n",
      "    Batch 56 / 247\n",
      "    Batch 57 / 247\n",
      "    Batch 58 / 247\n",
      "    Batch 59 / 247\n",
      "    Batch 60 / 247\n",
      "    Batch 61 / 247\n",
      "    Batch 62 / 247\n",
      "    Batch 63 / 247\n",
      "    Batch 64 / 247\n",
      "    Batch 65 / 247\n",
      "    Batch 66 / 247\n",
      "    Batch 67 / 247\n",
      "    Batch 68 / 247\n",
      "    Batch 69 / 247\n",
      "    Batch 70 / 247\n",
      "    Batch 71 / 247\n",
      "    Batch 72 / 247\n",
      "    Batch 73 / 247\n",
      "    Batch 74 / 247\n",
      "    Batch 75 / 247\n",
      "    Batch 76 / 247\n",
      "    Batch 77 / 247\n",
      "    Batch 78 / 247\n",
      "    Batch 79 / 247\n",
      "    Batch 80 / 247\n",
      "    Batch 81 / 247\n",
      "    Batch 82 / 247\n",
      "    Batch 83 / 247\n",
      "    Batch 84 / 247\n",
      "    Batch 85 / 247\n",
      "    Batch 86 / 247\n",
      "    Batch 87 / 247\n",
      "    Batch 88 / 247\n",
      "    Batch 89 / 247\n",
      "    Batch 90 / 247\n",
      "    Batch 91 / 247\n",
      "    Batch 92 / 247\n",
      "    Batch 93 / 247\n",
      "    Batch 94 / 247\n",
      "    Batch 95 / 247\n",
      "    Batch 96 / 247\n",
      "    Batch 97 / 247\n",
      "    Batch 98 / 247\n",
      "    Batch 99 / 247\n",
      "    Batch 100 / 247\n",
      "    Batch 101 / 247\n",
      "    Batch 102 / 247\n",
      "    Batch 103 / 247\n",
      "    Batch 104 / 247\n",
      "    Batch 105 / 247\n",
      "    Batch 106 / 247\n",
      "    Batch 107 / 247\n",
      "    Batch 108 / 247\n",
      "    Batch 109 / 247\n",
      "    Batch 110 / 247\n",
      "    Batch 111 / 247\n",
      "    Batch 112 / 247\n",
      "    Batch 113 / 247\n",
      "    Batch 114 / 247\n",
      "    Batch 115 / 247\n",
      "    Batch 116 / 247\n",
      "    Batch 117 / 247\n",
      "    Batch 118 / 247\n",
      "    Batch 119 / 247\n",
      "    Batch 120 / 247\n",
      "    Batch 121 / 247\n",
      "    Batch 122 / 247\n",
      "    Batch 123 / 247\n",
      "    Batch 124 / 247\n",
      "    Batch 125 / 247\n",
      "    Batch 126 / 247\n",
      "    Batch 127 / 247\n",
      "    Batch 128 / 247\n",
      "    Batch 129 / 247\n",
      "    Batch 130 / 247\n",
      "    Batch 131 / 247\n",
      "    Batch 132 / 247\n",
      "    Batch 133 / 247\n",
      "    Batch 134 / 247\n",
      "    Batch 135 / 247\n",
      "    Batch 136 / 247\n",
      "    Batch 137 / 247\n",
      "    Batch 138 / 247\n",
      "    Batch 139 / 247\n",
      "    Batch 140 / 247\n",
      "    Batch 141 / 247\n",
      "    Batch 142 / 247\n",
      "    Batch 143 / 247\n",
      "    Batch 144 / 247\n",
      "    Batch 145 / 247\n",
      "    Batch 146 / 247\n",
      "    Batch 147 / 247\n",
      "    Batch 148 / 247\n",
      "    Batch 149 / 247\n",
      "    Batch 150 / 247\n",
      "    Batch 151 / 247\n",
      "    Batch 152 / 247\n",
      "    Batch 153 / 247\n",
      "    Batch 154 / 247\n",
      "    Batch 155 / 247\n",
      "    Batch 156 / 247\n",
      "    Batch 157 / 247\n",
      "    Batch 158 / 247\n",
      "    Batch 159 / 247\n",
      "    Batch 160 / 247\n",
      "    Batch 161 / 247\n",
      "    Batch 162 / 247\n",
      "    Batch 163 / 247\n",
      "    Batch 164 / 247\n",
      "    Batch 165 / 247\n",
      "    Batch 166 / 247\n",
      "    Batch 167 / 247\n",
      "    Batch 168 / 247\n",
      "    Batch 169 / 247\n",
      "    Batch 170 / 247\n",
      "    Batch 171 / 247\n",
      "    Batch 172 / 247\n",
      "    Batch 173 / 247\n",
      "    Batch 174 / 247\n",
      "    Batch 175 / 247\n",
      "    Batch 176 / 247\n",
      "    Batch 177 / 247\n",
      "    Batch 178 / 247\n",
      "    Batch 179 / 247\n",
      "    Batch 180 / 247\n",
      "    Batch 181 / 247\n",
      "    Batch 182 / 247\n",
      "    Batch 183 / 247\n",
      "    Batch 184 / 247\n",
      "    Batch 185 / 247\n",
      "    Batch 186 / 247\n",
      "    Batch 187 / 247\n",
      "    Batch 188 / 247\n",
      "    Batch 189 / 247\n",
      "    Batch 190 / 247\n",
      "    Batch 191 / 247\n",
      "    Batch 192 / 247\n",
      "    Batch 193 / 247\n",
      "    Batch 194 / 247\n",
      "    Batch 195 / 247\n",
      "    Batch 196 / 247\n",
      "    Batch 197 / 247\n",
      "    Batch 198 / 247\n",
      "    Batch 199 / 247\n",
      "    Batch 200 / 247\n",
      "    Batch 201 / 247\n",
      "    Batch 202 / 247\n",
      "    Batch 203 / 247\n",
      "    Batch 204 / 247\n",
      "    Batch 205 / 247\n",
      "    Batch 206 / 247\n",
      "    Batch 207 / 247\n",
      "    Batch 208 / 247\n",
      "    Batch 209 / 247\n",
      "    Batch 210 / 247\n",
      "    Batch 211 / 247\n",
      "    Batch 212 / 247\n",
      "    Batch 213 / 247\n",
      "    Batch 214 / 247\n",
      "    Batch 215 / 247\n",
      "    Batch 216 / 247\n",
      "    Batch 217 / 247\n",
      "    Batch 218 / 247\n",
      "    Batch 219 / 247\n",
      "    Batch 220 / 247\n",
      "    Batch 221 / 247\n",
      "    Batch 222 / 247\n",
      "    Batch 223 / 247\n",
      "    Batch 224 / 247\n",
      "    Batch 225 / 247\n",
      "    Batch 226 / 247\n",
      "    Batch 227 / 247\n",
      "    Batch 228 / 247\n",
      "    Batch 229 / 247\n",
      "    Batch 230 / 247\n",
      "    Batch 231 / 247\n",
      "    Batch 232 / 247\n",
      "    Batch 233 / 247\n",
      "    Batch 234 / 247\n",
      "    Batch 235 / 247\n",
      "    Batch 236 / 247\n",
      "    Batch 237 / 247\n",
      "    Batch 238 / 247\n",
      "    Batch 239 / 247\n",
      "    Batch 240 / 247\n",
      "    Batch 241 / 247\n",
      "    Batch 242 / 247\n",
      "    Batch 243 / 247\n",
      "    Batch 244 / 247\n",
      "    Batch 245 / 247\n",
      "    Batch 246 / 247\n",
      "    Batch 247 / 247\n",
      "ROC-AUC score: 0.5011\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset before training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Training.\n",
      "Epoch 1 / 5\n",
      "    Batch 3 / 247: loss 0.6935\n",
      "    ROC-AUC score: 0.5137\n",
      "    Batch 6 / 247: loss 0.6938\n",
      "    ROC-AUC score: 0.5238\n",
      "    Batch 9 / 247: loss 0.6907\n",
      "    ROC-AUC score: 0.6923\n",
      "    Batch 12 / 247: loss 0.6924\n",
      "    ROC-AUC score: 0.4762\n",
      "    Batch 15 / 247: loss 0.6885\n",
      "    ROC-AUC score: 0.6508\n",
      "    Batch 18 / 247: loss 0.6919\n",
      "    ROC-AUC score: 0.6445\n",
      "    Batch 21 / 247: loss 0.6898\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 24 / 247: loss 0.6880\n",
      "    ROC-AUC score: 0.4727\n",
      "    Batch 27 / 247: loss 0.6834\n",
      "    ROC-AUC score: 0.8438\n",
      "    Batch 30 / 247: loss 0.6874\n",
      "    ROC-AUC score: 0.6802\n",
      "    Batch 33 / 247: loss 0.6839\n",
      "    ROC-AUC score: 0.6842\n",
      "    Batch 36 / 247: loss 0.6814\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 39 / 247: loss 0.6752\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 42 / 247: loss 0.6814\n",
      "    ROC-AUC score: 0.6865\n",
      "    Batch 45 / 247: loss 0.6741\n",
      "    ROC-AUC score: 0.8690\n",
      "    Batch 48 / 247: loss 0.6584\n",
      "    ROC-AUC score: 0.8647\n",
      "    Batch 51 / 247: loss 0.6810\n",
      "    ROC-AUC score: 0.6450\n",
      "    Batch 54 / 247: loss 0.6658\n",
      "    ROC-AUC score: 0.6389\n",
      "    Batch 57 / 247: loss 0.6504\n",
      "    ROC-AUC score: 0.8789\n",
      "    Batch 60 / 247: loss 0.6408\n",
      "    ROC-AUC score: 0.7656\n",
      "    Batch 63 / 247: loss 0.6477\n",
      "    ROC-AUC score: 0.9490\n",
      "    Batch 66 / 247: loss 0.6401\n",
      "    ROC-AUC score: 0.8417\n",
      "    Batch 69 / 247: loss 0.6484\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 72 / 247: loss 0.6106\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 75 / 247: loss 0.6151\n",
      "    ROC-AUC score: 0.8866\n",
      "    Batch 78 / 247: loss 0.6209\n",
      "    ROC-AUC score: 0.9180\n",
      "    Batch 81 / 247: loss 0.6373\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 84 / 247: loss 0.6135\n",
      "    ROC-AUC score: 0.7917\n",
      "    Batch 87 / 247: loss 0.5904\n",
      "    ROC-AUC score: 0.6680\n",
      "    Batch 90 / 247: loss 0.6339\n",
      "    ROC-AUC score: 0.7098\n",
      "    Batch 93 / 247: loss 0.6063\n",
      "    ROC-AUC score: 0.7579\n",
      "    Batch 96 / 247: loss 0.6456\n",
      "    ROC-AUC score: 0.7579\n",
      "    Batch 99 / 247: loss 0.6046\n",
      "    ROC-AUC score: 0.8254\n",
      "    Batch 102 / 247: loss 0.5878\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 105 / 247: loss 0.5945\n",
      "    ROC-AUC score: 0.8510\n",
      "    Batch 108 / 247: loss 0.6104\n",
      "    ROC-AUC score: 0.7540\n",
      "    Batch 111 / 247: loss 0.6424\n",
      "    ROC-AUC score: 0.7287\n",
      "    Batch 114 / 247: loss 0.6032\n",
      "    ROC-AUC score: 0.8980\n",
      "    Batch 117 / 247: loss 0.6304\n",
      "    ROC-AUC score: 0.7571\n",
      "    Batch 120 / 247: loss 0.6297\n",
      "    ROC-AUC score: 0.7364\n",
      "    Batch 123 / 247: loss 0.5736\n",
      "    ROC-AUC score: 0.9286\n",
      "    Batch 126 / 247: loss 0.6122\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 129 / 247: loss 0.6034\n",
      "    ROC-AUC score: 0.9286\n",
      "    Batch 132 / 247: loss 0.6074\n",
      "    ROC-AUC score: 0.8381\n",
      "    Batch 135 / 247: loss 0.5758\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 138 / 247: loss 0.6662\n",
      "    ROC-AUC score: 0.6923\n",
      "    Batch 141 / 247: loss 0.5753\n",
      "    ROC-AUC score: 0.8941\n",
      "    Batch 144 / 247: loss 0.6069\n",
      "    ROC-AUC score: 0.8375\n",
      "    Batch 147 / 247: loss 0.6319\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 150 / 247: loss 0.5302\n",
      "    ROC-AUC score: 0.8455\n",
      "    Batch 153 / 247: loss 0.6226\n",
      "    ROC-AUC score: 0.7733\n",
      "    Batch 156 / 247: loss 0.5760\n",
      "    ROC-AUC score: 0.9098\n",
      "    Batch 159 / 247: loss 0.5597\n",
      "    ROC-AUC score: 0.7935\n",
      "    Batch 162 / 247: loss 0.5025\n",
      "    ROC-AUC score: 0.9246\n",
      "    Batch 165 / 247: loss 0.5861\n",
      "    ROC-AUC score: 0.8281\n",
      "    Batch 168 / 247: loss 0.5879\n",
      "    ROC-AUC score: 0.7750\n",
      "    Batch 171 / 247: loss 0.6391\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 174 / 247: loss 0.5783\n",
      "    ROC-AUC score: 0.7935\n",
      "    Batch 177 / 247: loss 0.6033\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 180 / 247: loss 0.5536\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 183 / 247: loss 0.5599\n",
      "    ROC-AUC score: 0.8500\n",
      "    Batch 186 / 247: loss 0.6089\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 189 / 247: loss 0.5473\n",
      "    ROC-AUC score: 0.7530\n",
      "    Batch 192 / 247: loss 0.5934\n",
      "    ROC-AUC score: 0.8413\n",
      "    Batch 195 / 247: loss 0.6369\n",
      "    ROC-AUC score: 0.7208\n",
      "    Batch 198 / 247: loss 0.5498\n",
      "    ROC-AUC score: 0.7814\n",
      "    Batch 201 / 247: loss 0.5739\n",
      "    ROC-AUC score: 0.8866\n",
      "    Batch 204 / 247: loss 0.5850\n",
      "    ROC-AUC score: 0.7344\n",
      "    Batch 207 / 247: loss 0.6289\n",
      "    ROC-AUC score: 0.7812\n",
      "    Batch 210 / 247: loss 0.5817\n",
      "    ROC-AUC score: 0.8510\n",
      "    Batch 213 / 247: loss 0.5295\n",
      "    ROC-AUC score: 0.9608\n",
      "    Batch 216 / 247: loss 0.5931\n",
      "    ROC-AUC score: 0.7422\n",
      "    Batch 219 / 247: loss 0.6582\n",
      "    ROC-AUC score: 0.7578\n",
      "    Batch 222 / 247: loss 0.6104\n",
      "    ROC-AUC score: 0.8261\n",
      "    Batch 225 / 247: loss 0.5449\n",
      "    ROC-AUC score: 0.9567\n",
      "    Batch 228 / 247: loss 0.5871\n",
      "    ROC-AUC score: 0.8627\n",
      "    Batch 231 / 247: loss 0.6315\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 234 / 247: loss 0.5863\n",
      "    ROC-AUC score: 0.9603\n",
      "    Batch 237 / 247: loss 0.5992\n",
      "    ROC-AUC score: 0.9004\n",
      "    Batch 240 / 247: loss 0.5928\n",
      "    ROC-AUC score: 0.7695\n",
      "    Batch 243 / 247: loss 0.6401\n",
      "    ROC-AUC score: 0.7857\n",
      "    Batch 246 / 247: loss 0.5714\n",
      "    ROC-AUC score: 0.8452\n",
      "Epoch 2 / 5\n",
      "    Batch 3 / 247: loss 0.5866\n",
      "    ROC-AUC score: 0.8866\n",
      "    Batch 6 / 247: loss 0.6453\n",
      "    ROC-AUC score: 0.6310\n",
      "    Batch 9 / 247: loss 0.5652\n",
      "    ROC-AUC score: 0.8083\n",
      "    Batch 12 / 247: loss 0.6030\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 15 / 247: loss 0.5467\n",
      "    ROC-AUC score: 0.8417\n",
      "    Batch 18 / 247: loss 0.5667\n",
      "    ROC-AUC score: 0.7958\n",
      "    Batch 21 / 247: loss 0.5712\n",
      "    ROC-AUC score: 0.8826\n",
      "    Batch 24 / 247: loss 0.5505\n",
      "    ROC-AUC score: 0.9059\n",
      "    Batch 27 / 247: loss 0.5040\n",
      "    ROC-AUC score: 0.8690\n",
      "    Batch 30 / 247: loss 0.5813\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 33 / 247: loss 0.5367\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 36 / 247: loss 0.5195\n",
      "    ROC-AUC score: 0.9563\n",
      "    Batch 39 / 247: loss 0.6030\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 42 / 247: loss 0.5354\n",
      "    ROC-AUC score: 0.8259\n",
      "    Batch 45 / 247: loss 0.5475\n",
      "    ROC-AUC score: 0.8528\n",
      "    Batch 48 / 247: loss 0.6213\n",
      "    ROC-AUC score: 0.7273\n",
      "    Batch 51 / 247: loss 0.5951\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 54 / 247: loss 0.5827\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 57 / 247: loss 0.5729\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 60 / 247: loss 0.5929\n",
      "    ROC-AUC score: 0.8706\n",
      "    Batch 63 / 247: loss 0.5312\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 66 / 247: loss 0.5016\n",
      "    ROC-AUC score: 0.9258\n",
      "    Batch 69 / 247: loss 0.4945\n",
      "    ROC-AUC score: 0.8810\n",
      "    Batch 72 / 247: loss 0.6032\n",
      "    ROC-AUC score: 0.7216\n",
      "    Batch 75 / 247: loss 0.5725\n",
      "    ROC-AUC score: 0.7659\n",
      "    Batch 78 / 247: loss 0.5421\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 81 / 247: loss 0.6386\n",
      "    ROC-AUC score: 0.6194\n",
      "    Batch 84 / 247: loss 0.5984\n",
      "    ROC-AUC score: 0.7659\n",
      "    Batch 87 / 247: loss 0.5359\n",
      "    ROC-AUC score: 0.9167\n",
      "    Batch 90 / 247: loss 0.5759\n",
      "    ROC-AUC score: 0.7287\n",
      "    Batch 93 / 247: loss 0.6170\n",
      "    ROC-AUC score: 0.8636\n",
      "    Batch 96 / 247: loss 0.5836\n",
      "    ROC-AUC score: 0.8458\n",
      "    Batch 99 / 247: loss 0.5858\n",
      "    ROC-AUC score: 0.6353\n",
      "    Batch 102 / 247: loss 0.5474\n",
      "    ROC-AUC score: 0.8549\n",
      "    Batch 105 / 247: loss 0.5276\n",
      "    ROC-AUC score: 0.8452\n",
      "    Batch 108 / 247: loss 0.5402\n",
      "    ROC-AUC score: 0.7333\n",
      "    Batch 111 / 247: loss 0.5618\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 114 / 247: loss 0.5424\n",
      "    ROC-AUC score: 0.7137\n",
      "    Batch 117 / 247: loss 0.6043\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 120 / 247: loss 0.6048\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 123 / 247: loss 0.4932\n",
      "    ROC-AUC score: 0.8259\n",
      "    Batch 126 / 247: loss 0.6394\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 129 / 247: loss 0.5231\n",
      "    ROC-AUC score: 0.9127\n",
      "    Batch 132 / 247: loss 0.5713\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 135 / 247: loss 0.5732\n",
      "    ROC-AUC score: 0.8086\n",
      "    Batch 138 / 247: loss 0.6348\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 141 / 247: loss 0.5885\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 144 / 247: loss 0.5747\n",
      "    ROC-AUC score: 0.7937\n",
      "    Batch 147 / 247: loss 0.6308\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 150 / 247: loss 0.5267\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 153 / 247: loss 0.5011\n",
      "    ROC-AUC score: 0.9414\n",
      "    Batch 156 / 247: loss 0.5461\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 159 / 247: loss 0.4913\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 162 / 247: loss 0.5399\n",
      "    ROC-AUC score: 0.8690\n",
      "    Batch 165 / 247: loss 0.5249\n",
      "    ROC-AUC score: 0.9922\n",
      "    Batch 168 / 247: loss 0.4933\n",
      "    ROC-AUC score: 0.9150\n",
      "    Batch 171 / 247: loss 0.5450\n",
      "    ROC-AUC score: 0.8543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 174 / 247: loss 0.6775\n",
      "    ROC-AUC score: 0.6706\n",
      "    Batch 177 / 247: loss 0.6931\n",
      "    ROC-AUC score: 0.7738\n",
      "    Batch 180 / 247: loss 0.5436\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 183 / 247: loss 0.5192\n",
      "    ROC-AUC score: 0.8918\n",
      "    Batch 186 / 247: loss 0.5677\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 189 / 247: loss 0.5187\n",
      "    ROC-AUC score: 0.8818\n",
      "    Batch 192 / 247: loss 0.5922\n",
      "    ROC-AUC score: 0.8208\n",
      "    Batch 195 / 247: loss 0.5840\n",
      "    ROC-AUC score: 0.8340\n",
      "    Batch 198 / 247: loss 0.6071\n",
      "    ROC-AUC score: 0.7421\n",
      "    Batch 201 / 247: loss 0.5609\n",
      "    ROC-AUC score: 0.8706\n",
      "    Batch 204 / 247: loss 0.5608\n",
      "    ROC-AUC score: 0.7381\n",
      "    Batch 207 / 247: loss 0.6514\n",
      "    ROC-AUC score: 0.7578\n",
      "    Batch 210 / 247: loss 0.6471\n",
      "    ROC-AUC score: 0.6309\n",
      "    Batch 213 / 247: loss 0.5973\n",
      "    ROC-AUC score: 0.8988\n",
      "    Batch 216 / 247: loss 0.6197\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 219 / 247: loss 0.5602\n",
      "    ROC-AUC score: 0.9127\n",
      "    Batch 222 / 247: loss 0.6334\n",
      "    ROC-AUC score: 0.7031\n",
      "    Batch 225 / 247: loss 0.6137\n",
      "    ROC-AUC score: 0.7686\n",
      "    Batch 228 / 247: loss 0.5620\n",
      "    ROC-AUC score: 0.7897\n",
      "    Batch 231 / 247: loss 0.5369\n",
      "    ROC-AUC score: 0.9042\n",
      "    Batch 234 / 247: loss 0.5095\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 237 / 247: loss 0.5804\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 240 / 247: loss 0.5179\n",
      "    ROC-AUC score: 0.9227\n",
      "    Batch 243 / 247: loss 0.5539\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 246 / 247: loss 0.5323\n",
      "    ROC-AUC score: 0.7961\n",
      "Epoch 3 / 5\n",
      "    Batch 3 / 247: loss 0.5601\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 6 / 247: loss 0.5463\n",
      "    ROC-AUC score: 0.7359\n",
      "    Batch 9 / 247: loss 0.5631\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 12 / 247: loss 0.5735\n",
      "    ROC-AUC score: 0.7578\n",
      "    Batch 15 / 247: loss 0.5157\n",
      "    ROC-AUC score: 0.8636\n",
      "    Batch 18 / 247: loss 0.5583\n",
      "    ROC-AUC score: 0.8792\n",
      "    Batch 21 / 247: loss 0.5106\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 24 / 247: loss 0.5892\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 27 / 247: loss 0.5889\n",
      "    ROC-AUC score: 0.7381\n",
      "    Batch 30 / 247: loss 0.4995\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 33 / 247: loss 0.5498\n",
      "    ROC-AUC score: 0.8958\n",
      "    Batch 36 / 247: loss 0.6151\n",
      "    ROC-AUC score: 0.8583\n",
      "    Batch 39 / 247: loss 0.5249\n",
      "    ROC-AUC score: 0.9137\n",
      "    Batch 42 / 247: loss 0.5596\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 45 / 247: loss 0.5342\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 48 / 247: loss 0.5541\n",
      "    ROC-AUC score: 0.7817\n",
      "    Batch 51 / 247: loss 0.6094\n",
      "    ROC-AUC score: 0.6914\n",
      "    Batch 54 / 247: loss 0.5461\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 57 / 247: loss 0.5413\n",
      "    ROC-AUC score: 0.8225\n",
      "    Batch 60 / 247: loss 0.6037\n",
      "    ROC-AUC score: 0.6761\n",
      "    Batch 63 / 247: loss 0.5808\n",
      "    ROC-AUC score: 0.8792\n",
      "    Batch 66 / 247: loss 0.6554\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 69 / 247: loss 0.4939\n",
      "    ROC-AUC score: 0.8254\n",
      "    Batch 72 / 247: loss 0.5662\n",
      "    ROC-AUC score: 0.8510\n",
      "    Batch 75 / 247: loss 0.6378\n",
      "    ROC-AUC score: 0.7137\n",
      "    Batch 78 / 247: loss 0.5041\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 81 / 247: loss 0.5552\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 84 / 247: loss 0.5822\n",
      "    ROC-AUC score: 0.5569\n",
      "    Batch 87 / 247: loss 0.5985\n",
      "    ROC-AUC score: 0.7625\n",
      "    Batch 90 / 247: loss 0.5579\n",
      "    ROC-AUC score: 0.8785\n",
      "    Batch 93 / 247: loss 0.6047\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 96 / 247: loss 0.5431\n",
      "    ROC-AUC score: 0.8651\n",
      "    Batch 99 / 247: loss 0.5636\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 102 / 247: loss 0.6152\n",
      "    ROC-AUC score: 0.6562\n",
      "    Batch 105 / 247: loss 0.5401\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 108 / 247: loss 0.5204\n",
      "    ROC-AUC score: 0.8500\n",
      "    Batch 111 / 247: loss 0.6468\n",
      "    ROC-AUC score: 0.7540\n",
      "    Batch 114 / 247: loss 0.5787\n",
      "    ROC-AUC score: 0.7812\n",
      "    Batch 117 / 247: loss 0.5772\n",
      "    ROC-AUC score: 0.8789\n",
      "    Batch 120 / 247: loss 0.5038\n",
      "    ROC-AUC score: 0.8364\n",
      "    Batch 123 / 247: loss 0.4497\n",
      "    ROC-AUC score: 0.9137\n",
      "    Batch 126 / 247: loss 0.5080\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 129 / 247: loss 0.5635\n",
      "    ROC-AUC score: 0.8500\n",
      "    Batch 132 / 247: loss 0.5678\n",
      "    ROC-AUC score: 0.8057\n",
      "    Batch 135 / 247: loss 0.5469\n",
      "    ROC-AUC score: 0.8320\n",
      "    Batch 138 / 247: loss 0.5427\n",
      "    ROC-AUC score: 0.8958\n",
      "    Batch 141 / 247: loss 0.6138\n",
      "    ROC-AUC score: 0.7656\n",
      "    Batch 144 / 247: loss 0.4582\n",
      "    ROC-AUC score: 0.8708\n",
      "    Batch 147 / 247: loss 0.5264\n",
      "    ROC-AUC score: 0.8442\n",
      "    Batch 150 / 247: loss 0.6287\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 153 / 247: loss 0.5539\n",
      "    ROC-AUC score: 0.8889\n",
      "    Batch 156 / 247: loss 0.5813\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 159 / 247: loss 0.5481\n",
      "    ROC-AUC score: 0.8941\n",
      "    Batch 162 / 247: loss 0.5460\n",
      "    ROC-AUC score: 0.9190\n",
      "    Batch 165 / 247: loss 0.5786\n",
      "    ROC-AUC score: 0.9603\n",
      "    Batch 168 / 247: loss 0.6694\n",
      "    ROC-AUC score: 0.7608\n",
      "    Batch 171 / 247: loss 0.4581\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 174 / 247: loss 0.5243\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 177 / 247: loss 0.5899\n",
      "    ROC-AUC score: 0.8417\n",
      "    Batch 180 / 247: loss 0.5502\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 183 / 247: loss 0.5308\n",
      "    ROC-AUC score: 0.8672\n",
      "    Batch 186 / 247: loss 0.5512\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 189 / 247: loss 0.4681\n",
      "    ROC-AUC score: 0.8664\n",
      "    Batch 192 / 247: loss 0.5781\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 195 / 247: loss 0.5744\n",
      "    ROC-AUC score: 0.8167\n",
      "    Batch 198 / 247: loss 0.5558\n",
      "    ROC-AUC score: 0.8373\n",
      "    Batch 201 / 247: loss 0.5810\n",
      "    ROC-AUC score: 0.9216\n",
      "    Batch 204 / 247: loss 0.6415\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 207 / 247: loss 0.6194\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 210 / 247: loss 0.4988\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 213 / 247: loss 0.5403\n",
      "    ROC-AUC score: 0.8664\n",
      "    Batch 216 / 247: loss 0.5784\n",
      "    ROC-AUC score: 0.7579\n",
      "    Batch 219 / 247: loss 0.5494\n",
      "    ROC-AUC score: 0.8375\n",
      "    Batch 222 / 247: loss 0.5695\n",
      "    ROC-AUC score: 0.8273\n",
      "    Batch 225 / 247: loss 0.5925\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 228 / 247: loss 0.6171\n",
      "    ROC-AUC score: 0.6211\n",
      "    Batch 231 / 247: loss 0.5506\n",
      "    ROC-AUC score: 0.9167\n",
      "    Batch 234 / 247: loss 0.5951\n",
      "    ROC-AUC score: 0.9137\n",
      "    Batch 237 / 247: loss 0.5913\n",
      "    ROC-AUC score: 0.8340\n",
      "    Batch 240 / 247: loss 0.5359\n",
      "    ROC-AUC score: 0.9216\n",
      "    Batch 243 / 247: loss 0.6234\n",
      "    ROC-AUC score: 0.7421\n",
      "    Batch 246 / 247: loss 0.4982\n",
      "    ROC-AUC score: 0.8588\n",
      "Epoch 4 / 5\n",
      "    Batch 3 / 247: loss 0.5781\n",
      "    ROC-AUC score: 0.8373\n",
      "    Batch 6 / 247: loss 0.5453\n",
      "    ROC-AUC score: 0.9297\n",
      "    Batch 9 / 247: loss 0.5370\n",
      "    ROC-AUC score: 0.7059\n",
      "    Batch 12 / 247: loss 0.6053\n",
      "    ROC-AUC score: 0.7143\n",
      "    Batch 15 / 247: loss 0.5664\n",
      "    ROC-AUC score: 0.8359\n",
      "    Batch 18 / 247: loss 0.5174\n",
      "    ROC-AUC score: 0.8770\n",
      "    Batch 21 / 247: loss 0.5288\n",
      "    ROC-AUC score: 0.8225\n",
      "    Batch 24 / 247: loss 0.5647\n",
      "    ROC-AUC score: 0.9255\n",
      "    Batch 27 / 247: loss 0.5208\n",
      "    ROC-AUC score: 0.8008\n",
      "    Batch 30 / 247: loss 0.5346\n",
      "    ROC-AUC score: 0.7930\n",
      "    Batch 33 / 247: loss 0.5144\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 36 / 247: loss 0.5111\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 39 / 247: loss 0.5233\n",
      "    ROC-AUC score: 0.9405\n",
      "    Batch 42 / 247: loss 0.5562\n",
      "    ROC-AUC score: 0.8627\n",
      "    Batch 45 / 247: loss 0.5569\n",
      "    ROC-AUC score: 0.7652\n",
      "    Batch 48 / 247: loss 0.6203\n",
      "    ROC-AUC score: 0.7328\n",
      "    Batch 51 / 247: loss 0.5668\n",
      "    ROC-AUC score: 0.8711\n",
      "    Batch 54 / 247: loss 0.6142\n",
      "    ROC-AUC score: 0.8773\n",
      "    Batch 57 / 247: loss 0.5938\n",
      "    ROC-AUC score: 0.8281\n",
      "    Batch 60 / 247: loss 0.5503\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 63 / 247: loss 0.4872\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 66 / 247: loss 0.5295\n",
      "    ROC-AUC score: 0.9750\n",
      "    Batch 69 / 247: loss 0.5747\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 72 / 247: loss 0.5381\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 75 / 247: loss 0.5819\n",
      "    ROC-AUC score: 0.9125\n",
      "    Batch 78 / 247: loss 0.5329\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 81 / 247: loss 0.5970\n",
      "    ROC-AUC score: 0.7333\n",
      "    Batch 84 / 247: loss 0.5308\n",
      "    ROC-AUC score: 0.9417\n",
      "    Batch 87 / 247: loss 0.6181\n",
      "    ROC-AUC score: 0.7266\n",
      "    Batch 90 / 247: loss 0.5087\n",
      "    ROC-AUC score: 0.9125\n",
      "    Batch 93 / 247: loss 0.5230\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 96 / 247: loss 0.5339\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 99 / 247: loss 0.5826\n",
      "    ROC-AUC score: 0.7529\n",
      "    Batch 102 / 247: loss 0.5908\n",
      "    ROC-AUC score: 0.8471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 105 / 247: loss 0.5343\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 108 / 247: loss 0.6015\n",
      "    ROC-AUC score: 0.6349\n",
      "    Batch 111 / 247: loss 0.5143\n",
      "    ROC-AUC score: 0.9102\n",
      "    Batch 114 / 247: loss 0.5530\n",
      "    ROC-AUC score: 0.8413\n",
      "    Batch 117 / 247: loss 0.5637\n",
      "    ROC-AUC score: 0.8086\n",
      "    Batch 120 / 247: loss 0.5536\n",
      "    ROC-AUC score: 0.8980\n",
      "    Batch 123 / 247: loss 0.5885\n",
      "    ROC-AUC score: 0.6627\n",
      "    Batch 126 / 247: loss 0.4607\n",
      "    ROC-AUC score: 0.8968\n",
      "    Batch 129 / 247: loss 0.5514\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 132 / 247: loss 0.5476\n",
      "    ROC-AUC score: 0.6984\n",
      "    Batch 135 / 247: loss 0.5414\n",
      "    ROC-AUC score: 0.7814\n",
      "    Batch 138 / 247: loss 0.5640\n",
      "    ROC-AUC score: 0.8510\n",
      "    Batch 141 / 247: loss 0.5978\n",
      "    ROC-AUC score: 0.8849\n",
      "    Batch 144 / 247: loss 0.5742\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 147 / 247: loss 0.5769\n",
      "    ROC-AUC score: 0.7085\n",
      "    Batch 150 / 247: loss 0.5406\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 153 / 247: loss 0.5726\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 156 / 247: loss 0.5898\n",
      "    ROC-AUC score: 0.8770\n",
      "    Batch 159 / 247: loss 0.6027\n",
      "    ROC-AUC score: 0.8727\n",
      "    Batch 162 / 247: loss 0.6760\n",
      "    ROC-AUC score: 0.7750\n",
      "    Batch 165 / 247: loss 0.6043\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 168 / 247: loss 0.5599\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 171 / 247: loss 0.5903\n",
      "    ROC-AUC score: 0.9098\n",
      "    Batch 174 / 247: loss 0.5265\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 177 / 247: loss 0.6177\n",
      "    ROC-AUC score: 0.6941\n",
      "    Batch 180 / 247: loss 0.5388\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 183 / 247: loss 0.5573\n",
      "    ROC-AUC score: 0.7216\n",
      "    Batch 186 / 247: loss 0.5211\n",
      "    ROC-AUC score: 0.8259\n",
      "    Batch 189 / 247: loss 0.6192\n",
      "    ROC-AUC score: 0.5781\n",
      "    Batch 192 / 247: loss 0.5718\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 195 / 247: loss 0.5963\n",
      "    ROC-AUC score: 0.8138\n",
      "    Batch 198 / 247: loss 0.5502\n",
      "    ROC-AUC score: 0.7188\n",
      "    Batch 201 / 247: loss 0.5784\n",
      "    ROC-AUC score: 0.6706\n",
      "    Batch 204 / 247: loss 0.5758\n",
      "    ROC-AUC score: 0.8314\n",
      "    Batch 207 / 247: loss 0.4936\n",
      "    ROC-AUC score: 0.9059\n",
      "    Batch 210 / 247: loss 0.5153\n",
      "    ROC-AUC score: 0.8583\n",
      "    Batch 213 / 247: loss 0.5431\n",
      "    ROC-AUC score: 0.8182\n",
      "    Batch 216 / 247: loss 0.6060\n",
      "    ROC-AUC score: 0.8214\n",
      "    Batch 219 / 247: loss 0.5686\n",
      "    ROC-AUC score: 0.8528\n",
      "    Batch 222 / 247: loss 0.6009\n",
      "    ROC-AUC score: 0.8078\n",
      "    Batch 225 / 247: loss 0.5193\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 228 / 247: loss 0.5702\n",
      "    ROC-AUC score: 0.7422\n",
      "    Batch 231 / 247: loss 0.5508\n",
      "    ROC-AUC score: 0.8623\n",
      "    Batch 234 / 247: loss 0.5200\n",
      "    ROC-AUC score: 0.7686\n",
      "    Batch 237 / 247: loss 0.5865\n",
      "    ROC-AUC score: 0.7941\n",
      "    Batch 240 / 247: loss 0.5309\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 243 / 247: loss 0.5705\n",
      "    ROC-AUC score: 0.7042\n",
      "    Batch 246 / 247: loss 0.5268\n",
      "    ROC-AUC score: 0.7937\n",
      "Epoch 5 / 5\n",
      "    Batch 3 / 247: loss 0.5267\n",
      "    ROC-AUC score: 0.7692\n",
      "    Batch 6 / 247: loss 0.5915\n",
      "    ROC-AUC score: 0.8866\n",
      "    Batch 9 / 247: loss 0.5291\n",
      "    ROC-AUC score: 0.8078\n",
      "    Batch 12 / 247: loss 0.5173\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 15 / 247: loss 0.6399\n",
      "    ROC-AUC score: 0.6927\n",
      "    Batch 18 / 247: loss 0.6152\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 21 / 247: loss 0.5726\n",
      "    ROC-AUC score: 0.8458\n",
      "    Batch 24 / 247: loss 0.5479\n",
      "    ROC-AUC score: 0.8611\n",
      "    Batch 27 / 247: loss 0.5956\n",
      "    ROC-AUC score: 0.7917\n",
      "    Batch 30 / 247: loss 0.5114\n",
      "    ROC-AUC score: 0.8968\n",
      "    Batch 33 / 247: loss 0.5406\n",
      "    ROC-AUC score: 0.9062\n",
      "    Batch 36 / 247: loss 0.4675\n",
      "    ROC-AUC score: 0.9028\n",
      "    Batch 39 / 247: loss 0.5922\n",
      "    ROC-AUC score: 0.7667\n",
      "    Batch 42 / 247: loss 0.4751\n",
      "    ROC-AUC score: 0.9405\n",
      "    Batch 45 / 247: loss 0.6227\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 48 / 247: loss 0.5029\n",
      "    ROC-AUC score: 0.9412\n",
      "    Batch 51 / 247: loss 0.5253\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 54 / 247: loss 0.5537\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 57 / 247: loss 0.5706\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 60 / 247: loss 0.5802\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 63 / 247: loss 0.5474\n",
      "    ROC-AUC score: 0.8902\n",
      "    Batch 66 / 247: loss 0.5352\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 69 / 247: loss 0.4982\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 72 / 247: loss 0.5461\n",
      "    ROC-AUC score: 0.8750\n",
      "    Batch 75 / 247: loss 0.5885\n",
      "    ROC-AUC score: 0.8045\n",
      "    Batch 78 / 247: loss 0.5503\n",
      "    ROC-AUC score: 0.9069\n",
      "    Batch 81 / 247: loss 0.5659\n",
      "    ROC-AUC score: 0.9048\n",
      "    Batch 84 / 247: loss 0.6308\n",
      "    ROC-AUC score: 0.5938\n",
      "    Batch 87 / 247: loss 0.5053\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 90 / 247: loss 0.5629\n",
      "    ROC-AUC score: 0.8340\n",
      "    Batch 93 / 247: loss 0.5415\n",
      "    ROC-AUC score: 0.7449\n",
      "    Batch 96 / 247: loss 0.5979\n",
      "    ROC-AUC score: 0.7373\n",
      "    Batch 99 / 247: loss 0.6076\n",
      "    ROC-AUC score: 0.7698\n",
      "    Batch 102 / 247: loss 0.5868\n",
      "    ROC-AUC score: 0.8792\n",
      "    Batch 105 / 247: loss 0.6021\n",
      "    ROC-AUC score: 0.9059\n",
      "    Batch 108 / 247: loss 0.5221\n",
      "    ROC-AUC score: 0.8510\n",
      "    Batch 111 / 247: loss 0.6272\n",
      "    ROC-AUC score: 0.7778\n",
      "    Batch 114 / 247: loss 0.5596\n",
      "    ROC-AUC score: 0.7804\n",
      "    Batch 117 / 247: loss 0.5293\n",
      "    ROC-AUC score: 0.7383\n",
      "    Batch 120 / 247: loss 0.5731\n",
      "    ROC-AUC score: 0.8213\n",
      "    Batch 123 / 247: loss 0.6145\n",
      "    ROC-AUC score: 0.7652\n",
      "    Batch 126 / 247: loss 0.6095\n",
      "    ROC-AUC score: 0.6250\n",
      "    Batch 129 / 247: loss 0.5187\n",
      "    ROC-AUC score: 0.9333\n",
      "    Batch 132 / 247: loss 0.5659\n",
      "    ROC-AUC score: 0.7679\n",
      "    Batch 135 / 247: loss 0.5358\n",
      "    ROC-AUC score: 0.7937\n",
      "    Batch 138 / 247: loss 0.5343\n",
      "    ROC-AUC score: 0.8254\n",
      "    Batch 141 / 247: loss 0.5193\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 144 / 247: loss 0.5247\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 147 / 247: loss 0.4547\n",
      "    ROC-AUC score: 0.9492\n",
      "    Batch 150 / 247: loss 0.5681\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 153 / 247: loss 0.5634\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 156 / 247: loss 0.6069\n",
      "    ROC-AUC score: 0.9570\n",
      "    Batch 159 / 247: loss 0.5840\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 162 / 247: loss 0.5521\n",
      "    ROC-AUC score: 0.7246\n",
      "    Batch 165 / 247: loss 0.6008\n",
      "    ROC-AUC score: 0.8300\n",
      "    Batch 168 / 247: loss 0.6242\n",
      "    ROC-AUC score: 0.6356\n",
      "    Batch 171 / 247: loss 0.4854\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 174 / 247: loss 0.5408\n",
      "    ROC-AUC score: 0.7698\n",
      "    Batch 177 / 247: loss 0.5907\n",
      "    ROC-AUC score: 0.8704\n",
      "    Batch 180 / 247: loss 0.6056\n",
      "    ROC-AUC score: 0.6746\n",
      "    Batch 183 / 247: loss 0.5645\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 186 / 247: loss 0.5220\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 189 / 247: loss 0.5454\n",
      "    ROC-AUC score: 0.8413\n",
      "    Batch 192 / 247: loss 0.5548\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 195 / 247: loss 0.5743\n",
      "    ROC-AUC score: 0.9325\n",
      "    Batch 198 / 247: loss 0.5492\n",
      "    ROC-AUC score: 0.7255\n",
      "    Batch 201 / 247: loss 0.5672\n",
      "    ROC-AUC score: 0.9167\n",
      "    Batch 204 / 247: loss 0.5224\n",
      "    ROC-AUC score: 0.8594\n",
      "    Batch 207 / 247: loss 0.5215\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 210 / 247: loss 0.5509\n",
      "    ROC-AUC score: 0.8203\n",
      "    Batch 213 / 247: loss 0.5268\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 216 / 247: loss 0.5056\n",
      "    ROC-AUC score: 0.7004\n",
      "    Batch 219 / 247: loss 0.6547\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 222 / 247: loss 0.4908\n",
      "    ROC-AUC score: 0.9686\n",
      "    Batch 225 / 247: loss 0.6145\n",
      "    ROC-AUC score: 0.7617\n",
      "    Batch 228 / 247: loss 0.6267\n",
      "    ROC-AUC score: 0.6468\n",
      "    Batch 231 / 247: loss 0.6133\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 234 / 247: loss 0.5371\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 237 / 247: loss 0.5647\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 240 / 247: loss 0.5136\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 243 / 247: loss 0.5036\n",
      "    ROC-AUC score: 0.8528\n",
      "    Batch 246 / 247: loss 0.5917\n",
      "    ROC-AUC score: 0.7891\n",
      "Finished training.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    use_visdom = config['visdom']\n",
    "    if use_visdom:\n",
    "        vis = visdom.Visdom()\n",
    "        loss_window = None\n",
    "    criterion = utils.get_criterion(config['task'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                           weight_decay=config['weight_decay'])\n",
    "    epochs = config['epochs']\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.8)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 400], gamma=0.7)\n",
    "    model.train()\n",
    "    print('--------------------------------')\n",
    "    print('Training.')\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "        running_loss = 0.0\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            loss = criterion(scores, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "            if (idx + 1) % stats_per_batch == 0:\n",
    "                running_loss /= stats_per_batch\n",
    "                print('    Batch {} / {}: loss {:.4f}'.format(\n",
    "                    idx+1, num_batches, running_loss))\n",
    "                if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "                    area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "                    print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "                running_loss = 0.0\n",
    "                num_correct, num_examples = 0, 0\n",
    "            if use_visdom:\n",
    "                if loss_window is None:\n",
    "                    loss_window = vis.line(\n",
    "                        Y=[loss.item()],\n",
    "                        X=[epoch*num_batches+idx],\n",
    "                        opts=dict(xlabel='batch', ylabel='Loss', title='Training Loss', legend=['Loss']))\n",
    "                else:\n",
    "                    vis.line(\n",
    "                        [loss.item()],\n",
    "                        [epoch*num_batches+idx],\n",
    "                        win=loss_window,\n",
    "                        update='append')\n",
    "            scheduler.step()\n",
    "    if use_visdom:\n",
    "        vis.close(win=loss_window)\n",
    "    print('Finished training.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['load']:\n",
    "    if config['save']:\n",
    "        print('--------------------------------')\n",
    "        directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                'trained_models')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        fname = utils.get_fname(config)\n",
    "        path = os.path.join(directory, fname)\n",
    "        print('Saving model at {}'.format(path))\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print('Finished saving model.')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset after training.\n",
      "    Batch 1 / 247\n",
      "    Batch 2 / 247\n",
      "    Batch 3 / 247\n",
      "    Batch 4 / 247\n",
      "    Batch 5 / 247\n",
      "    Batch 6 / 247\n",
      "    Batch 7 / 247\n",
      "    Batch 8 / 247\n",
      "    Batch 9 / 247\n",
      "    Batch 10 / 247\n",
      "    Batch 11 / 247\n",
      "    Batch 12 / 247\n",
      "    Batch 13 / 247\n",
      "    Batch 14 / 247\n",
      "    Batch 15 / 247\n",
      "    Batch 16 / 247\n",
      "    Batch 17 / 247\n",
      "    Batch 18 / 247\n",
      "    Batch 19 / 247\n",
      "    Batch 20 / 247\n",
      "    Batch 21 / 247\n",
      "    Batch 22 / 247\n",
      "    Batch 23 / 247\n",
      "    Batch 24 / 247\n",
      "    Batch 25 / 247\n",
      "    Batch 26 / 247\n",
      "    Batch 27 / 247\n",
      "    Batch 28 / 247\n",
      "    Batch 29 / 247\n",
      "    Batch 30 / 247\n",
      "    Batch 31 / 247\n",
      "    Batch 32 / 247\n",
      "    Batch 33 / 247\n",
      "    Batch 34 / 247\n",
      "    Batch 35 / 247\n",
      "    Batch 36 / 247\n",
      "    Batch 37 / 247\n",
      "    Batch 38 / 247\n",
      "    Batch 39 / 247\n",
      "    Batch 40 / 247\n",
      "    Batch 41 / 247\n",
      "    Batch 42 / 247\n",
      "    Batch 43 / 247\n",
      "    Batch 44 / 247\n",
      "    Batch 45 / 247\n",
      "    Batch 46 / 247\n",
      "    Batch 47 / 247\n",
      "    Batch 48 / 247\n",
      "    Batch 49 / 247\n",
      "    Batch 50 / 247\n",
      "    Batch 51 / 247\n",
      "    Batch 52 / 247\n",
      "    Batch 53 / 247\n",
      "    Batch 54 / 247\n",
      "    Batch 55 / 247\n",
      "    Batch 56 / 247\n",
      "    Batch 57 / 247\n",
      "    Batch 58 / 247\n",
      "    Batch 59 / 247\n",
      "    Batch 60 / 247\n",
      "    Batch 61 / 247\n",
      "    Batch 62 / 247\n",
      "    Batch 63 / 247\n",
      "    Batch 64 / 247\n",
      "    Batch 65 / 247\n",
      "    Batch 66 / 247\n",
      "    Batch 67 / 247\n",
      "    Batch 68 / 247\n",
      "    Batch 69 / 247\n",
      "    Batch 70 / 247\n",
      "    Batch 71 / 247\n",
      "    Batch 72 / 247\n",
      "    Batch 73 / 247\n",
      "    Batch 74 / 247\n",
      "    Batch 75 / 247\n",
      "    Batch 76 / 247\n",
      "    Batch 77 / 247\n",
      "    Batch 78 / 247\n",
      "    Batch 79 / 247\n",
      "    Batch 80 / 247\n",
      "    Batch 81 / 247\n",
      "    Batch 82 / 247\n",
      "    Batch 83 / 247\n",
      "    Batch 84 / 247\n",
      "    Batch 85 / 247\n",
      "    Batch 86 / 247\n",
      "    Batch 87 / 247\n",
      "    Batch 88 / 247\n",
      "    Batch 89 / 247\n",
      "    Batch 90 / 247\n",
      "    Batch 91 / 247\n",
      "    Batch 92 / 247\n",
      "    Batch 93 / 247\n",
      "    Batch 94 / 247\n",
      "    Batch 95 / 247\n",
      "    Batch 96 / 247\n",
      "    Batch 97 / 247\n",
      "    Batch 98 / 247\n",
      "    Batch 99 / 247\n",
      "    Batch 100 / 247\n",
      "    Batch 101 / 247\n",
      "    Batch 102 / 247\n",
      "    Batch 103 / 247\n",
      "    Batch 104 / 247\n",
      "    Batch 105 / 247\n",
      "    Batch 106 / 247\n",
      "    Batch 107 / 247\n",
      "    Batch 108 / 247\n",
      "    Batch 109 / 247\n",
      "    Batch 110 / 247\n",
      "    Batch 111 / 247\n",
      "    Batch 112 / 247\n",
      "    Batch 113 / 247\n",
      "    Batch 114 / 247\n",
      "    Batch 115 / 247\n",
      "    Batch 116 / 247\n",
      "    Batch 117 / 247\n",
      "    Batch 118 / 247\n",
      "    Batch 119 / 247\n",
      "    Batch 120 / 247\n",
      "    Batch 121 / 247\n",
      "    Batch 122 / 247\n",
      "    Batch 123 / 247\n",
      "    Batch 124 / 247\n",
      "    Batch 125 / 247\n",
      "    Batch 126 / 247\n",
      "    Batch 127 / 247\n",
      "    Batch 128 / 247\n",
      "    Batch 129 / 247\n",
      "    Batch 130 / 247\n",
      "    Batch 131 / 247\n",
      "    Batch 132 / 247\n",
      "    Batch 133 / 247\n",
      "    Batch 134 / 247\n",
      "    Batch 135 / 247\n",
      "    Batch 136 / 247\n",
      "    Batch 137 / 247\n",
      "    Batch 138 / 247\n",
      "    Batch 139 / 247\n",
      "    Batch 140 / 247\n",
      "    Batch 141 / 247\n",
      "    Batch 142 / 247\n",
      "    Batch 143 / 247\n",
      "    Batch 144 / 247\n",
      "    Batch 145 / 247\n",
      "    Batch 146 / 247\n",
      "    Batch 147 / 247\n",
      "    Batch 148 / 247\n",
      "    Batch 149 / 247\n",
      "    Batch 150 / 247\n",
      "    Batch 151 / 247\n",
      "    Batch 152 / 247\n",
      "    Batch 153 / 247\n",
      "    Batch 154 / 247\n",
      "    Batch 155 / 247\n",
      "    Batch 156 / 247\n",
      "    Batch 157 / 247\n",
      "    Batch 158 / 247\n",
      "    Batch 159 / 247\n",
      "    Batch 160 / 247\n",
      "    Batch 161 / 247\n",
      "    Batch 162 / 247\n",
      "    Batch 163 / 247\n",
      "    Batch 164 / 247\n",
      "    Batch 165 / 247\n",
      "    Batch 166 / 247\n",
      "    Batch 167 / 247\n",
      "    Batch 168 / 247\n",
      "    Batch 169 / 247\n",
      "    Batch 170 / 247\n",
      "    Batch 171 / 247\n",
      "    Batch 172 / 247\n",
      "    Batch 173 / 247\n",
      "    Batch 174 / 247\n",
      "    Batch 175 / 247\n",
      "    Batch 176 / 247\n",
      "    Batch 177 / 247\n",
      "    Batch 178 / 247\n",
      "    Batch 179 / 247\n",
      "    Batch 180 / 247\n",
      "    Batch 181 / 247\n",
      "    Batch 182 / 247\n",
      "    Batch 183 / 247\n",
      "    Batch 184 / 247\n",
      "    Batch 185 / 247\n",
      "    Batch 186 / 247\n",
      "    Batch 187 / 247\n",
      "    Batch 188 / 247\n",
      "    Batch 189 / 247\n",
      "    Batch 190 / 247\n",
      "    Batch 191 / 247\n",
      "    Batch 192 / 247\n",
      "    Batch 193 / 247\n",
      "    Batch 194 / 247\n",
      "    Batch 195 / 247\n",
      "    Batch 196 / 247\n",
      "    Batch 197 / 247\n",
      "    Batch 198 / 247\n",
      "    Batch 199 / 247\n",
      "    Batch 200 / 247\n",
      "    Batch 201 / 247\n",
      "    Batch 202 / 247\n",
      "    Batch 203 / 247\n",
      "    Batch 204 / 247\n",
      "    Batch 205 / 247\n",
      "    Batch 206 / 247\n",
      "    Batch 207 / 247\n",
      "    Batch 208 / 247\n",
      "    Batch 209 / 247\n",
      "    Batch 210 / 247\n",
      "    Batch 211 / 247\n",
      "    Batch 212 / 247\n",
      "    Batch 213 / 247\n",
      "    Batch 214 / 247\n",
      "    Batch 215 / 247\n",
      "    Batch 216 / 247\n",
      "    Batch 217 / 247\n",
      "    Batch 218 / 247\n",
      "    Batch 219 / 247\n",
      "    Batch 220 / 247\n",
      "    Batch 221 / 247\n",
      "    Batch 222 / 247\n",
      "    Batch 223 / 247\n",
      "    Batch 224 / 247\n",
      "    Batch 225 / 247\n",
      "    Batch 226 / 247\n",
      "    Batch 227 / 247\n",
      "    Batch 228 / 247\n",
      "    Batch 229 / 247\n",
      "    Batch 230 / 247\n",
      "    Batch 231 / 247\n",
      "    Batch 232 / 247\n",
      "    Batch 233 / 247\n",
      "    Batch 234 / 247\n",
      "    Batch 235 / 247\n",
      "    Batch 236 / 247\n",
      "    Batch 237 / 247\n",
      "    Batch 238 / 247\n",
      "    Batch 239 / 247\n",
      "    Batch 240 / 247\n",
      "    Batch 241 / 247\n",
      "    Batch 242 / 247\n",
      "    Batch 243 / 247\n",
      "    Batch 244 / 247\n",
      "    Batch 245 / 247\n",
      "    Batch 246 / 247\n",
      "    Batch 247 / 247\n",
      "ROC-AUC score: 0.8293\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset after training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the true positive rate and true negative rate vs threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    tpr, fpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    tnr = 1 - fpr\n",
    "    plt.plot(thresholds, tpr, label='tpr')\n",
    "    plt.plot(thresholds, tnr, label='tnr')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.title('TPR / TNR vs Threshold')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an appropriate threshold and generate classification report on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 1 / 247\n",
      "    Batch 2 / 247\n",
      "    Batch 3 / 247\n",
      "    Batch 4 / 247\n",
      "    Batch 5 / 247\n",
      "    Batch 6 / 247\n",
      "    Batch 7 / 247\n",
      "    Batch 8 / 247\n",
      "    Batch 9 / 247\n",
      "    Batch 10 / 247\n",
      "    Batch 11 / 247\n",
      "    Batch 12 / 247\n",
      "    Batch 13 / 247\n",
      "    Batch 14 / 247\n",
      "    Batch 15 / 247\n",
      "    Batch 16 / 247\n",
      "    Batch 17 / 247\n",
      "    Batch 18 / 247\n",
      "    Batch 19 / 247\n",
      "    Batch 20 / 247\n",
      "    Batch 21 / 247\n",
      "    Batch 22 / 247\n",
      "    Batch 23 / 247\n",
      "    Batch 24 / 247\n",
      "    Batch 25 / 247\n",
      "    Batch 26 / 247\n",
      "    Batch 27 / 247\n",
      "    Batch 28 / 247\n",
      "    Batch 29 / 247\n",
      "    Batch 30 / 247\n",
      "    Batch 31 / 247\n",
      "    Batch 32 / 247\n",
      "    Batch 33 / 247\n",
      "    Batch 34 / 247\n",
      "    Batch 35 / 247\n",
      "    Batch 36 / 247\n",
      "    Batch 37 / 247\n",
      "    Batch 38 / 247\n",
      "    Batch 39 / 247\n",
      "    Batch 40 / 247\n",
      "    Batch 41 / 247\n",
      "    Batch 42 / 247\n",
      "    Batch 43 / 247\n",
      "    Batch 44 / 247\n",
      "    Batch 45 / 247\n",
      "    Batch 46 / 247\n",
      "    Batch 47 / 247\n",
      "    Batch 48 / 247\n",
      "    Batch 49 / 247\n",
      "    Batch 50 / 247\n",
      "    Batch 51 / 247\n",
      "    Batch 52 / 247\n",
      "    Batch 53 / 247\n",
      "    Batch 54 / 247\n",
      "    Batch 55 / 247\n",
      "    Batch 56 / 247\n",
      "    Batch 57 / 247\n",
      "    Batch 58 / 247\n",
      "    Batch 59 / 247\n",
      "    Batch 60 / 247\n",
      "    Batch 61 / 247\n",
      "    Batch 62 / 247\n",
      "    Batch 63 / 247\n",
      "    Batch 64 / 247\n",
      "    Batch 65 / 247\n",
      "    Batch 66 / 247\n",
      "    Batch 67 / 247\n",
      "    Batch 68 / 247\n",
      "    Batch 69 / 247\n",
      "    Batch 70 / 247\n",
      "    Batch 71 / 247\n",
      "    Batch 72 / 247\n",
      "    Batch 73 / 247\n",
      "    Batch 74 / 247\n",
      "    Batch 75 / 247\n",
      "    Batch 76 / 247\n",
      "    Batch 77 / 247\n",
      "    Batch 78 / 247\n",
      "    Batch 79 / 247\n",
      "    Batch 80 / 247\n",
      "    Batch 81 / 247\n",
      "    Batch 82 / 247\n",
      "    Batch 83 / 247\n",
      "    Batch 84 / 247\n",
      "    Batch 85 / 247\n",
      "    Batch 86 / 247\n",
      "    Batch 87 / 247\n",
      "    Batch 88 / 247\n",
      "    Batch 89 / 247\n",
      "    Batch 90 / 247\n",
      "    Batch 91 / 247\n",
      "    Batch 92 / 247\n",
      "    Batch 93 / 247\n",
      "    Batch 94 / 247\n",
      "    Batch 95 / 247\n",
      "    Batch 96 / 247\n",
      "    Batch 97 / 247\n",
      "    Batch 98 / 247\n",
      "    Batch 99 / 247\n",
      "    Batch 100 / 247\n",
      "    Batch 101 / 247\n",
      "    Batch 102 / 247\n",
      "    Batch 103 / 247\n",
      "    Batch 104 / 247\n",
      "    Batch 105 / 247\n",
      "    Batch 106 / 247\n",
      "    Batch 107 / 247\n",
      "    Batch 108 / 247\n",
      "    Batch 109 / 247\n",
      "    Batch 110 / 247\n",
      "    Batch 111 / 247\n",
      "    Batch 112 / 247\n",
      "    Batch 113 / 247\n",
      "    Batch 114 / 247\n",
      "    Batch 115 / 247\n",
      "    Batch 116 / 247\n",
      "    Batch 117 / 247\n",
      "    Batch 118 / 247\n",
      "    Batch 119 / 247\n",
      "    Batch 120 / 247\n",
      "    Batch 121 / 247\n",
      "    Batch 122 / 247\n",
      "    Batch 123 / 247\n",
      "    Batch 124 / 247\n",
      "    Batch 125 / 247\n",
      "    Batch 126 / 247\n",
      "    Batch 127 / 247\n",
      "    Batch 128 / 247\n",
      "    Batch 129 / 247\n",
      "    Batch 130 / 247\n",
      "    Batch 131 / 247\n",
      "    Batch 132 / 247\n",
      "    Batch 133 / 247\n",
      "    Batch 134 / 247\n",
      "    Batch 135 / 247\n",
      "    Batch 136 / 247\n",
      "    Batch 137 / 247\n",
      "    Batch 138 / 247\n",
      "    Batch 139 / 247\n",
      "    Batch 140 / 247\n",
      "    Batch 141 / 247\n",
      "    Batch 142 / 247\n",
      "    Batch 143 / 247\n",
      "    Batch 144 / 247\n",
      "    Batch 145 / 247\n",
      "    Batch 146 / 247\n",
      "    Batch 147 / 247\n",
      "    Batch 148 / 247\n",
      "    Batch 149 / 247\n",
      "    Batch 150 / 247\n",
      "    Batch 151 / 247\n",
      "    Batch 152 / 247\n",
      "    Batch 153 / 247\n",
      "    Batch 154 / 247\n",
      "    Batch 155 / 247\n",
      "    Batch 156 / 247\n",
      "    Batch 157 / 247\n",
      "    Batch 158 / 247\n",
      "    Batch 159 / 247\n",
      "    Batch 160 / 247\n",
      "    Batch 161 / 247\n",
      "    Batch 162 / 247\n",
      "    Batch 163 / 247\n",
      "    Batch 164 / 247\n",
      "    Batch 165 / 247\n",
      "    Batch 166 / 247\n",
      "    Batch 167 / 247\n",
      "    Batch 168 / 247\n",
      "    Batch 169 / 247\n",
      "    Batch 170 / 247\n",
      "    Batch 171 / 247\n",
      "    Batch 172 / 247\n",
      "    Batch 173 / 247\n",
      "    Batch 174 / 247\n",
      "    Batch 175 / 247\n",
      "    Batch 176 / 247\n",
      "    Batch 177 / 247\n",
      "    Batch 178 / 247\n",
      "    Batch 179 / 247\n",
      "    Batch 180 / 247\n",
      "    Batch 181 / 247\n",
      "    Batch 182 / 247\n",
      "    Batch 183 / 247\n",
      "    Batch 184 / 247\n",
      "    Batch 185 / 247\n",
      "    Batch 186 / 247\n",
      "    Batch 187 / 247\n",
      "    Batch 188 / 247\n",
      "    Batch 189 / 247\n",
      "    Batch 190 / 247\n",
      "    Batch 191 / 247\n",
      "    Batch 192 / 247\n",
      "    Batch 193 / 247\n",
      "    Batch 194 / 247\n",
      "    Batch 195 / 247\n",
      "    Batch 196 / 247\n",
      "    Batch 197 / 247\n",
      "    Batch 198 / 247\n",
      "    Batch 199 / 247\n",
      "    Batch 200 / 247\n",
      "    Batch 201 / 247\n",
      "    Batch 202 / 247\n",
      "    Batch 203 / 247\n",
      "    Batch 204 / 247\n",
      "    Batch 205 / 247\n",
      "    Batch 206 / 247\n",
      "    Batch 207 / 247\n",
      "    Batch 208 / 247\n",
      "    Batch 209 / 247\n",
      "    Batch 210 / 247\n",
      "    Batch 211 / 247\n",
      "    Batch 212 / 247\n",
      "    Batch 213 / 247\n",
      "    Batch 214 / 247\n",
      "    Batch 215 / 247\n",
      "    Batch 216 / 247\n",
      "    Batch 217 / 247\n",
      "    Batch 218 / 247\n",
      "    Batch 219 / 247\n",
      "    Batch 220 / 247\n",
      "    Batch 221 / 247\n",
      "    Batch 222 / 247\n",
      "    Batch 223 / 247\n",
      "    Batch 224 / 247\n",
      "    Batch 225 / 247\n",
      "    Batch 226 / 247\n",
      "    Batch 227 / 247\n",
      "    Batch 228 / 247\n",
      "    Batch 229 / 247\n",
      "    Batch 230 / 247\n",
      "    Batch 231 / 247\n",
      "    Batch 232 / 247\n",
      "    Batch 233 / 247\n",
      "    Batch 234 / 247\n",
      "    Batch 235 / 247\n",
      "    Batch 236 / 247\n",
      "    Batch 237 / 247\n",
      "    Batch 238 / 247\n",
      "    Batch 239 / 247\n",
      "    Batch 240 / 247\n",
      "    Batch 241 / 247\n",
      "    Batch 242 / 247\n",
      "    Batch 243 / 247\n",
      "    Batch 244 / 247\n",
      "    Batch 245 / 247\n",
      "    Batch 246 / 247\n",
      "    Batch 247 / 247\n",
      "Threshold: 0.4152, accuracy: 0.7605\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.76      0.76      3943\n",
      "         1.0       0.76      0.76      0.76      3943\n",
      "\n",
      "    accuracy                           0.76      7886\n",
      "   macro avg       0.76      0.76      0.76      7886\n",
      "weighted avg       0.76      0.76      0.76      7886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx1 = np.where(tpr <= tnr)[0]\n",
    "idx2 = np.where(tpr >= tnr)[0]\n",
    "t = thresholds[idx1[-1]]\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "with torch.no_grad():\n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        edges, features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        all_pairs = torch.mm(out, out.t())\n",
    "        scores = all_pairs[edges.T]\n",
    "        predictions = (scores >= t).long()\n",
    "        y_true.extend(labels.detach().numpy())\n",
    "        y_pred.extend(predictions.detach().numpy())\n",
    "        total_correct += torch.sum(predictions == labels.long()).item()\n",
    "        total_examples += len(labels) \n",
    "        print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "print('Threshold: {:.4f}, accuracy: {:.4f}'.format(t, total_correct / total_examples))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "print('Classification report\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: val\n",
      "Number of vertices: 113\n",
      "Number of static edges: 1636\n",
      "Number of temporal edges: 10409\n",
      "Number of examples/datapoints: 9928\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the validation dataset after training.\n",
      "    Batch 3 / 311: loss 0.5670, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8889\n",
      "    Batch 6 / 311: loss 0.6169, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7287\n",
      "    Batch 9 / 311: loss 0.5804, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7656\n",
      "    Batch 12 / 311: loss 0.5573, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 15 / 311: loss 0.6659, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6230\n",
      "    Batch 18 / 311: loss 0.6408, accuracy 0.7083\n",
      "    ROC-AUC score: 0.5977\n",
      "    Batch 21 / 311: loss 0.5338, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8706\n",
      "    Batch 24 / 311: loss 0.5713, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8057\n",
      "    Batch 27 / 311: loss 0.6059, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 30 / 311: loss 0.5689, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8701\n",
      "    Batch 33 / 311: loss 0.6048, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 36 / 311: loss 0.6950, accuracy 0.6354\n",
      "    ROC-AUC score: 0.5397\n",
      "    Batch 39 / 311: loss 0.6436, accuracy 0.6875\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 42 / 311: loss 0.5431, accuracy 0.7396\n",
      "    ROC-AUC score: 0.9042\n",
      "    Batch 45 / 311: loss 0.6139, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7917\n",
      "    Batch 48 / 311: loss 0.5753, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7086\n",
      "    Batch 51 / 311: loss 0.5780, accuracy 0.7292\n",
      "    ROC-AUC score: 0.9023\n",
      "    Batch 54 / 311: loss 0.6806, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 57 / 311: loss 0.5727, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8208\n",
      "    Batch 60 / 311: loss 0.5928, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 63 / 311: loss 0.5450, accuracy 0.7708\n",
      "    ROC-AUC score: 0.9643\n",
      "    Batch 66 / 311: loss 0.5592, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 69 / 311: loss 0.5865, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7333\n",
      "    Batch 72 / 311: loss 0.5300, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8273\n",
      "    Batch 75 / 311: loss 0.6654, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8532\n",
      "    Batch 78 / 311: loss 0.5942, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7446\n",
      "    Batch 81 / 311: loss 0.6455, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7875\n",
      "    Batch 84 / 311: loss 0.5965, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 87 / 311: loss 0.6759, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8636\n",
      "    Batch 90 / 311: loss 0.5692, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8750\n",
      "    Batch 93 / 311: loss 0.4975, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8711\n",
      "    Batch 96 / 311: loss 0.6123, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8056\n",
      "    Batch 99 / 311: loss 0.5996, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8785\n",
      "    Batch 102 / 311: loss 0.4932, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8097\n",
      "    Batch 105 / 311: loss 0.6058, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8281\n",
      "    Batch 108 / 311: loss 0.6223, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6944\n",
      "    Batch 111 / 311: loss 0.6334, accuracy 0.6979\n",
      "    ROC-AUC score: 0.9176\n",
      "    Batch 114 / 311: loss 0.6249, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8492\n",
      "    Batch 117 / 311: loss 0.6271, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 120 / 311: loss 0.6405, accuracy 0.6979\n",
      "    ROC-AUC score: 0.6083\n",
      "    Batch 123 / 311: loss 0.5826, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8398\n",
      "    Batch 126 / 311: loss 0.6089, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 129 / 311: loss 0.6604, accuracy 0.6250\n",
      "    ROC-AUC score: 0.8242\n",
      "    Batch 132 / 311: loss 0.6335, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 135 / 311: loss 0.6094, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 138 / 311: loss 0.6172, accuracy 0.6979\n",
      "    ROC-AUC score: 0.6980\n",
      "    Batch 141 / 311: loss 0.6053, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 144 / 311: loss 0.7162, accuracy 0.6979\n",
      "    ROC-AUC score: 0.9028\n",
      "    Batch 147 / 311: loss 0.5987, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7063\n",
      "    Batch 150 / 311: loss 0.5334, accuracy 0.8021\n",
      "    ROC-AUC score: 0.8477\n",
      "    Batch 153 / 311: loss 0.6183, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 156 / 311: loss 0.5561, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7255\n",
      "    Batch 159 / 311: loss 0.6267, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7059\n",
      "    Batch 162 / 311: loss 0.6324, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 165 / 311: loss 0.6254, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8477\n",
      "    Batch 168 / 311: loss 0.5658, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 171 / 311: loss 0.6487, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8528\n",
      "    Batch 174 / 311: loss 0.5669, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 177 / 311: loss 0.5832, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 180 / 311: loss 0.5440, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8828\n",
      "    Batch 183 / 311: loss 0.7087, accuracy 0.6458\n",
      "    ROC-AUC score: 0.6091\n",
      "    Batch 186 / 311: loss 0.6312, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8340\n",
      "    Batch 189 / 311: loss 0.5552, accuracy 0.8021\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 192 / 311: loss 0.5467, accuracy 0.8125\n",
      "    ROC-AUC score: 0.8477\n",
      "    Batch 195 / 311: loss 0.5548, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 198 / 311: loss 0.6751, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 201 / 311: loss 0.6000, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7656\n",
      "    Batch 204 / 311: loss 0.6626, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7608\n",
      "    Batch 207 / 311: loss 0.5929, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 210 / 311: loss 0.6409, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 213 / 311: loss 0.6368, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8773\n",
      "    Batch 216 / 311: loss 0.6503, accuracy 0.7812\n",
      "    ROC-AUC score: 0.6706\n",
      "    Batch 219 / 311: loss 0.6186, accuracy 0.7188\n",
      "    ROC-AUC score: 0.6353\n",
      "    Batch 222 / 311: loss 0.6669, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 225 / 311: loss 0.6592, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6964\n",
      "    Batch 228 / 311: loss 0.6502, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8300\n",
      "    Batch 231 / 311: loss 0.5380, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8824\n",
      "    Batch 234 / 311: loss 0.5868, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8945\n",
      "    Batch 237 / 311: loss 0.6639, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 240 / 311: loss 0.6529, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8664\n",
      "    Batch 243 / 311: loss 0.6212, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8492\n",
      "    Batch 246 / 311: loss 0.5861, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 249 / 311: loss 0.6338, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7698\n",
      "    Batch 252 / 311: loss 0.6567, accuracy 0.7500\n",
      "    ROC-AUC score: 0.6909\n",
      "    Batch 255 / 311: loss 0.5947, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8135\n",
      "    Batch 258 / 311: loss 0.5347, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8789\n",
      "    Batch 261 / 311: loss 0.5748, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8009\n",
      "    Batch 264 / 311: loss 0.5910, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 267 / 311: loss 0.5404, accuracy 0.8021\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 270 / 311: loss 0.5946, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 273 / 311: loss 0.6432, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8500\n",
      "    Batch 276 / 311: loss 0.5744, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8417\n",
      "    Batch 279 / 311: loss 0.5870, accuracy 0.7812\n",
      "    ROC-AUC score: 0.9286\n",
      "    Batch 282 / 311: loss 0.5869, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8213\n",
      "    Batch 285 / 311: loss 0.5756, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 288 / 311: loss 0.6297, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7136\n",
      "    Batch 291 / 311: loss 0.6101, accuracy 0.6458\n",
      "    ROC-AUC score: 0.8398\n",
      "    Batch 294 / 311: loss 0.6131, accuracy 0.8021\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 297 / 311: loss 0.6362, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7852\n",
      "    Batch 300 / 311: loss 0.6325, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7461\n",
      "    Batch 303 / 311: loss 0.5726, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8373\n",
      "    Batch 306 / 311: loss 0.6132, accuracy 0.7917\n",
      "    ROC-AUC score: 0.7490\n",
      "    Batch 309 / 311: loss 0.7573, accuracy 0.5729\n",
      "    ROC-AUC score: 0.5397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.6064, accuracy 0.7283\n",
      "ROC-AUC score: 0.7841\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.71      0.72      4964\n",
      "         1.0       0.72      0.74      0.73      4964\n",
      "\n",
      "    accuracy                           0.73      9928\n",
      "   macro avg       0.73      0.73      0.73      9928\n",
      "weighted avg       0.73      0.73      0.73      9928\n",
      "\n",
      "Finished validating.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'val',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the validation dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished validating.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: test\n",
      "Number of vertices: 113\n",
      "Number of static edges: 2096\n",
      "Number of temporal edges: 15613\n",
      "Number of examples/datapoints: 10362\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the test dataset after training.\n",
      "    Batch 3 / 324: loss 0.6863, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6406\n",
      "    Batch 6 / 324: loss 0.6138, accuracy 0.7500\n",
      "    ROC-AUC score: 0.6833\n",
      "    Batch 9 / 324: loss 0.6949, accuracy 0.6667\n",
      "    ROC-AUC score: 0.8208\n",
      "    Batch 12 / 324: loss 0.6524, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7109\n",
      "    Batch 15 / 324: loss 0.6553, accuracy 0.7083\n",
      "    ROC-AUC score: 0.7148\n",
      "    Batch 18 / 324: loss 0.6827, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6958\n",
      "    Batch 21 / 324: loss 0.7158, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7542\n",
      "    Batch 24 / 324: loss 0.7280, accuracy 0.6458\n",
      "    ROC-AUC score: 0.4625\n",
      "    Batch 27 / 324: loss 0.6547, accuracy 0.7188\n",
      "    ROC-AUC score: 0.6235\n",
      "    Batch 30 / 324: loss 0.6236, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 33 / 324: loss 0.7796, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6190\n",
      "    Batch 36 / 324: loss 0.7204, accuracy 0.6146\n",
      "    ROC-AUC score: 0.5278\n",
      "    Batch 39 / 324: loss 0.6490, accuracy 0.7396\n",
      "    ROC-AUC score: 0.9484\n",
      "    Batch 42 / 324: loss 0.6676, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7185\n",
      "    Batch 45 / 324: loss 0.6315, accuracy 0.6979\n",
      "    ROC-AUC score: 0.6721\n",
      "    Batch 48 / 324: loss 0.5846, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6523\n",
      "    Batch 51 / 324: loss 0.6422, accuracy 0.7708\n",
      "    ROC-AUC score: 0.7302\n",
      "    Batch 54 / 324: loss 0.7424, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 57 / 324: loss 0.6830, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7451\n",
      "    Batch 60 / 324: loss 0.6224, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8980\n",
      "    Batch 63 / 324: loss 0.6554, accuracy 0.7188\n",
      "    ROC-AUC score: 0.5703\n",
      "    Batch 66 / 324: loss 0.6544, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7659\n",
      "    Batch 69 / 324: loss 0.6635, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6914\n",
      "    Batch 72 / 324: loss 0.6985, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6587\n",
      "    Batch 75 / 324: loss 0.6963, accuracy 0.6979\n",
      "    ROC-AUC score: 0.7266\n",
      "    Batch 78 / 324: loss 0.6430, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 81 / 324: loss 0.6406, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7373\n",
      "    Batch 84 / 324: loss 0.6665, accuracy 0.7604\n",
      "    ROC-AUC score: 0.6320\n",
      "    Batch 87 / 324: loss 0.5551, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 90 / 324: loss 0.6619, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7854\n",
      "    Batch 93 / 324: loss 0.7231, accuracy 0.6250\n",
      "    ROC-AUC score: 0.8056\n",
      "    Batch 96 / 324: loss 0.6311, accuracy 0.7604\n",
      "    ROC-AUC score: 0.6154\n",
      "    Batch 99 / 324: loss 0.6684, accuracy 0.7396\n",
      "    ROC-AUC score: 0.5789\n",
      "    Batch 102 / 324: loss 0.6013, accuracy 0.7917\n",
      "    ROC-AUC score: 0.6706\n",
      "    Batch 105 / 324: loss 0.6293, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 108 / 324: loss 0.6508, accuracy 0.6562\n",
      "    ROC-AUC score: 0.7208\n",
      "    Batch 111 / 324: loss 0.6452, accuracy 0.7292\n",
      "    ROC-AUC score: 0.5664\n",
      "    Batch 114 / 324: loss 0.6358, accuracy 0.7188\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 117 / 324: loss 0.6573, accuracy 0.6562\n",
      "    ROC-AUC score: 0.5992\n",
      "    Batch 120 / 324: loss 0.6739, accuracy 0.6667\n",
      "    ROC-AUC score: 0.5909\n",
      "    Batch 123 / 324: loss 0.6725, accuracy 0.7188\n",
      "    ROC-AUC score: 0.5584\n",
      "    Batch 126 / 324: loss 0.7393, accuracy 0.6562\n",
      "    ROC-AUC score: 0.4208\n",
      "    Batch 129 / 324: loss 0.6454, accuracy 0.6979\n",
      "    ROC-AUC score: 0.5344\n",
      "    Batch 132 / 324: loss 0.6431, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6784\n",
      "    Batch 135 / 324: loss 0.6711, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7460\n",
      "    Batch 138 / 324: loss 0.6472, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6397\n",
      "    Batch 141 / 324: loss 0.7195, accuracy 0.6979\n",
      "    ROC-AUC score: 0.5385\n",
      "    Batch 144 / 324: loss 0.6232, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 147 / 324: loss 0.6785, accuracy 0.6146\n",
      "    ROC-AUC score: 0.6235\n",
      "    Batch 150 / 324: loss 0.6694, accuracy 0.7396\n",
      "    ROC-AUC score: 0.4740\n",
      "    Batch 153 / 324: loss 0.7104, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6125\n",
      "    Batch 156 / 324: loss 0.6163, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8254\n",
      "    Batch 159 / 324: loss 0.6693, accuracy 0.7292\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 162 / 324: loss 0.6956, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6407\n",
      "    Batch 165 / 324: loss 0.6385, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7837\n",
      "    Batch 168 / 324: loss 0.6300, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6425\n",
      "    Batch 171 / 324: loss 0.7034, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6270\n",
      "    Batch 174 / 324: loss 0.7198, accuracy 0.6458\n",
      "    ROC-AUC score: 0.5635\n",
      "    Batch 177 / 324: loss 0.6654, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7857\n",
      "    Batch 180 / 324: loss 0.6430, accuracy 0.6771\n",
      "    ROC-AUC score: 0.5750\n",
      "    Batch 183 / 324: loss 0.6865, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6784\n",
      "    Batch 186 / 324: loss 0.6665, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 189 / 324: loss 0.6135, accuracy 0.7396\n",
      "    ROC-AUC score: 0.6356\n",
      "    Batch 192 / 324: loss 0.6874, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7738\n",
      "    Batch 195 / 324: loss 0.6239, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 198 / 324: loss 0.6946, accuracy 0.6875\n",
      "    ROC-AUC score: 0.7083\n",
      "    Batch 201 / 324: loss 0.5810, accuracy 0.7812\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 204 / 324: loss 0.6434, accuracy 0.7500\n",
      "    ROC-AUC score: 0.6194\n",
      "    Batch 207 / 324: loss 0.5759, accuracy 0.7812\n",
      "    ROC-AUC score: 0.9458\n",
      "    Batch 210 / 324: loss 0.6319, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6471\n",
      "    Batch 213 / 324: loss 0.7016, accuracy 0.6146\n",
      "    ROC-AUC score: 0.5312\n",
      "    Batch 216 / 324: loss 0.6971, accuracy 0.6771\n",
      "    ROC-AUC score: 0.5647\n",
      "    Batch 219 / 324: loss 0.6494, accuracy 0.7292\n",
      "    ROC-AUC score: 0.6518\n",
      "    Batch 222 / 324: loss 0.6809, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 225 / 324: loss 0.6406, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6275\n",
      "    Batch 228 / 324: loss 0.6867, accuracy 0.6771\n",
      "    ROC-AUC score: 0.5686\n",
      "    Batch 231 / 324: loss 0.7957, accuracy 0.6354\n",
      "    ROC-AUC score: 0.5820\n",
      "    Batch 234 / 324: loss 0.6051, accuracy 0.7708\n",
      "    ROC-AUC score: 0.9453\n",
      "    Batch 237 / 324: loss 0.7024, accuracy 0.7292\n",
      "    ROC-AUC score: 0.6984\n",
      "    Batch 240 / 324: loss 0.6737, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7421\n",
      "    Batch 243 / 324: loss 0.6446, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7091\n",
      "    Batch 246 / 324: loss 0.7075, accuracy 0.6771\n",
      "    ROC-AUC score: 0.6583\n",
      "    Batch 249 / 324: loss 0.6675, accuracy 0.7292\n",
      "    ROC-AUC score: 0.7255\n",
      "    Batch 252 / 324: loss 0.6297, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7206\n",
      "    Batch 255 / 324: loss 0.6689, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6071\n",
      "    Batch 258 / 324: loss 0.6311, accuracy 0.7708\n",
      "    ROC-AUC score: 0.6706\n",
      "    Batch 261 / 324: loss 0.6690, accuracy 0.7188\n",
      "    ROC-AUC score: 0.6349\n",
      "    Batch 264 / 324: loss 0.6169, accuracy 0.7708\n",
      "    ROC-AUC score: 0.6111\n",
      "    Batch 267 / 324: loss 0.7345, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7143\n",
      "    Batch 270 / 324: loss 0.6793, accuracy 0.7292\n",
      "    ROC-AUC score: 0.5317\n",
      "    Batch 273 / 324: loss 0.6475, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6627\n",
      "    Batch 276 / 324: loss 0.6224, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8086\n",
      "    Batch 279 / 324: loss 0.6306, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 282 / 324: loss 0.6437, accuracy 0.8125\n",
      "    ROC-AUC score: 0.7188\n",
      "    Batch 285 / 324: loss 0.6668, accuracy 0.7812\n",
      "    ROC-AUC score: 0.6958\n",
      "    Batch 288 / 324: loss 0.6748, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8086\n",
      "    Batch 291 / 324: loss 0.5929, accuracy 0.8021\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 294 / 324: loss 0.7548, accuracy 0.6458\n",
      "    ROC-AUC score: 0.6588\n",
      "    Batch 297 / 324: loss 0.6919, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8254\n",
      "    Batch 300 / 324: loss 0.6619, accuracy 0.6667\n",
      "    ROC-AUC score: 0.5238\n",
      "    Batch 303 / 324: loss 0.6469, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 306 / 324: loss 0.7222, accuracy 0.6146\n",
      "    ROC-AUC score: 0.4459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 309 / 324: loss 0.6590, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8792\n",
      "    Batch 312 / 324: loss 0.6735, accuracy 0.7188\n",
      "    ROC-AUC score: 0.7341\n",
      "    Batch 315 / 324: loss 0.6571, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6627\n",
      "    Batch 318 / 324: loss 0.7617, accuracy 0.5938\n",
      "    ROC-AUC score: 0.5882\n",
      "    Batch 321 / 324: loss 0.6371, accuracy 0.7292\n",
      "    ROC-AUC score: 0.6710\n",
      "    Batch 324 / 324: loss 0.6747, accuracy 0.7111\n",
      "    ROC-AUC score: 0.8909\n",
      "Loss 0.6647, accuracy 0.7055\n",
      "ROC-AUC score: 0.6920\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.71      0.71      5181\n",
      "         1.0       0.71      0.70      0.70      5181\n",
      "\n",
      "    accuracy                           0.71     10362\n",
      "   macro avg       0.71      0.71      0.71     10362\n",
      "weighted avg       0.71      0.71      0.71     10362\n",
      "\n",
      "Finished testing.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'test',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the test dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished testing.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
