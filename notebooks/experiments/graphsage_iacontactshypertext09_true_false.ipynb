{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import visdom\n",
    "\n",
    "from datasets import link_prediction\n",
    "from layers import MeanAggregator, LSTMAggregator, MaxPoolAggregator, MeanPoolAggregator\n",
    "import models\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up arguments for datasets, models and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"task\" : \"link_prediction\",\n",
    "    \n",
    "    \"dataset\" : \"IAContactsHypertext\",\n",
    "    \"dataset_path\" : \"/Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"generate_neg_examples\" : True,\n",
    "    \n",
    "    \"duplicate_examples\" : True,\n",
    "    \"repeat_examples\" : False,\n",
    "    \n",
    "    \"self_loop\" : False,\n",
    "    \"normalize_adj\" : False,\n",
    "    \n",
    "    \"cuda\" : \"True\",\n",
    "    \"model\" : \"GraphSAGE\",\n",
    "    \"agg_class\" : \"MaxPoolAggregator\",\n",
    "    \"hidden_dims\" : [64],\n",
    "    \"dropout\" : 0.5,\n",
    "    \"num_samples\" : -1,\n",
    "    \n",
    "    \"epochs\" : 3,\n",
    "    \"batch_size\" : 32,\n",
    "    \"lr\" : 1e-3,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"visdom\" : True,\n",
    "    \n",
    "    \"load\" : False,\n",
    "    \"save\" : False\n",
    "}\n",
    "config = args\n",
    "config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "\n",
    "if config['cuda'] and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "config['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset, dataloader and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: train\n",
      "Number of vertices: 113\n",
      "Number of static edges: 1010\n",
      "Number of temporal edges: 6245\n",
      "Number of examples/datapoints: 7044\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'train',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "input_dim, output_dim = dataset.get_dims()\n",
    "config['input_dim'], config['output_dim'] = input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE(\n",
      "  (aggregators): ModuleList(\n",
      "    (0): MaxPoolAggregator(\n",
      "      (fc1): Linear(in_features=113, out_features=113, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): MaxPoolAggregator(\n",
      "      (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=226, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = utils.get_model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score for the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset before training.\n",
      "    Batch 1 / 221\n",
      "    Batch 2 / 221\n",
      "    Batch 3 / 221\n",
      "    Batch 4 / 221\n",
      "    Batch 5 / 221\n",
      "    Batch 6 / 221\n",
      "    Batch 7 / 221\n",
      "    Batch 8 / 221\n",
      "    Batch 9 / 221\n",
      "    Batch 10 / 221\n",
      "    Batch 11 / 221\n",
      "    Batch 12 / 221\n",
      "    Batch 13 / 221\n",
      "    Batch 14 / 221\n",
      "    Batch 15 / 221\n",
      "    Batch 16 / 221\n",
      "    Batch 17 / 221\n",
      "    Batch 18 / 221\n",
      "    Batch 19 / 221\n",
      "    Batch 20 / 221\n",
      "    Batch 21 / 221\n",
      "    Batch 22 / 221\n",
      "    Batch 23 / 221\n",
      "    Batch 24 / 221\n",
      "    Batch 25 / 221\n",
      "    Batch 26 / 221\n",
      "    Batch 27 / 221\n",
      "    Batch 28 / 221\n",
      "    Batch 29 / 221\n",
      "    Batch 30 / 221\n",
      "    Batch 31 / 221\n",
      "    Batch 32 / 221\n",
      "    Batch 33 / 221\n",
      "    Batch 34 / 221\n",
      "    Batch 35 / 221\n",
      "    Batch 36 / 221\n",
      "    Batch 37 / 221\n",
      "    Batch 38 / 221\n",
      "    Batch 39 / 221\n",
      "    Batch 40 / 221\n",
      "    Batch 41 / 221\n",
      "    Batch 42 / 221\n",
      "    Batch 43 / 221\n",
      "    Batch 44 / 221\n",
      "    Batch 45 / 221\n",
      "    Batch 46 / 221\n",
      "    Batch 47 / 221\n",
      "    Batch 48 / 221\n",
      "    Batch 49 / 221\n",
      "    Batch 50 / 221\n",
      "    Batch 51 / 221\n",
      "    Batch 52 / 221\n",
      "    Batch 53 / 221\n",
      "    Batch 54 / 221\n",
      "    Batch 55 / 221\n",
      "    Batch 56 / 221\n",
      "    Batch 57 / 221\n",
      "    Batch 58 / 221\n",
      "    Batch 59 / 221\n",
      "    Batch 60 / 221\n",
      "    Batch 61 / 221\n",
      "    Batch 62 / 221\n",
      "    Batch 63 / 221\n",
      "    Batch 64 / 221\n",
      "    Batch 65 / 221\n",
      "    Batch 66 / 221\n",
      "    Batch 67 / 221\n",
      "    Batch 68 / 221\n",
      "    Batch 69 / 221\n",
      "    Batch 70 / 221\n",
      "    Batch 71 / 221\n",
      "    Batch 72 / 221\n",
      "    Batch 73 / 221\n",
      "    Batch 74 / 221\n",
      "    Batch 75 / 221\n",
      "    Batch 76 / 221\n",
      "    Batch 77 / 221\n",
      "    Batch 78 / 221\n",
      "    Batch 79 / 221\n",
      "    Batch 80 / 221\n",
      "    Batch 81 / 221\n",
      "    Batch 82 / 221\n",
      "    Batch 83 / 221\n",
      "    Batch 84 / 221\n",
      "    Batch 85 / 221\n",
      "    Batch 86 / 221\n",
      "    Batch 87 / 221\n",
      "    Batch 88 / 221\n",
      "    Batch 89 / 221\n",
      "    Batch 90 / 221\n",
      "    Batch 91 / 221\n",
      "    Batch 92 / 221\n",
      "    Batch 93 / 221\n",
      "    Batch 94 / 221\n",
      "    Batch 95 / 221\n",
      "    Batch 96 / 221\n",
      "    Batch 97 / 221\n",
      "    Batch 98 / 221\n",
      "    Batch 99 / 221\n",
      "    Batch 100 / 221\n",
      "    Batch 101 / 221\n",
      "    Batch 102 / 221\n",
      "    Batch 103 / 221\n",
      "    Batch 104 / 221\n",
      "    Batch 105 / 221\n",
      "    Batch 106 / 221\n",
      "    Batch 107 / 221\n",
      "    Batch 108 / 221\n",
      "    Batch 109 / 221\n",
      "    Batch 110 / 221\n",
      "    Batch 111 / 221\n",
      "    Batch 112 / 221\n",
      "    Batch 113 / 221\n",
      "    Batch 114 / 221\n",
      "    Batch 115 / 221\n",
      "    Batch 116 / 221\n",
      "    Batch 117 / 221\n",
      "    Batch 118 / 221\n",
      "    Batch 119 / 221\n",
      "    Batch 120 / 221\n",
      "    Batch 121 / 221\n",
      "    Batch 122 / 221\n",
      "    Batch 123 / 221\n",
      "    Batch 124 / 221\n",
      "    Batch 125 / 221\n",
      "    Batch 126 / 221\n",
      "    Batch 127 / 221\n",
      "    Batch 128 / 221\n",
      "    Batch 129 / 221\n",
      "    Batch 130 / 221\n",
      "    Batch 131 / 221\n",
      "    Batch 132 / 221\n",
      "    Batch 133 / 221\n",
      "    Batch 134 / 221\n",
      "    Batch 135 / 221\n",
      "    Batch 136 / 221\n",
      "    Batch 137 / 221\n",
      "    Batch 138 / 221\n",
      "    Batch 139 / 221\n",
      "    Batch 140 / 221\n",
      "    Batch 141 / 221\n",
      "    Batch 142 / 221\n",
      "    Batch 143 / 221\n",
      "    Batch 144 / 221\n",
      "    Batch 145 / 221\n",
      "    Batch 146 / 221\n",
      "    Batch 147 / 221\n",
      "    Batch 148 / 221\n",
      "    Batch 149 / 221\n",
      "    Batch 150 / 221\n",
      "    Batch 151 / 221\n",
      "    Batch 152 / 221\n",
      "    Batch 153 / 221\n",
      "    Batch 154 / 221\n",
      "    Batch 155 / 221\n",
      "    Batch 156 / 221\n",
      "    Batch 157 / 221\n",
      "    Batch 158 / 221\n",
      "    Batch 159 / 221\n",
      "    Batch 160 / 221\n",
      "    Batch 161 / 221\n",
      "    Batch 162 / 221\n",
      "    Batch 163 / 221\n",
      "    Batch 164 / 221\n",
      "    Batch 165 / 221\n",
      "    Batch 166 / 221\n",
      "    Batch 167 / 221\n",
      "    Batch 168 / 221\n",
      "    Batch 169 / 221\n",
      "    Batch 170 / 221\n",
      "    Batch 171 / 221\n",
      "    Batch 172 / 221\n",
      "    Batch 173 / 221\n",
      "    Batch 174 / 221\n",
      "    Batch 175 / 221\n",
      "    Batch 176 / 221\n",
      "    Batch 177 / 221\n",
      "    Batch 178 / 221\n",
      "    Batch 179 / 221\n",
      "    Batch 180 / 221\n",
      "    Batch 181 / 221\n",
      "    Batch 182 / 221\n",
      "    Batch 183 / 221\n",
      "    Batch 184 / 221\n",
      "    Batch 185 / 221\n",
      "    Batch 186 / 221\n",
      "    Batch 187 / 221\n",
      "    Batch 188 / 221\n",
      "    Batch 189 / 221\n",
      "    Batch 190 / 221\n",
      "    Batch 191 / 221\n",
      "    Batch 192 / 221\n",
      "    Batch 193 / 221\n",
      "    Batch 194 / 221\n",
      "    Batch 195 / 221\n",
      "    Batch 196 / 221\n",
      "    Batch 197 / 221\n",
      "    Batch 198 / 221\n",
      "    Batch 199 / 221\n",
      "    Batch 200 / 221\n",
      "    Batch 201 / 221\n",
      "    Batch 202 / 221\n",
      "    Batch 203 / 221\n",
      "    Batch 204 / 221\n",
      "    Batch 205 / 221\n",
      "    Batch 206 / 221\n",
      "    Batch 207 / 221\n",
      "    Batch 208 / 221\n",
      "    Batch 209 / 221\n",
      "    Batch 210 / 221\n",
      "    Batch 211 / 221\n",
      "    Batch 212 / 221\n",
      "    Batch 213 / 221\n",
      "    Batch 214 / 221\n",
      "    Batch 215 / 221\n",
      "    Batch 216 / 221\n",
      "    Batch 217 / 221\n",
      "    Batch 218 / 221\n",
      "    Batch 219 / 221\n",
      "    Batch 220 / 221\n",
      "    Batch 221 / 221\n",
      "ROC-AUC score: 0.5431\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset before training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Training.\n",
      "Epoch 1 / 3\n",
      "    Batch 3 / 221: loss 0.6931\n",
      "    ROC-AUC score: 0.5043\n",
      "    Batch 6 / 221: loss 0.6926\n",
      "    ROC-AUC score: 0.8917\n",
      "    Batch 9 / 221: loss 0.6928\n",
      "    ROC-AUC score: 0.5951\n",
      "    Batch 12 / 221: loss 0.6905\n",
      "    ROC-AUC score: 0.7965\n",
      "    Batch 15 / 221: loss 0.6918\n",
      "    ROC-AUC score: 0.7698\n",
      "    Batch 18 / 221: loss 0.6926\n",
      "    ROC-AUC score: 0.6710\n",
      "    Batch 21 / 221: loss 0.6898\n",
      "    ROC-AUC score: 0.8208\n",
      "    Batch 24 / 221: loss 0.6894\n",
      "    ROC-AUC score: 0.8167\n",
      "    Batch 27 / 221: loss 0.6886\n",
      "    ROC-AUC score: 0.7333\n",
      "    Batch 30 / 221: loss 0.6875\n",
      "    ROC-AUC score: 0.7333\n",
      "    Batch 33 / 221: loss 0.6848\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 36 / 221: loss 0.6823\n",
      "    ROC-AUC score: 0.7530\n",
      "    Batch 39 / 221: loss 0.6814\n",
      "    ROC-AUC score: 0.6429\n",
      "    Batch 42 / 221: loss 0.6753\n",
      "    ROC-AUC score: 0.8770\n",
      "    Batch 45 / 221: loss 0.6698\n",
      "    ROC-AUC score: 0.7539\n",
      "    Batch 48 / 221: loss 0.6645\n",
      "    ROC-AUC score: 0.7542\n",
      "    Batch 51 / 221: loss 0.6732\n",
      "    ROC-AUC score: 0.7024\n",
      "    Batch 54 / 221: loss 0.6681\n",
      "    ROC-AUC score: 0.5500\n",
      "    Batch 57 / 221: loss 0.6576\n",
      "    ROC-AUC score: 0.8500\n",
      "    Batch 60 / 221: loss 0.6672\n",
      "    ROC-AUC score: 0.6583\n",
      "    Batch 63 / 221: loss 0.6861\n",
      "    ROC-AUC score: 0.7446\n",
      "    Batch 66 / 221: loss 0.6266\n",
      "    ROC-AUC score: 0.7529\n",
      "    Batch 69 / 221: loss 0.6388\n",
      "    ROC-AUC score: 0.7965\n",
      "    Batch 72 / 221: loss 0.6552\n",
      "    ROC-AUC score: 0.7686\n",
      "    Batch 75 / 221: loss 0.6624\n",
      "    ROC-AUC score: 0.6154\n",
      "    Batch 78 / 221: loss 0.6212\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 81 / 221: loss 0.6235\n",
      "    ROC-AUC score: 0.8633\n",
      "    Batch 84 / 221: loss 0.6506\n",
      "    ROC-AUC score: 0.7857\n",
      "    Batch 87 / 221: loss 0.6230\n",
      "    ROC-AUC score: 0.8355\n",
      "    Batch 90 / 221: loss 0.6288\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 93 / 221: loss 0.6427\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 96 / 221: loss 0.6149\n",
      "    ROC-AUC score: 0.7422\n",
      "    Batch 99 / 221: loss 0.5824\n",
      "    ROC-AUC score: 0.9372\n",
      "    Batch 102 / 221: loss 0.6447\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 105 / 221: loss 0.6324\n",
      "    ROC-AUC score: 0.7103\n",
      "    Batch 108 / 221: loss 0.6172\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 111 / 221: loss 0.6221\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 114 / 221: loss 0.6503\n",
      "    ROC-AUC score: 0.7714\n",
      "    Batch 117 / 221: loss 0.5790\n",
      "    ROC-AUC score: 0.8214\n",
      "    Batch 120 / 221: loss 0.6458\n",
      "    ROC-AUC score: 0.7166\n",
      "    Batch 123 / 221: loss 0.5793\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 126 / 221: loss 0.6247\n",
      "    ROC-AUC score: 0.6599\n",
      "    Batch 129 / 221: loss 0.6152\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 132 / 221: loss 0.5543\n",
      "    ROC-AUC score: 0.7617\n",
      "    Batch 135 / 221: loss 0.6110\n",
      "    ROC-AUC score: 0.8214\n",
      "    Batch 138 / 221: loss 0.5824\n",
      "    ROC-AUC score: 0.8555\n",
      "    Batch 141 / 221: loss 0.5415\n",
      "    ROC-AUC score: 0.9167\n",
      "    Batch 144 / 221: loss 0.6082\n",
      "    ROC-AUC score: 0.8139\n",
      "    Batch 147 / 221: loss 0.5291\n",
      "    ROC-AUC score: 0.8225\n",
      "    Batch 150 / 221: loss 0.6276\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 153 / 221: loss 0.6441\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 156 / 221: loss 0.5610\n",
      "    ROC-AUC score: 0.9137\n",
      "    Batch 159 / 221: loss 0.6086\n",
      "    ROC-AUC score: 0.5909\n",
      "    Batch 162 / 221: loss 0.6016\n",
      "    ROC-AUC score: 0.6542\n",
      "    Batch 165 / 221: loss 0.5928\n",
      "    ROC-AUC score: 0.8551\n",
      "    Batch 168 / 221: loss 0.5826\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 171 / 221: loss 0.5567\n",
      "    ROC-AUC score: 0.8019\n",
      "    Batch 174 / 221: loss 0.6038\n",
      "    ROC-AUC score: 0.7294\n",
      "    Batch 177 / 221: loss 0.5700\n",
      "    ROC-AUC score: 0.8254\n",
      "    Batch 180 / 221: loss 0.6216\n",
      "    ROC-AUC score: 0.8320\n",
      "    Batch 183 / 221: loss 0.5482\n",
      "    ROC-AUC score: 0.7222\n",
      "    Batch 186 / 221: loss 0.5170\n",
      "    ROC-AUC score: 0.8373\n",
      "    Batch 189 / 221: loss 0.5734\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 192 / 221: loss 0.6305\n",
      "    ROC-AUC score: 0.8167\n",
      "    Batch 195 / 221: loss 0.5703\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 198 / 221: loss 0.6292\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 201 / 221: loss 0.5607\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 204 / 221: loss 0.5535\n",
      "    ROC-AUC score: 0.9405\n",
      "    Batch 207 / 221: loss 0.5558\n",
      "    ROC-AUC score: 0.7417\n",
      "    Batch 210 / 221: loss 0.6712\n",
      "    ROC-AUC score: 0.8510\n",
      "    Batch 213 / 221: loss 0.6179\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 216 / 221: loss 0.6866\n",
      "    ROC-AUC score: 0.7100\n",
      "    Batch 219 / 221: loss 0.6374\n",
      "    ROC-AUC score: 0.7804\n",
      "Epoch 2 / 3\n",
      "    Batch 3 / 221: loss 0.6138\n",
      "    ROC-AUC score: 0.7305\n",
      "    Batch 6 / 221: loss 0.5733\n",
      "    ROC-AUC score: 0.8135\n",
      "    Batch 9 / 221: loss 0.5474\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 12 / 221: loss 0.6369\n",
      "    ROC-AUC score: 0.7583\n",
      "    Batch 15 / 221: loss 0.5141\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 18 / 221: loss 0.5933\n",
      "    ROC-AUC score: 0.8792\n",
      "    Batch 21 / 221: loss 0.5839\n",
      "    ROC-AUC score: 0.7045\n",
      "    Batch 24 / 221: loss 0.6043\n",
      "    ROC-AUC score: 0.7396\n",
      "    Batch 27 / 221: loss 0.5809\n",
      "    ROC-AUC score: 0.8831\n",
      "    Batch 30 / 221: loss 0.5987\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 33 / 221: loss 0.5625\n",
      "    ROC-AUC score: 0.8583\n",
      "    Batch 36 / 221: loss 0.5791\n",
      "    ROC-AUC score: 0.9336\n",
      "    Batch 39 / 221: loss 0.5281\n",
      "    ROC-AUC score: 0.8945\n",
      "    Batch 42 / 221: loss 0.6408\n",
      "    ROC-AUC score: 0.7013\n",
      "    Batch 45 / 221: loss 0.5538\n",
      "    ROC-AUC score: 0.9246\n",
      "    Batch 48 / 221: loss 0.5215\n",
      "    ROC-AUC score: 0.8664\n",
      "    Batch 51 / 221: loss 0.5090\n",
      "    ROC-AUC score: 0.9102\n",
      "    Batch 54 / 221: loss 0.5927\n",
      "    ROC-AUC score: 0.7373\n",
      "    Batch 57 / 221: loss 0.5459\n",
      "    ROC-AUC score: 0.7292\n",
      "    Batch 60 / 221: loss 0.5674\n",
      "    ROC-AUC score: 0.7292\n",
      "    Batch 63 / 221: loss 0.5767\n",
      "    ROC-AUC score: 0.8510\n",
      "    Batch 66 / 221: loss 0.4931\n",
      "    ROC-AUC score: 0.9246\n",
      "    Batch 69 / 221: loss 0.5276\n",
      "    ROC-AUC score: 0.8008\n",
      "    Batch 72 / 221: loss 0.5904\n",
      "    ROC-AUC score: 0.8543\n",
      "    Batch 75 / 221: loss 0.5593\n",
      "    ROC-AUC score: 0.8500\n",
      "    Batch 78 / 221: loss 0.5586\n",
      "    ROC-AUC score: 0.7451\n",
      "    Batch 81 / 221: loss 0.5340\n",
      "    ROC-AUC score: 0.9141\n",
      "    Batch 84 / 221: loss 0.5723\n",
      "    ROC-AUC score: 0.6914\n",
      "    Batch 87 / 221: loss 0.5642\n",
      "    ROC-AUC score: 0.8594\n",
      "    Batch 90 / 221: loss 0.5989\n",
      "    ROC-AUC score: 0.7725\n",
      "    Batch 93 / 221: loss 0.5750\n",
      "    ROC-AUC score: 0.7955\n",
      "    Batch 96 / 221: loss 0.5711\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 99 / 221: loss 0.5688\n",
      "    ROC-AUC score: 0.9004\n",
      "    Batch 102 / 221: loss 0.6163\n",
      "    ROC-AUC score: 0.8292\n",
      "    Batch 105 / 221: loss 0.5575\n",
      "    ROC-AUC score: 0.7750\n",
      "    Batch 108 / 221: loss 0.5341\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 111 / 221: loss 0.6585\n",
      "    ROC-AUC score: 0.6812\n",
      "    Batch 114 / 221: loss 0.5165\n",
      "    ROC-AUC score: 0.8373\n",
      "    Batch 117 / 221: loss 0.5802\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 120 / 221: loss 0.6162\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 123 / 221: loss 0.6191\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 126 / 221: loss 0.5790\n",
      "    ROC-AUC score: 0.8227\n",
      "    Batch 129 / 221: loss 0.5947\n",
      "    ROC-AUC score: 0.7246\n",
      "    Batch 132 / 221: loss 0.5097\n",
      "    ROC-AUC score: 0.8730\n",
      "    Batch 135 / 221: loss 0.4910\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 138 / 221: loss 0.5982\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 141 / 221: loss 0.5293\n",
      "    ROC-AUC score: 0.8583\n",
      "    Batch 144 / 221: loss 0.5314\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 147 / 221: loss 0.5048\n",
      "    ROC-AUC score: 0.7917\n",
      "    Batch 150 / 221: loss 0.5364\n",
      "    ROC-AUC score: 0.6958\n",
      "    Batch 153 / 221: loss 0.5842\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 156 / 221: loss 0.5520\n",
      "    ROC-AUC score: 0.9091\n",
      "    Batch 159 / 221: loss 0.6029\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 162 / 221: loss 0.5955\n",
      "    ROC-AUC score: 0.8009\n",
      "    Batch 165 / 221: loss 0.4609\n",
      "    ROC-AUC score: 0.9083\n",
      "    Batch 168 / 221: loss 0.5609\n",
      "    ROC-AUC score: 0.7449\n",
      "    Batch 171 / 221: loss 0.6096\n",
      "    ROC-AUC score: 0.7500\n",
      "    Batch 174 / 221: loss 0.5982\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 177 / 221: loss 0.5871\n",
      "    ROC-AUC score: 0.7891\n",
      "    Batch 180 / 221: loss 0.6542\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 183 / 221: loss 0.5124\n",
      "    ROC-AUC score: 0.8381\n",
      "    Batch 186 / 221: loss 0.5476\n",
      "    ROC-AUC score: 0.6587\n",
      "    Batch 189 / 221: loss 0.5969\n",
      "    ROC-AUC score: 0.8833\n",
      "    Batch 192 / 221: loss 0.6296\n",
      "    ROC-AUC score: 0.7608\n",
      "    Batch 195 / 221: loss 0.6774\n",
      "    ROC-AUC score: 0.6863\n",
      "    Batch 198 / 221: loss 0.5290\n",
      "    ROC-AUC score: 0.8042\n",
      "    Batch 201 / 221: loss 0.6995\n",
      "    ROC-AUC score: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 204 / 221: loss 0.5994\n",
      "    ROC-AUC score: 0.6792\n",
      "    Batch 207 / 221: loss 0.5959\n",
      "    ROC-AUC score: 0.7020\n",
      "    Batch 210 / 221: loss 0.6213\n",
      "    ROC-AUC score: 0.6471\n",
      "    Batch 213 / 221: loss 0.5886\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 216 / 221: loss 0.5880\n",
      "    ROC-AUC score: 0.7738\n",
      "    Batch 219 / 221: loss 0.5345\n",
      "    ROC-AUC score: 0.8770\n",
      "Epoch 3 / 3\n",
      "    Batch 3 / 221: loss 0.4993\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 6 / 221: loss 0.5531\n",
      "    ROC-AUC score: 0.8492\n",
      "    Batch 9 / 221: loss 0.5786\n",
      "    ROC-AUC score: 0.8658\n",
      "    Batch 12 / 221: loss 0.6104\n",
      "    ROC-AUC score: 0.8627\n",
      "    Batch 15 / 221: loss 0.5034\n",
      "    ROC-AUC score: 0.8091\n",
      "    Batch 18 / 221: loss 0.5450\n",
      "    ROC-AUC score: 0.8458\n",
      "    Batch 21 / 221: loss 0.5920\n",
      "    ROC-AUC score: 0.8139\n",
      "    Batch 24 / 221: loss 0.5379\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 27 / 221: loss 0.5618\n",
      "    ROC-AUC score: 0.9150\n",
      "    Batch 30 / 221: loss 0.5933\n",
      "    ROC-AUC score: 0.7126\n",
      "    Batch 33 / 221: loss 0.5936\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 36 / 221: loss 0.5223\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 39 / 221: loss 0.5309\n",
      "    ROC-AUC score: 0.8477\n",
      "    Batch 42 / 221: loss 0.6224\n",
      "    ROC-AUC score: 0.8009\n",
      "    Batch 45 / 221: loss 0.5353\n",
      "    ROC-AUC score: 0.8300\n",
      "    Batch 48 / 221: loss 0.5196\n",
      "    ROC-AUC score: 0.9555\n",
      "    Batch 51 / 221: loss 0.5322\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 54 / 221: loss 0.5393\n",
      "    ROC-AUC score: 0.7835\n",
      "    Batch 57 / 221: loss 0.5987\n",
      "    ROC-AUC score: 0.7698\n",
      "    Batch 60 / 221: loss 0.5284\n",
      "    ROC-AUC score: 0.7879\n",
      "    Batch 63 / 221: loss 0.5526\n",
      "    ROC-AUC score: 0.8785\n",
      "    Batch 66 / 221: loss 0.5517\n",
      "    ROC-AUC score: 0.7857\n",
      "    Batch 69 / 221: loss 0.5911\n",
      "    ROC-AUC score: 0.8500\n",
      "    Batch 72 / 221: loss 0.6648\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 75 / 221: loss 0.5811\n",
      "    ROC-AUC score: 0.8867\n",
      "    Batch 78 / 221: loss 0.5585\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 81 / 221: loss 0.5560\n",
      "    ROC-AUC score: 0.7814\n",
      "    Batch 84 / 221: loss 0.5301\n",
      "    ROC-AUC score: 0.8097\n",
      "    Batch 87 / 221: loss 0.6005\n",
      "    ROC-AUC score: 0.7935\n",
      "    Batch 90 / 221: loss 0.6364\n",
      "    ROC-AUC score: 0.8500\n",
      "    Batch 93 / 221: loss 0.5262\n",
      "    ROC-AUC score: 0.9023\n",
      "    Batch 96 / 221: loss 0.5017\n",
      "    ROC-AUC score: 0.9087\n",
      "    Batch 99 / 221: loss 0.5518\n",
      "    ROC-AUC score: 0.7773\n",
      "    Batch 102 / 221: loss 0.5638\n",
      "    ROC-AUC score: 0.6406\n",
      "    Batch 105 / 221: loss 0.5361\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 108 / 221: loss 0.5563\n",
      "    ROC-AUC score: 0.8706\n",
      "    Batch 111 / 221: loss 0.6368\n",
      "    ROC-AUC score: 0.7935\n",
      "    Batch 114 / 221: loss 0.5238\n",
      "    ROC-AUC score: 0.8254\n",
      "    Batch 117 / 221: loss 0.5597\n",
      "    ROC-AUC score: 0.8254\n",
      "    Batch 120 / 221: loss 0.5813\n",
      "    ROC-AUC score: 0.7098\n",
      "    Batch 123 / 221: loss 0.5970\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 126 / 221: loss 0.5860\n",
      "    ROC-AUC score: 0.9062\n",
      "    Batch 129 / 221: loss 0.5813\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 132 / 221: loss 0.5698\n",
      "    ROC-AUC score: 0.9375\n",
      "    Batch 135 / 221: loss 0.6043\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 138 / 221: loss 0.5928\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 141 / 221: loss 0.5188\n",
      "    ROC-AUC score: 0.8083\n",
      "    Batch 144 / 221: loss 0.4965\n",
      "    ROC-AUC score: 0.8945\n",
      "    Batch 147 / 221: loss 0.5754\n",
      "    ROC-AUC score: 0.7530\n",
      "    Batch 150 / 221: loss 0.5544\n",
      "    ROC-AUC score: 0.7539\n",
      "    Batch 153 / 221: loss 0.5177\n",
      "    ROC-AUC score: 0.8672\n",
      "    Batch 156 / 221: loss 0.5443\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 159 / 221: loss 0.6179\n",
      "    ROC-AUC score: 0.7344\n",
      "    Batch 162 / 221: loss 0.6854\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 165 / 221: loss 0.6353\n",
      "    ROC-AUC score: 0.7611\n",
      "    Batch 168 / 221: loss 0.5899\n",
      "    ROC-AUC score: 0.8810\n",
      "    Batch 171 / 221: loss 0.6183\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 174 / 221: loss 0.5790\n",
      "    ROC-AUC score: 0.8242\n",
      "    Batch 177 / 221: loss 0.6057\n",
      "    ROC-AUC score: 0.6548\n",
      "    Batch 180 / 221: loss 0.5697\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 183 / 221: loss 0.5494\n",
      "    ROC-AUC score: 0.8824\n",
      "    Batch 186 / 221: loss 0.5862\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 189 / 221: loss 0.5392\n",
      "    ROC-AUC score: 0.8392\n",
      "    Batch 192 / 221: loss 0.5553\n",
      "    ROC-AUC score: 0.7692\n",
      "    Batch 195 / 221: loss 0.5088\n",
      "    ROC-AUC score: 0.8091\n",
      "    Batch 198 / 221: loss 0.5096\n",
      "    ROC-AUC score: 0.8902\n",
      "    Batch 201 / 221: loss 0.5141\n",
      "    ROC-AUC score: 0.9405\n",
      "    Batch 204 / 221: loss 0.5548\n",
      "    ROC-AUC score: 0.7373\n",
      "    Batch 207 / 221: loss 0.6139\n",
      "    ROC-AUC score: 0.7137\n",
      "    Batch 210 / 221: loss 0.6002\n",
      "    ROC-AUC score: 0.8275\n",
      "    Batch 213 / 221: loss 0.6198\n",
      "    ROC-AUC score: 0.8611\n",
      "    Batch 216 / 221: loss 0.5515\n",
      "    ROC-AUC score: 0.8704\n",
      "    Batch 219 / 221: loss 0.5319\n",
      "    ROC-AUC score: 0.8477\n",
      "Finished training.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    use_visdom = config['visdom']\n",
    "    if use_visdom:\n",
    "        vis = visdom.Visdom()\n",
    "        loss_window = None\n",
    "    criterion = utils.get_criterion(config['task'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                           weight_decay=config['weight_decay'])\n",
    "    epochs = config['epochs']\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.8)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[300], gamma=0.7)\n",
    "    model.train()\n",
    "    print('--------------------------------')\n",
    "    print('Training.')\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "        running_loss = 0.0\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            loss = criterion(scores, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "            if (idx + 1) % stats_per_batch == 0:\n",
    "                running_loss /= stats_per_batch\n",
    "                print('    Batch {} / {}: loss {:.4f}'.format(\n",
    "                    idx+1, num_batches, running_loss))\n",
    "                if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "                    area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "                    print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "                running_loss = 0.0\n",
    "                num_correct, num_examples = 0, 0\n",
    "            if use_visdom:\n",
    "                if loss_window is None:\n",
    "                    loss_window = vis.line(\n",
    "                        Y=[loss.item()],\n",
    "                        X=[epoch*num_batches+idx],\n",
    "                        opts=dict(xlabel='batch', ylabel='Loss', title='Training Loss', legend=['Loss']))\n",
    "                else:\n",
    "                    vis.line(\n",
    "                        [loss.item()],\n",
    "                        [epoch*num_batches+idx],\n",
    "                        win=loss_window,\n",
    "                        update='append')\n",
    "            scheduler.step()\n",
    "    if use_visdom:\n",
    "        vis.close(win=loss_window)\n",
    "    print('Finished training.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['load']:\n",
    "    if config['save']:\n",
    "        print('--------------------------------')\n",
    "        directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                'trained_models')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        fname = utils.get_fname(config)\n",
    "        path = os.path.join(directory, fname)\n",
    "        print('Saving model at {}'.format(path))\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print('Finished saving model.')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset after training.\n",
      "    Batch 1 / 221\n",
      "    Batch 2 / 221\n",
      "    Batch 3 / 221\n",
      "    Batch 4 / 221\n",
      "    Batch 5 / 221\n",
      "    Batch 6 / 221\n",
      "    Batch 7 / 221\n",
      "    Batch 8 / 221\n",
      "    Batch 9 / 221\n",
      "    Batch 10 / 221\n",
      "    Batch 11 / 221\n",
      "    Batch 12 / 221\n",
      "    Batch 13 / 221\n",
      "    Batch 14 / 221\n",
      "    Batch 15 / 221\n",
      "    Batch 16 / 221\n",
      "    Batch 17 / 221\n",
      "    Batch 18 / 221\n",
      "    Batch 19 / 221\n",
      "    Batch 20 / 221\n",
      "    Batch 21 / 221\n",
      "    Batch 22 / 221\n",
      "    Batch 23 / 221\n",
      "    Batch 24 / 221\n",
      "    Batch 25 / 221\n",
      "    Batch 26 / 221\n",
      "    Batch 27 / 221\n",
      "    Batch 28 / 221\n",
      "    Batch 29 / 221\n",
      "    Batch 30 / 221\n",
      "    Batch 31 / 221\n",
      "    Batch 32 / 221\n",
      "    Batch 33 / 221\n",
      "    Batch 34 / 221\n",
      "    Batch 35 / 221\n",
      "    Batch 36 / 221\n",
      "    Batch 37 / 221\n",
      "    Batch 38 / 221\n",
      "    Batch 39 / 221\n",
      "    Batch 40 / 221\n",
      "    Batch 41 / 221\n",
      "    Batch 42 / 221\n",
      "    Batch 43 / 221\n",
      "    Batch 44 / 221\n",
      "    Batch 45 / 221\n",
      "    Batch 46 / 221\n",
      "    Batch 47 / 221\n",
      "    Batch 48 / 221\n",
      "    Batch 49 / 221\n",
      "    Batch 50 / 221\n",
      "    Batch 51 / 221\n",
      "    Batch 52 / 221\n",
      "    Batch 53 / 221\n",
      "    Batch 54 / 221\n",
      "    Batch 55 / 221\n",
      "    Batch 56 / 221\n",
      "    Batch 57 / 221\n",
      "    Batch 58 / 221\n",
      "    Batch 59 / 221\n",
      "    Batch 60 / 221\n",
      "    Batch 61 / 221\n",
      "    Batch 62 / 221\n",
      "    Batch 63 / 221\n",
      "    Batch 64 / 221\n",
      "    Batch 65 / 221\n",
      "    Batch 66 / 221\n",
      "    Batch 67 / 221\n",
      "    Batch 68 / 221\n",
      "    Batch 69 / 221\n",
      "    Batch 70 / 221\n",
      "    Batch 71 / 221\n",
      "    Batch 72 / 221\n",
      "    Batch 73 / 221\n",
      "    Batch 74 / 221\n",
      "    Batch 75 / 221\n",
      "    Batch 76 / 221\n",
      "    Batch 77 / 221\n",
      "    Batch 78 / 221\n",
      "    Batch 79 / 221\n",
      "    Batch 80 / 221\n",
      "    Batch 81 / 221\n",
      "    Batch 82 / 221\n",
      "    Batch 83 / 221\n",
      "    Batch 84 / 221\n",
      "    Batch 85 / 221\n",
      "    Batch 86 / 221\n",
      "    Batch 87 / 221\n",
      "    Batch 88 / 221\n",
      "    Batch 89 / 221\n",
      "    Batch 90 / 221\n",
      "    Batch 91 / 221\n",
      "    Batch 92 / 221\n",
      "    Batch 93 / 221\n",
      "    Batch 94 / 221\n",
      "    Batch 95 / 221\n",
      "    Batch 96 / 221\n",
      "    Batch 97 / 221\n",
      "    Batch 98 / 221\n",
      "    Batch 99 / 221\n",
      "    Batch 100 / 221\n",
      "    Batch 101 / 221\n",
      "    Batch 102 / 221\n",
      "    Batch 103 / 221\n",
      "    Batch 104 / 221\n",
      "    Batch 105 / 221\n",
      "    Batch 106 / 221\n",
      "    Batch 107 / 221\n",
      "    Batch 108 / 221\n",
      "    Batch 109 / 221\n",
      "    Batch 110 / 221\n",
      "    Batch 111 / 221\n",
      "    Batch 112 / 221\n",
      "    Batch 113 / 221\n",
      "    Batch 114 / 221\n",
      "    Batch 115 / 221\n",
      "    Batch 116 / 221\n",
      "    Batch 117 / 221\n",
      "    Batch 118 / 221\n",
      "    Batch 119 / 221\n",
      "    Batch 120 / 221\n",
      "    Batch 121 / 221\n",
      "    Batch 122 / 221\n",
      "    Batch 123 / 221\n",
      "    Batch 124 / 221\n",
      "    Batch 125 / 221\n",
      "    Batch 126 / 221\n",
      "    Batch 127 / 221\n",
      "    Batch 128 / 221\n",
      "    Batch 129 / 221\n",
      "    Batch 130 / 221\n",
      "    Batch 131 / 221\n",
      "    Batch 132 / 221\n",
      "    Batch 133 / 221\n",
      "    Batch 134 / 221\n",
      "    Batch 135 / 221\n",
      "    Batch 136 / 221\n",
      "    Batch 137 / 221\n",
      "    Batch 138 / 221\n",
      "    Batch 139 / 221\n",
      "    Batch 140 / 221\n",
      "    Batch 141 / 221\n",
      "    Batch 142 / 221\n",
      "    Batch 143 / 221\n",
      "    Batch 144 / 221\n",
      "    Batch 145 / 221\n",
      "    Batch 146 / 221\n",
      "    Batch 147 / 221\n",
      "    Batch 148 / 221\n",
      "    Batch 149 / 221\n",
      "    Batch 150 / 221\n",
      "    Batch 151 / 221\n",
      "    Batch 152 / 221\n",
      "    Batch 153 / 221\n",
      "    Batch 154 / 221\n",
      "    Batch 155 / 221\n",
      "    Batch 156 / 221\n",
      "    Batch 157 / 221\n",
      "    Batch 158 / 221\n",
      "    Batch 159 / 221\n",
      "    Batch 160 / 221\n",
      "    Batch 161 / 221\n",
      "    Batch 162 / 221\n",
      "    Batch 163 / 221\n",
      "    Batch 164 / 221\n",
      "    Batch 165 / 221\n",
      "    Batch 166 / 221\n",
      "    Batch 167 / 221\n",
      "    Batch 168 / 221\n",
      "    Batch 169 / 221\n",
      "    Batch 170 / 221\n",
      "    Batch 171 / 221\n",
      "    Batch 172 / 221\n",
      "    Batch 173 / 221\n",
      "    Batch 174 / 221\n",
      "    Batch 175 / 221\n",
      "    Batch 176 / 221\n",
      "    Batch 177 / 221\n",
      "    Batch 178 / 221\n",
      "    Batch 179 / 221\n",
      "    Batch 180 / 221\n",
      "    Batch 181 / 221\n",
      "    Batch 182 / 221\n",
      "    Batch 183 / 221\n",
      "    Batch 184 / 221\n",
      "    Batch 185 / 221\n",
      "    Batch 186 / 221\n",
      "    Batch 187 / 221\n",
      "    Batch 188 / 221\n",
      "    Batch 189 / 221\n",
      "    Batch 190 / 221\n",
      "    Batch 191 / 221\n",
      "    Batch 192 / 221\n",
      "    Batch 193 / 221\n",
      "    Batch 194 / 221\n",
      "    Batch 195 / 221\n",
      "    Batch 196 / 221\n",
      "    Batch 197 / 221\n",
      "    Batch 198 / 221\n",
      "    Batch 199 / 221\n",
      "    Batch 200 / 221\n",
      "    Batch 201 / 221\n",
      "    Batch 202 / 221\n",
      "    Batch 203 / 221\n",
      "    Batch 204 / 221\n",
      "    Batch 205 / 221\n",
      "    Batch 206 / 221\n",
      "    Batch 207 / 221\n",
      "    Batch 208 / 221\n",
      "    Batch 209 / 221\n",
      "    Batch 210 / 221\n",
      "    Batch 211 / 221\n",
      "    Batch 212 / 221\n",
      "    Batch 213 / 221\n",
      "    Batch 214 / 221\n",
      "    Batch 215 / 221\n",
      "    Batch 216 / 221\n",
      "    Batch 217 / 221\n",
      "    Batch 218 / 221\n",
      "    Batch 219 / 221\n",
      "    Batch 220 / 221\n",
      "    Batch 221 / 221\n",
      "ROC-AUC score: 0.8196\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset after training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the true positive rate and true negative rate vs threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    tpr, fpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    tnr = 1 - fpr\n",
    "    plt.plot(thresholds, tpr, label='tpr')\n",
    "    plt.plot(thresholds, tnr, label='tnr')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.title('TPR / TNR vs Threshold')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an appropriate threshold and generate classification report on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 1 / 221\n",
      "    Batch 2 / 221\n",
      "    Batch 3 / 221\n",
      "    Batch 4 / 221\n",
      "    Batch 5 / 221\n",
      "    Batch 6 / 221\n",
      "    Batch 7 / 221\n",
      "    Batch 8 / 221\n",
      "    Batch 9 / 221\n",
      "    Batch 10 / 221\n",
      "    Batch 11 / 221\n",
      "    Batch 12 / 221\n",
      "    Batch 13 / 221\n",
      "    Batch 14 / 221\n",
      "    Batch 15 / 221\n",
      "    Batch 16 / 221\n",
      "    Batch 17 / 221\n",
      "    Batch 18 / 221\n",
      "    Batch 19 / 221\n",
      "    Batch 20 / 221\n",
      "    Batch 21 / 221\n",
      "    Batch 22 / 221\n",
      "    Batch 23 / 221\n",
      "    Batch 24 / 221\n",
      "    Batch 25 / 221\n",
      "    Batch 26 / 221\n",
      "    Batch 27 / 221\n",
      "    Batch 28 / 221\n",
      "    Batch 29 / 221\n",
      "    Batch 30 / 221\n",
      "    Batch 31 / 221\n",
      "    Batch 32 / 221\n",
      "    Batch 33 / 221\n",
      "    Batch 34 / 221\n",
      "    Batch 35 / 221\n",
      "    Batch 36 / 221\n",
      "    Batch 37 / 221\n",
      "    Batch 38 / 221\n",
      "    Batch 39 / 221\n",
      "    Batch 40 / 221\n",
      "    Batch 41 / 221\n",
      "    Batch 42 / 221\n",
      "    Batch 43 / 221\n",
      "    Batch 44 / 221\n",
      "    Batch 45 / 221\n",
      "    Batch 46 / 221\n",
      "    Batch 47 / 221\n",
      "    Batch 48 / 221\n",
      "    Batch 49 / 221\n",
      "    Batch 50 / 221\n",
      "    Batch 51 / 221\n",
      "    Batch 52 / 221\n",
      "    Batch 53 / 221\n",
      "    Batch 54 / 221\n",
      "    Batch 55 / 221\n",
      "    Batch 56 / 221\n",
      "    Batch 57 / 221\n",
      "    Batch 58 / 221\n",
      "    Batch 59 / 221\n",
      "    Batch 60 / 221\n",
      "    Batch 61 / 221\n",
      "    Batch 62 / 221\n",
      "    Batch 63 / 221\n",
      "    Batch 64 / 221\n",
      "    Batch 65 / 221\n",
      "    Batch 66 / 221\n",
      "    Batch 67 / 221\n",
      "    Batch 68 / 221\n",
      "    Batch 69 / 221\n",
      "    Batch 70 / 221\n",
      "    Batch 71 / 221\n",
      "    Batch 72 / 221\n",
      "    Batch 73 / 221\n",
      "    Batch 74 / 221\n",
      "    Batch 75 / 221\n",
      "    Batch 76 / 221\n",
      "    Batch 77 / 221\n",
      "    Batch 78 / 221\n",
      "    Batch 79 / 221\n",
      "    Batch 80 / 221\n",
      "    Batch 81 / 221\n",
      "    Batch 82 / 221\n",
      "    Batch 83 / 221\n",
      "    Batch 84 / 221\n",
      "    Batch 85 / 221\n",
      "    Batch 86 / 221\n",
      "    Batch 87 / 221\n",
      "    Batch 88 / 221\n",
      "    Batch 89 / 221\n",
      "    Batch 90 / 221\n",
      "    Batch 91 / 221\n",
      "    Batch 92 / 221\n",
      "    Batch 93 / 221\n",
      "    Batch 94 / 221\n",
      "    Batch 95 / 221\n",
      "    Batch 96 / 221\n",
      "    Batch 97 / 221\n",
      "    Batch 98 / 221\n",
      "    Batch 99 / 221\n",
      "    Batch 100 / 221\n",
      "    Batch 101 / 221\n",
      "    Batch 102 / 221\n",
      "    Batch 103 / 221\n",
      "    Batch 104 / 221\n",
      "    Batch 105 / 221\n",
      "    Batch 106 / 221\n",
      "    Batch 107 / 221\n",
      "    Batch 108 / 221\n",
      "    Batch 109 / 221\n",
      "    Batch 110 / 221\n",
      "    Batch 111 / 221\n",
      "    Batch 112 / 221\n",
      "    Batch 113 / 221\n",
      "    Batch 114 / 221\n",
      "    Batch 115 / 221\n",
      "    Batch 116 / 221\n",
      "    Batch 117 / 221\n",
      "    Batch 118 / 221\n",
      "    Batch 119 / 221\n",
      "    Batch 120 / 221\n",
      "    Batch 121 / 221\n",
      "    Batch 122 / 221\n",
      "    Batch 123 / 221\n",
      "    Batch 124 / 221\n",
      "    Batch 125 / 221\n",
      "    Batch 126 / 221\n",
      "    Batch 127 / 221\n",
      "    Batch 128 / 221\n",
      "    Batch 129 / 221\n",
      "    Batch 130 / 221\n",
      "    Batch 131 / 221\n",
      "    Batch 132 / 221\n",
      "    Batch 133 / 221\n",
      "    Batch 134 / 221\n",
      "    Batch 135 / 221\n",
      "    Batch 136 / 221\n",
      "    Batch 137 / 221\n",
      "    Batch 138 / 221\n",
      "    Batch 139 / 221\n",
      "    Batch 140 / 221\n",
      "    Batch 141 / 221\n",
      "    Batch 142 / 221\n",
      "    Batch 143 / 221\n",
      "    Batch 144 / 221\n",
      "    Batch 145 / 221\n",
      "    Batch 146 / 221\n",
      "    Batch 147 / 221\n",
      "    Batch 148 / 221\n",
      "    Batch 149 / 221\n",
      "    Batch 150 / 221\n",
      "    Batch 151 / 221\n",
      "    Batch 152 / 221\n",
      "    Batch 153 / 221\n",
      "    Batch 154 / 221\n",
      "    Batch 155 / 221\n",
      "    Batch 156 / 221\n",
      "    Batch 157 / 221\n",
      "    Batch 158 / 221\n",
      "    Batch 159 / 221\n",
      "    Batch 160 / 221\n",
      "    Batch 161 / 221\n",
      "    Batch 162 / 221\n",
      "    Batch 163 / 221\n",
      "    Batch 164 / 221\n",
      "    Batch 165 / 221\n",
      "    Batch 166 / 221\n",
      "    Batch 167 / 221\n",
      "    Batch 168 / 221\n",
      "    Batch 169 / 221\n",
      "    Batch 170 / 221\n",
      "    Batch 171 / 221\n",
      "    Batch 172 / 221\n",
      "    Batch 173 / 221\n",
      "    Batch 174 / 221\n",
      "    Batch 175 / 221\n",
      "    Batch 176 / 221\n",
      "    Batch 177 / 221\n",
      "    Batch 178 / 221\n",
      "    Batch 179 / 221\n",
      "    Batch 180 / 221\n",
      "    Batch 181 / 221\n",
      "    Batch 182 / 221\n",
      "    Batch 183 / 221\n",
      "    Batch 184 / 221\n",
      "    Batch 185 / 221\n",
      "    Batch 186 / 221\n",
      "    Batch 187 / 221\n",
      "    Batch 188 / 221\n",
      "    Batch 189 / 221\n",
      "    Batch 190 / 221\n",
      "    Batch 191 / 221\n",
      "    Batch 192 / 221\n",
      "    Batch 193 / 221\n",
      "    Batch 194 / 221\n",
      "    Batch 195 / 221\n",
      "    Batch 196 / 221\n",
      "    Batch 197 / 221\n",
      "    Batch 198 / 221\n",
      "    Batch 199 / 221\n",
      "    Batch 200 / 221\n",
      "    Batch 201 / 221\n",
      "    Batch 202 / 221\n",
      "    Batch 203 / 221\n",
      "    Batch 204 / 221\n",
      "    Batch 205 / 221\n",
      "    Batch 206 / 221\n",
      "    Batch 207 / 221\n",
      "    Batch 208 / 221\n",
      "    Batch 209 / 221\n",
      "    Batch 210 / 221\n",
      "    Batch 211 / 221\n",
      "    Batch 212 / 221\n",
      "    Batch 213 / 221\n",
      "    Batch 214 / 221\n",
      "    Batch 215 / 221\n",
      "    Batch 216 / 221\n",
      "    Batch 217 / 221\n",
      "    Batch 218 / 221\n",
      "    Batch 219 / 221\n",
      "    Batch 220 / 221\n",
      "    Batch 221 / 221\n",
      "Threshold: 0.3769, accuracy: 0.7438\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.74      0.74      3522\n",
      "         1.0       0.74      0.74      0.74      3522\n",
      "\n",
      "    accuracy                           0.74      7044\n",
      "   macro avg       0.74      0.74      0.74      7044\n",
      "weighted avg       0.74      0.74      0.74      7044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx1 = np.where(tpr <= tnr)[0]\n",
    "idx2 = np.where(tpr >= tnr)[0]\n",
    "t = thresholds[idx1[-1]]\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "with torch.no_grad():\n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        edges, features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        all_pairs = torch.mm(out, out.t())\n",
    "        scores = all_pairs[edges.T]\n",
    "        predictions = (scores >= t).long()\n",
    "        y_true.extend(labels.detach().numpy())\n",
    "        y_pred.extend(predictions.detach().numpy())\n",
    "        total_correct += torch.sum(predictions == labels.long()).item()\n",
    "        total_examples += len(labels) \n",
    "        print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "print('Threshold: {:.4f}, accuracy: {:.4f}'.format(t, total_correct / total_examples))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "print('Classification report\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: test\n",
      "Number of vertices: 113\n",
      "Number of static edges: 2096\n",
      "Number of temporal edges: 15613\n",
      "Number of examples/datapoints: 8278\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the test dataset after training.\n",
      "    Batch 3 / 259: loss 0.7254, accuracy 0.5208\n",
      "    ROC-AUC score: 0.6980\n",
      "    Batch 6 / 259: loss 0.6096, accuracy 0.7083\n",
      "    ROC-AUC score: 0.6518\n",
      "    Batch 9 / 259: loss 0.6656, accuracy 0.6458\n",
      "    ROC-AUC score: 0.8381\n",
      "    Batch 12 / 259: loss 0.6595, accuracy 0.5833\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 15 / 259: loss 0.6200, accuracy 0.6354\n",
      "    ROC-AUC score: 0.5628\n",
      "    Batch 18 / 259: loss 0.7174, accuracy 0.5729\n",
      "    ROC-AUC score: 0.6468\n",
      "    Batch 21 / 259: loss 0.6948, accuracy 0.6042\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 24 / 259: loss 0.6787, accuracy 0.5521\n",
      "    ROC-AUC score: 0.6397\n",
      "    Batch 27 / 259: loss 0.6883, accuracy 0.5312\n",
      "    ROC-AUC score: 0.5556\n",
      "    Batch 30 / 259: loss 0.7333, accuracy 0.5625\n",
      "    ROC-AUC score: 0.6270\n",
      "    Batch 33 / 259: loss 0.7670, accuracy 0.5417\n",
      "    ROC-AUC score: 0.5208\n",
      "    Batch 36 / 259: loss 0.7712, accuracy 0.5625\n",
      "    ROC-AUC score: 0.5238\n",
      "    Batch 39 / 259: loss 0.6859, accuracy 0.5625\n",
      "    ROC-AUC score: 0.6275\n",
      "    Batch 42 / 259: loss 0.7298, accuracy 0.5521\n",
      "    ROC-AUC score: 0.6583\n",
      "    Batch 45 / 259: loss 0.6682, accuracy 0.5833\n",
      "    ROC-AUC score: 0.7255\n",
      "    Batch 48 / 259: loss 0.7728, accuracy 0.5521\n",
      "    ROC-AUC score: 0.5045\n",
      "    Batch 51 / 259: loss 0.6853, accuracy 0.4688\n",
      "    ROC-AUC score: 0.8698\n",
      "    Batch 54 / 259: loss 0.6605, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7302\n",
      "    Batch 57 / 259: loss 0.6408, accuracy 0.5833\n",
      "    ROC-AUC score: 0.8664\n",
      "    Batch 60 / 259: loss 0.6659, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7778\n",
      "    Batch 63 / 259: loss 0.7365, accuracy 0.5208\n",
      "    ROC-AUC score: 0.5304\n",
      "    Batch 66 / 259: loss 0.7067, accuracy 0.5833\n",
      "    ROC-AUC score: 0.7659\n",
      "    Batch 69 / 259: loss 0.6270, accuracy 0.5938\n",
      "    ROC-AUC score: 0.5500\n",
      "    Batch 72 / 259: loss 0.6266, accuracy 0.6667\n",
      "    ROC-AUC score: 0.8294\n",
      "    Batch 75 / 259: loss 0.6982, accuracy 0.5521\n",
      "    ROC-AUC score: 0.5556\n",
      "    Batch 78 / 259: loss 0.7209, accuracy 0.6042\n",
      "    ROC-AUC score: 0.7083\n",
      "    Batch 81 / 259: loss 0.6691, accuracy 0.6042\n",
      "    ROC-AUC score: 0.5273\n",
      "    Batch 84 / 259: loss 0.6471, accuracy 0.6250\n",
      "    ROC-AUC score: 0.5922\n",
      "    Batch 87 / 259: loss 0.6605, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6992\n",
      "    Batch 90 / 259: loss 0.6725, accuracy 0.5521\n",
      "    ROC-AUC score: 0.6964\n",
      "    Batch 93 / 259: loss 0.6624, accuracy 0.5104\n",
      "    ROC-AUC score: 0.6588\n",
      "    Batch 96 / 259: loss 0.6711, accuracy 0.6771\n",
      "    ROC-AUC score: 0.5397\n",
      "    Batch 99 / 259: loss 0.6354, accuracy 0.6562\n",
      "    ROC-AUC score: 0.6792\n",
      "    Batch 102 / 259: loss 0.7091, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7266\n",
      "    Batch 105 / 259: loss 0.6951, accuracy 0.6146\n",
      "    ROC-AUC score: 0.6980\n",
      "    Batch 108 / 259: loss 0.6685, accuracy 0.6042\n",
      "    ROC-AUC score: 0.7489\n",
      "    Batch 111 / 259: loss 0.7285, accuracy 0.5729\n",
      "    ROC-AUC score: 0.5476\n",
      "    Batch 114 / 259: loss 0.6504, accuracy 0.6354\n",
      "    ROC-AUC score: 0.6984\n",
      "    Batch 117 / 259: loss 0.6948, accuracy 0.5938\n",
      "    ROC-AUC score: 0.6964\n",
      "    Batch 120 / 259: loss 0.6353, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6537\n",
      "    Batch 123 / 259: loss 0.6161, accuracy 0.6146\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 126 / 259: loss 0.6635, accuracy 0.6146\n",
      "    ROC-AUC score: 0.7833\n",
      "    Batch 129 / 259: loss 0.6406, accuracy 0.6875\n",
      "    ROC-AUC score: 0.6627\n",
      "    Batch 132 / 259: loss 0.7265, accuracy 0.4792\n",
      "    ROC-AUC score: 0.5304\n",
      "    Batch 135 / 259: loss 0.6666, accuracy 0.5417\n",
      "    ROC-AUC score: 0.4549\n",
      "    Batch 138 / 259: loss 0.7520, accuracy 0.5000\n",
      "    ROC-AUC score: 0.4500\n",
      "    Batch 141 / 259: loss 0.6665, accuracy 0.6042\n",
      "    ROC-AUC score: 0.5703\n",
      "    Batch 144 / 259: loss 0.6739, accuracy 0.6146\n",
      "    ROC-AUC score: 0.6397\n",
      "    Batch 147 / 259: loss 0.6675, accuracy 0.5521\n",
      "    ROC-AUC score: 0.7578\n",
      "    Batch 150 / 259: loss 0.6550, accuracy 0.6458\n",
      "    ROC-AUC score: 0.7451\n",
      "    Batch 153 / 259: loss 0.6804, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7706\n",
      "    Batch 156 / 259: loss 0.6782, accuracy 0.5625\n",
      "    ROC-AUC score: 0.4667\n",
      "    Batch 159 / 259: loss 0.5949, accuracy 0.6042\n",
      "    ROC-AUC score: 0.8300\n",
      "    Batch 162 / 259: loss 0.7084, accuracy 0.5417\n",
      "    ROC-AUC score: 0.5490\n",
      "    Batch 165 / 259: loss 0.6982, accuracy 0.5938\n",
      "    ROC-AUC score: 0.6640\n",
      "    Batch 168 / 259: loss 0.6751, accuracy 0.6562\n",
      "    ROC-AUC score: 0.8545\n",
      "    Batch 171 / 259: loss 0.6419, accuracy 0.5729\n",
      "    ROC-AUC score: 0.7013\n",
      "    Batch 174 / 259: loss 0.6855, accuracy 0.5625\n",
      "    ROC-AUC score: 0.7020\n",
      "    Batch 177 / 259: loss 0.7013, accuracy 0.5625\n",
      "    ROC-AUC score: 0.5368\n",
      "    Batch 180 / 259: loss 0.6163, accuracy 0.5729\n",
      "    ROC-AUC score: 0.7421\n",
      "    Batch 183 / 259: loss 0.6255, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6941\n",
      "    Batch 186 / 259: loss 0.6567, accuracy 0.6042\n",
      "    ROC-AUC score: 0.5490\n",
      "    Batch 189 / 259: loss 0.6584, accuracy 0.6667\n",
      "    ROC-AUC score: 0.6842\n",
      "    Batch 192 / 259: loss 0.6426, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 195 / 259: loss 0.5907, accuracy 0.6354\n",
      "    ROC-AUC score: 0.7294\n",
      "    Batch 198 / 259: loss 0.6133, accuracy 0.5833\n",
      "    ROC-AUC score: 0.6083\n",
      "    Batch 201 / 259: loss 0.6505, accuracy 0.5729\n",
      "    ROC-AUC score: 0.6641\n",
      "    Batch 204 / 259: loss 0.6167, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7692\n",
      "    Batch 207 / 259: loss 0.6055, accuracy 0.6042\n",
      "    ROC-AUC score: 0.7909\n",
      "    Batch 210 / 259: loss 0.6523, accuracy 0.6146\n",
      "    ROC-AUC score: 0.7166\n",
      "    Batch 213 / 259: loss 0.6559, accuracy 0.6354\n",
      "    ROC-AUC score: 0.5873\n",
      "    Batch 216 / 259: loss 0.7024, accuracy 0.5729\n",
      "    ROC-AUC score: 0.5101\n",
      "    Batch 219 / 259: loss 0.6687, accuracy 0.6250\n",
      "    ROC-AUC score: 0.6992\n",
      "    Batch 222 / 259: loss 0.6337, accuracy 0.5938\n",
      "    ROC-AUC score: 0.8042\n",
      "    Batch 225 / 259: loss 0.6394, accuracy 0.6562\n",
      "    ROC-AUC score: 0.8078\n",
      "    Batch 228 / 259: loss 0.6936, accuracy 0.5729\n",
      "    ROC-AUC score: 0.6367\n",
      "    Batch 231 / 259: loss 0.7142, accuracy 0.5521\n",
      "    ROC-AUC score: 0.4745\n",
      "    Batch 234 / 259: loss 0.6685, accuracy 0.5208\n",
      "    ROC-AUC score: 0.5789\n",
      "    Batch 237 / 259: loss 0.6533, accuracy 0.6042\n",
      "    ROC-AUC score: 0.5176\n",
      "    Batch 240 / 259: loss 0.6645, accuracy 0.5729\n",
      "    ROC-AUC score: 0.7882\n",
      "    Batch 243 / 259: loss 0.7366, accuracy 0.5417\n",
      "    ROC-AUC score: 0.5312\n",
      "    Batch 246 / 259: loss 0.6771, accuracy 0.5833\n",
      "    ROC-AUC score: 0.5725\n",
      "    Batch 249 / 259: loss 0.6607, accuracy 0.5938\n",
      "    ROC-AUC score: 0.8510\n",
      "    Batch 252 / 259: loss 0.7322, accuracy 0.5729\n",
      "    ROC-AUC score: 0.6113\n",
      "    Batch 255 / 259: loss 0.6910, accuracy 0.5729\n",
      "    ROC-AUC score: 0.4706\n",
      "    Batch 258 / 259: loss 0.7209, accuracy 0.5417\n",
      "    ROC-AUC score: 0.4542\n",
      "Loss 0.6741, accuracy 0.5907\n",
      "ROC-AUC score: 0.6592\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.73      0.64      4139\n",
      "         1.0       0.62      0.45      0.53      4139\n",
      "\n",
      "    accuracy                           0.59      8278\n",
      "   macro avg       0.60      0.59      0.58      8278\n",
      "weighted avg       0.60      0.59      0.58      8278\n",
      "\n",
      "Finished testing.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'test',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the test dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished testing.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
