{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import visdom\n",
    "\n",
    "from datasets import link_prediction\n",
    "from layers import MeanAggregator, LSTMAggregator, MaxPoolAggregator, MeanPoolAggregator\n",
    "import models\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up arguments for datasets, models and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"task\" : \"link_prediction\",\n",
    "    \n",
    "    \"dataset\" : \"IARadoslawEmail\",\n",
    "    \"dataset_path\" : \"/Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-radoslaw-email/ia-radoslaw-email.edges\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"generate_neg_examples\" : True,\n",
    "    \n",
    "    \"duplicate_examples\" : False,\n",
    "    \"repeat_examples\" : False,\n",
    "    \n",
    "    \"self_loop\" : False,\n",
    "    \"normalize_adj\" : False,\n",
    "    \n",
    "    \"cuda\" : \"True\",\n",
    "    \"model\" : \"GraphSAGE\",\n",
    "    \"agg_class\" : \"MaxPoolAggregator\",\n",
    "    \"hidden_dims\" : [64],\n",
    "    \"dropout\" : 0.5,\n",
    "    \"num_samples\" : -1,\n",
    "    \n",
    "    \"epochs\" : 3,\n",
    "    \"batch_size\" : 32,\n",
    "    \"lr\" : 1e-4,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"visdom\" : True,\n",
    "    \n",
    "    \"load\" : False,\n",
    "    \"save\" : False\n",
    "}\n",
    "config = args\n",
    "config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "\n",
    "if config['cuda'] and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "config['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset, dataloader and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-radoslaw-email/ia-radoslaw-email.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: train\n",
      "Number of vertices: 167\n",
      "Number of static edges: 3935\n",
      "Number of temporal edges: 24878\n",
      "Number of examples/datapoints: 4326\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'train',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "input_dim, output_dim = dataset.get_dims()\n",
    "config['input_dim'], config['output_dim'] = input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE(\n",
      "  (aggregators): ModuleList(\n",
      "    (0): MaxPoolAggregator(\n",
      "      (fc1): Linear(in_features=167, out_features=167, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): MaxPoolAggregator(\n",
      "      (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=334, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = utils.get_model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score for the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset before training.\n",
      "    Batch 1 / 136\n",
      "    Batch 2 / 136\n",
      "    Batch 3 / 136\n",
      "    Batch 4 / 136\n",
      "    Batch 5 / 136\n",
      "    Batch 6 / 136\n",
      "    Batch 7 / 136\n",
      "    Batch 8 / 136\n",
      "    Batch 9 / 136\n",
      "    Batch 10 / 136\n",
      "    Batch 11 / 136\n",
      "    Batch 12 / 136\n",
      "    Batch 13 / 136\n",
      "    Batch 14 / 136\n",
      "    Batch 15 / 136\n",
      "    Batch 16 / 136\n",
      "    Batch 17 / 136\n",
      "    Batch 18 / 136\n",
      "    Batch 19 / 136\n",
      "    Batch 20 / 136\n",
      "    Batch 21 / 136\n",
      "    Batch 22 / 136\n",
      "    Batch 23 / 136\n",
      "    Batch 24 / 136\n",
      "    Batch 25 / 136\n",
      "    Batch 26 / 136\n",
      "    Batch 27 / 136\n",
      "    Batch 28 / 136\n",
      "    Batch 29 / 136\n",
      "    Batch 30 / 136\n",
      "    Batch 31 / 136\n",
      "    Batch 32 / 136\n",
      "    Batch 33 / 136\n",
      "    Batch 34 / 136\n",
      "    Batch 35 / 136\n",
      "    Batch 36 / 136\n",
      "    Batch 37 / 136\n",
      "    Batch 38 / 136\n",
      "    Batch 39 / 136\n",
      "    Batch 40 / 136\n",
      "    Batch 41 / 136\n",
      "    Batch 42 / 136\n",
      "    Batch 43 / 136\n",
      "    Batch 44 / 136\n",
      "    Batch 45 / 136\n",
      "    Batch 46 / 136\n",
      "    Batch 47 / 136\n",
      "    Batch 48 / 136\n",
      "    Batch 49 / 136\n",
      "    Batch 50 / 136\n",
      "    Batch 51 / 136\n",
      "    Batch 52 / 136\n",
      "    Batch 53 / 136\n",
      "    Batch 54 / 136\n",
      "    Batch 55 / 136\n",
      "    Batch 56 / 136\n",
      "    Batch 57 / 136\n",
      "    Batch 58 / 136\n",
      "    Batch 59 / 136\n",
      "    Batch 60 / 136\n",
      "    Batch 61 / 136\n",
      "    Batch 62 / 136\n",
      "    Batch 63 / 136\n",
      "    Batch 64 / 136\n",
      "    Batch 65 / 136\n",
      "    Batch 66 / 136\n",
      "    Batch 67 / 136\n",
      "    Batch 68 / 136\n",
      "    Batch 69 / 136\n",
      "    Batch 70 / 136\n",
      "    Batch 71 / 136\n",
      "    Batch 72 / 136\n",
      "    Batch 73 / 136\n",
      "    Batch 74 / 136\n",
      "    Batch 75 / 136\n",
      "    Batch 76 / 136\n",
      "    Batch 77 / 136\n",
      "    Batch 78 / 136\n",
      "    Batch 79 / 136\n",
      "    Batch 80 / 136\n",
      "    Batch 81 / 136\n",
      "    Batch 82 / 136\n",
      "    Batch 83 / 136\n",
      "    Batch 84 / 136\n",
      "    Batch 85 / 136\n",
      "    Batch 86 / 136\n",
      "    Batch 87 / 136\n",
      "    Batch 88 / 136\n",
      "    Batch 89 / 136\n",
      "    Batch 90 / 136\n",
      "    Batch 91 / 136\n",
      "    Batch 92 / 136\n",
      "    Batch 93 / 136\n",
      "    Batch 94 / 136\n",
      "    Batch 95 / 136\n",
      "    Batch 96 / 136\n",
      "    Batch 97 / 136\n",
      "    Batch 98 / 136\n",
      "    Batch 99 / 136\n",
      "    Batch 100 / 136\n",
      "    Batch 101 / 136\n",
      "    Batch 102 / 136\n",
      "    Batch 103 / 136\n",
      "    Batch 104 / 136\n",
      "    Batch 105 / 136\n",
      "    Batch 106 / 136\n",
      "    Batch 107 / 136\n",
      "    Batch 108 / 136\n",
      "    Batch 109 / 136\n",
      "    Batch 110 / 136\n",
      "    Batch 111 / 136\n",
      "    Batch 112 / 136\n",
      "    Batch 113 / 136\n",
      "    Batch 114 / 136\n",
      "    Batch 115 / 136\n",
      "    Batch 116 / 136\n",
      "    Batch 117 / 136\n",
      "    Batch 118 / 136\n",
      "    Batch 119 / 136\n",
      "    Batch 120 / 136\n",
      "    Batch 121 / 136\n",
      "    Batch 122 / 136\n",
      "    Batch 123 / 136\n",
      "    Batch 124 / 136\n",
      "    Batch 125 / 136\n",
      "    Batch 126 / 136\n",
      "    Batch 127 / 136\n",
      "    Batch 128 / 136\n",
      "    Batch 129 / 136\n",
      "    Batch 130 / 136\n",
      "    Batch 131 / 136\n",
      "    Batch 132 / 136\n",
      "    Batch 133 / 136\n",
      "    Batch 134 / 136\n",
      "    Batch 135 / 136\n",
      "    Batch 136 / 136\n",
      "ROC-AUC score: 0.5827\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset before training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Training.\n",
      "Epoch 1 / 3\n",
      "    Batch 3 / 136: loss 0.6914\n",
      "    ROC-AUC score: 0.6314\n",
      "    Batch 6 / 136: loss 0.6943\n",
      "    ROC-AUC score: 0.6273\n",
      "    Batch 9 / 136: loss 0.6919\n",
      "    ROC-AUC score: 0.8056\n",
      "    Batch 12 / 136: loss 0.6930\n",
      "    ROC-AUC score: 0.6926\n",
      "    Batch 15 / 136: loss 0.6910\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 18 / 136: loss 0.6909\n",
      "    ROC-AUC score: 0.6825\n",
      "    Batch 21 / 136: loss 0.6892\n",
      "    ROC-AUC score: 0.7451\n",
      "    Batch 24 / 136: loss 0.6898\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 27 / 136: loss 0.6876\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 30 / 136: loss 0.6874\n",
      "    ROC-AUC score: 0.8259\n",
      "    Batch 33 / 136: loss 0.6874\n",
      "    ROC-AUC score: 0.7273\n",
      "    Batch 36 / 136: loss 0.6867\n",
      "    ROC-AUC score: 0.8833\n",
      "    Batch 39 / 136: loss 0.6845\n",
      "    ROC-AUC score: 0.9414\n",
      "    Batch 42 / 136: loss 0.6845\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 45 / 136: loss 0.6803\n",
      "    ROC-AUC score: 0.6833\n",
      "    Batch 48 / 136: loss 0.6814\n",
      "    ROC-AUC score: 0.8281\n",
      "    Batch 51 / 136: loss 0.6797\n",
      "    ROC-AUC score: 0.9647\n",
      "    Batch 54 / 136: loss 0.6809\n",
      "    ROC-AUC score: 0.8984\n",
      "    Batch 57 / 136: loss 0.6748\n",
      "    ROC-AUC score: 0.8242\n",
      "    Batch 60 / 136: loss 0.6753\n",
      "    ROC-AUC score: 0.8867\n",
      "    Batch 63 / 136: loss 0.6809\n",
      "    ROC-AUC score: 0.8312\n",
      "    Batch 66 / 136: loss 0.6804\n",
      "    ROC-AUC score: 0.7579\n",
      "    Batch 69 / 136: loss 0.6818\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 72 / 136: loss 0.6694\n",
      "    ROC-AUC score: 0.9059\n",
      "    Batch 75 / 136: loss 0.6723\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 78 / 136: loss 0.6669\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 81 / 136: loss 0.6650\n",
      "    ROC-AUC score: 0.9412\n",
      "    Batch 84 / 136: loss 0.6724\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 87 / 136: loss 0.6824\n",
      "    ROC-AUC score: 0.7897\n",
      "    Batch 90 / 136: loss 0.6582\n",
      "    ROC-AUC score: 0.9048\n",
      "    Batch 93 / 136: loss 0.6735\n",
      "    ROC-AUC score: 0.7738\n",
      "    Batch 96 / 136: loss 0.6526\n",
      "    ROC-AUC score: 0.8214\n",
      "    Batch 99 / 136: loss 0.6668\n",
      "    ROC-AUC score: 0.8452\n",
      "    Batch 102 / 136: loss 0.6577\n",
      "    ROC-AUC score: 0.5411\n",
      "    Batch 105 / 136: loss 0.6562\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 108 / 136: loss 0.6626\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 111 / 136: loss 0.6515\n",
      "    ROC-AUC score: 0.9297\n",
      "    Batch 114 / 136: loss 0.6516\n",
      "    ROC-AUC score: 0.8988\n",
      "    Batch 117 / 136: loss 0.6620\n",
      "    ROC-AUC score: 0.8745\n",
      "    Batch 120 / 136: loss 0.6630\n",
      "    ROC-AUC score: 0.7188\n",
      "    Batch 123 / 136: loss 0.6569\n",
      "    ROC-AUC score: 0.8355\n",
      "    Batch 126 / 136: loss 0.6491\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 129 / 136: loss 0.6388\n",
      "    ROC-AUC score: 0.8828\n",
      "    Batch 132 / 136: loss 0.6349\n",
      "    ROC-AUC score: 0.9325\n",
      "    Batch 135 / 136: loss 0.6600\n",
      "    ROC-AUC score: 0.9190\n",
      "Epoch 2 / 3\n",
      "    Batch 3 / 136: loss 0.6402\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 6 / 136: loss 0.6630\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 9 / 136: loss 0.6532\n",
      "    ROC-AUC score: 0.8409\n",
      "    Batch 12 / 136: loss 0.6430\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 15 / 136: loss 0.6315\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 18 / 136: loss 0.6592\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 21 / 136: loss 0.6581\n",
      "    ROC-AUC score: 0.7103\n",
      "    Batch 24 / 136: loss 0.6489\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 27 / 136: loss 0.6436\n",
      "    ROC-AUC score: 0.9150\n",
      "    Batch 30 / 136: loss 0.6410\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 33 / 136: loss 0.6380\n",
      "    ROC-AUC score: 0.7227\n",
      "    Batch 36 / 136: loss 0.6315\n",
      "    ROC-AUC score: 0.9405\n",
      "    Batch 39 / 136: loss 0.6498\n",
      "    ROC-AUC score: 0.8009\n",
      "    Batch 42 / 136: loss 0.6257\n",
      "    ROC-AUC score: 0.9206\n",
      "    Batch 45 / 136: loss 0.6231\n",
      "    ROC-AUC score: 0.9514\n",
      "    Batch 48 / 136: loss 0.6546\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 51 / 136: loss 0.6279\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 54 / 136: loss 0.6477\n",
      "    ROC-AUC score: 0.9451\n",
      "    Batch 57 / 136: loss 0.6403\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 60 / 136: loss 0.6498\n",
      "    ROC-AUC score: 0.7422\n",
      "    Batch 63 / 136: loss 0.6377\n",
      "    ROC-AUC score: 0.8902\n",
      "    Batch 66 / 136: loss 0.6339\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 69 / 136: loss 0.6369\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 72 / 136: loss 0.6213\n",
      "    ROC-AUC score: 0.8599\n",
      "    Batch 75 / 136: loss 0.6530\n",
      "    ROC-AUC score: 0.8398\n",
      "    Batch 78 / 136: loss 0.6539\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 81 / 136: loss 0.6393\n",
      "    ROC-AUC score: 0.8818\n",
      "    Batch 84 / 136: loss 0.6301\n",
      "    ROC-AUC score: 0.9177\n",
      "    Batch 87 / 136: loss 0.6474\n",
      "    ROC-AUC score: 0.9610\n",
      "    Batch 90 / 136: loss 0.5989\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 93 / 136: loss 0.6204\n",
      "    ROC-AUC score: 0.7930\n",
      "    Batch 96 / 136: loss 0.6134\n",
      "    ROC-AUC score: 0.9394\n",
      "    Batch 99 / 136: loss 0.6939\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 102 / 136: loss 0.5855\n",
      "    ROC-AUC score: 0.9312\n",
      "    Batch 105 / 136: loss 0.6109\n",
      "    ROC-AUC score: 0.8502\n",
      "    Batch 108 / 136: loss 0.6386\n",
      "    ROC-AUC score: 0.9325\n",
      "    Batch 111 / 136: loss 0.6153\n",
      "    ROC-AUC score: 0.9609\n",
      "    Batch 114 / 136: loss 0.6302\n",
      "    ROC-AUC score: 0.7922\n",
      "    Batch 117 / 136: loss 0.6100\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 120 / 136: loss 0.5978\n",
      "    ROC-AUC score: 0.9127\n",
      "    Batch 123 / 136: loss 0.6281\n",
      "    ROC-AUC score: 0.8462\n",
      "    Batch 126 / 136: loss 0.6139\n",
      "    ROC-AUC score: 0.9216\n",
      "    Batch 129 / 136: loss 0.5777\n",
      "    ROC-AUC score: 0.9643\n",
      "    Batch 132 / 136: loss 0.6285\n",
      "    ROC-AUC score: 0.8784\n",
      "    Batch 135 / 136: loss 0.6119\n",
      "    ROC-AUC score: 0.8706\n",
      "Epoch 3 / 3\n",
      "    Batch 3 / 136: loss 0.6077\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 6 / 136: loss 0.6233\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 9 / 136: loss 0.6375\n",
      "    ROC-AUC score: 0.9028\n",
      "    Batch 12 / 136: loss 0.6446\n",
      "    ROC-AUC score: 0.9595\n",
      "    Batch 15 / 136: loss 0.6527\n",
      "    ROC-AUC score: 0.7381\n",
      "    Batch 18 / 136: loss 0.6000\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 21 / 136: loss 0.5892\n",
      "    ROC-AUC score: 0.8281\n",
      "    Batch 24 / 136: loss 0.6226\n",
      "    ROC-AUC score: 0.8549\n",
      "    Batch 27 / 136: loss 0.5797\n",
      "    ROC-AUC score: 0.8413\n",
      "    Batch 30 / 136: loss 0.5950\n",
      "    ROC-AUC score: 0.9059\n",
      "    Batch 33 / 136: loss 0.5699\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 36 / 136: loss 0.5819\n",
      "    ROC-AUC score: 0.7458\n",
      "    Batch 39 / 136: loss 0.6146\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 42 / 136: loss 0.5995\n",
      "    ROC-AUC score: 0.9176\n",
      "    Batch 45 / 136: loss 0.6746\n",
      "    ROC-AUC score: 0.9458\n",
      "    Batch 48 / 136: loss 0.6181\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 51 / 136: loss 0.6206\n",
      "    ROC-AUC score: 0.8452\n",
      "    Batch 54 / 136: loss 0.5976\n",
      "    ROC-AUC score: 0.9451\n",
      "    Batch 57 / 136: loss 0.5874\n",
      "    ROC-AUC score: 0.9167\n",
      "    Batch 60 / 136: loss 0.5824\n",
      "    ROC-AUC score: 0.9127\n",
      "    Batch 63 / 136: loss 0.6324\n",
      "    ROC-AUC score: 0.9294\n",
      "    Batch 66 / 136: loss 0.5634\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 69 / 136: loss 0.5912\n",
      "    ROC-AUC score: 0.8413\n",
      "    Batch 72 / 136: loss 0.6268\n",
      "    ROC-AUC score: 0.8744\n",
      "    Batch 75 / 136: loss 0.6243\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 78 / 136: loss 0.6224\n",
      "    ROC-AUC score: 0.8333\n",
      "    Batch 81 / 136: loss 0.6491\n",
      "    ROC-AUC score: 0.7817\n",
      "    Batch 84 / 136: loss 0.6125\n",
      "    ROC-AUC score: 0.9069\n",
      "    Batch 87 / 136: loss 0.5963\n",
      "    ROC-AUC score: 0.9844\n",
      "    Batch 90 / 136: loss 0.6717\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 93 / 136: loss 0.5621\n",
      "    ROC-AUC score: 0.7937\n",
      "    Batch 96 / 136: loss 0.6071\n",
      "    ROC-AUC score: 0.9683\n",
      "    Batch 99 / 136: loss 0.6398\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 102 / 136: loss 0.6338\n",
      "    ROC-AUC score: 0.7686\n",
      "    Batch 105 / 136: loss 0.5884\n",
      "    ROC-AUC score: 0.8867\n",
      "    Batch 108 / 136: loss 0.6069\n",
      "    ROC-AUC score: 0.8826\n",
      "    Batch 111 / 136: loss 0.5757\n",
      "    ROC-AUC score: 0.9455\n",
      "    Batch 114 / 136: loss 0.6253\n",
      "    ROC-AUC score: 0.8438\n",
      "    Batch 117 / 136: loss 0.6675\n",
      "    ROC-AUC score: 0.9246\n",
      "    Batch 120 / 136: loss 0.6079\n",
      "    ROC-AUC score: 0.8633\n",
      "    Batch 123 / 136: loss 0.6130\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 126 / 136: loss 0.6226\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 129 / 136: loss 0.5501\n",
      "    ROC-AUC score: 0.9150\n",
      "    Batch 132 / 136: loss 0.5937\n",
      "    ROC-AUC score: 0.8078\n",
      "    Batch 135 / 136: loss 0.5643\n",
      "    ROC-AUC score: 0.8452\n",
      "Finished training.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    use_visdom = config['visdom']\n",
    "    if use_visdom:\n",
    "        vis = visdom.Visdom()\n",
    "        loss_window = None\n",
    "    criterion = utils.get_criterion(config['task'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                           weight_decay=config['weight_decay'])\n",
    "    epochs = config['epochs']\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.8)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 500, 1000], gamma=0.7)\n",
    "    model.train()\n",
    "    print('--------------------------------')\n",
    "    print('Training.')\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "        running_loss = 0.0\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            loss = criterion(scores, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "            if (idx + 1) % stats_per_batch == 0:\n",
    "                running_loss /= stats_per_batch\n",
    "                print('    Batch {} / {}: loss {:.4f}'.format(\n",
    "                    idx+1, num_batches, running_loss))\n",
    "                if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "                    area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "                    print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "                running_loss = 0.0\n",
    "                num_correct, num_examples = 0, 0\n",
    "            if use_visdom:\n",
    "                if loss_window is None:\n",
    "                    loss_window = vis.line(\n",
    "                        Y=[loss.item()],\n",
    "                        X=[epoch*num_batches+idx],\n",
    "                        opts=dict(xlabel='batch', ylabel='Loss', title='Training Loss', legend=['Loss']))\n",
    "                else:\n",
    "                    vis.line(\n",
    "                        [loss.item()],\n",
    "                        [epoch*num_batches+idx],\n",
    "                        win=loss_window,\n",
    "                        update='append')\n",
    "            scheduler.step()\n",
    "    if use_visdom:\n",
    "        vis.close(win=loss_window)\n",
    "    print('Finished training.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['load']:\n",
    "    if config['save']:\n",
    "        print('--------------------------------')\n",
    "        directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                'trained_models')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        fname = utils.get_fname(config)\n",
    "        path = os.path.join(directory, fname)\n",
    "        print('Saving model at {}'.format(path))\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print('Finished saving model.')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset after training.\n",
      "    Batch 1 / 136\n",
      "    Batch 2 / 136\n",
      "    Batch 3 / 136\n",
      "    Batch 4 / 136\n",
      "    Batch 5 / 136\n",
      "    Batch 6 / 136\n",
      "    Batch 7 / 136\n",
      "    Batch 8 / 136\n",
      "    Batch 9 / 136\n",
      "    Batch 10 / 136\n",
      "    Batch 11 / 136\n",
      "    Batch 12 / 136\n",
      "    Batch 13 / 136\n",
      "    Batch 14 / 136\n",
      "    Batch 15 / 136\n",
      "    Batch 16 / 136\n",
      "    Batch 17 / 136\n",
      "    Batch 18 / 136\n",
      "    Batch 19 / 136\n",
      "    Batch 20 / 136\n",
      "    Batch 21 / 136\n",
      "    Batch 22 / 136\n",
      "    Batch 23 / 136\n",
      "    Batch 24 / 136\n",
      "    Batch 25 / 136\n",
      "    Batch 26 / 136\n",
      "    Batch 27 / 136\n",
      "    Batch 28 / 136\n",
      "    Batch 29 / 136\n",
      "    Batch 30 / 136\n",
      "    Batch 31 / 136\n",
      "    Batch 32 / 136\n",
      "    Batch 33 / 136\n",
      "    Batch 34 / 136\n",
      "    Batch 35 / 136\n",
      "    Batch 36 / 136\n",
      "    Batch 37 / 136\n",
      "    Batch 38 / 136\n",
      "    Batch 39 / 136\n",
      "    Batch 40 / 136\n",
      "    Batch 41 / 136\n",
      "    Batch 42 / 136\n",
      "    Batch 43 / 136\n",
      "    Batch 44 / 136\n",
      "    Batch 45 / 136\n",
      "    Batch 46 / 136\n",
      "    Batch 47 / 136\n",
      "    Batch 48 / 136\n",
      "    Batch 49 / 136\n",
      "    Batch 50 / 136\n",
      "    Batch 51 / 136\n",
      "    Batch 52 / 136\n",
      "    Batch 53 / 136\n",
      "    Batch 54 / 136\n",
      "    Batch 55 / 136\n",
      "    Batch 56 / 136\n",
      "    Batch 57 / 136\n",
      "    Batch 58 / 136\n",
      "    Batch 59 / 136\n",
      "    Batch 60 / 136\n",
      "    Batch 61 / 136\n",
      "    Batch 62 / 136\n",
      "    Batch 63 / 136\n",
      "    Batch 64 / 136\n",
      "    Batch 65 / 136\n",
      "    Batch 66 / 136\n",
      "    Batch 67 / 136\n",
      "    Batch 68 / 136\n",
      "    Batch 69 / 136\n",
      "    Batch 70 / 136\n",
      "    Batch 71 / 136\n",
      "    Batch 72 / 136\n",
      "    Batch 73 / 136\n",
      "    Batch 74 / 136\n",
      "    Batch 75 / 136\n",
      "    Batch 76 / 136\n",
      "    Batch 77 / 136\n",
      "    Batch 78 / 136\n",
      "    Batch 79 / 136\n",
      "    Batch 80 / 136\n",
      "    Batch 81 / 136\n",
      "    Batch 82 / 136\n",
      "    Batch 83 / 136\n",
      "    Batch 84 / 136\n",
      "    Batch 85 / 136\n",
      "    Batch 86 / 136\n",
      "    Batch 87 / 136\n",
      "    Batch 88 / 136\n",
      "    Batch 89 / 136\n",
      "    Batch 90 / 136\n",
      "    Batch 91 / 136\n",
      "    Batch 92 / 136\n",
      "    Batch 93 / 136\n",
      "    Batch 94 / 136\n",
      "    Batch 95 / 136\n",
      "    Batch 96 / 136\n",
      "    Batch 97 / 136\n",
      "    Batch 98 / 136\n",
      "    Batch 99 / 136\n",
      "    Batch 100 / 136\n",
      "    Batch 101 / 136\n",
      "    Batch 102 / 136\n",
      "    Batch 103 / 136\n",
      "    Batch 104 / 136\n",
      "    Batch 105 / 136\n",
      "    Batch 106 / 136\n",
      "    Batch 107 / 136\n",
      "    Batch 108 / 136\n",
      "    Batch 109 / 136\n",
      "    Batch 110 / 136\n",
      "    Batch 111 / 136\n",
      "    Batch 112 / 136\n",
      "    Batch 113 / 136\n",
      "    Batch 114 / 136\n",
      "    Batch 115 / 136\n",
      "    Batch 116 / 136\n",
      "    Batch 117 / 136\n",
      "    Batch 118 / 136\n",
      "    Batch 119 / 136\n",
      "    Batch 120 / 136\n",
      "    Batch 121 / 136\n",
      "    Batch 122 / 136\n",
      "    Batch 123 / 136\n",
      "    Batch 124 / 136\n",
      "    Batch 125 / 136\n",
      "    Batch 126 / 136\n",
      "    Batch 127 / 136\n",
      "    Batch 128 / 136\n",
      "    Batch 129 / 136\n",
      "    Batch 130 / 136\n",
      "    Batch 131 / 136\n",
      "    Batch 132 / 136\n",
      "    Batch 133 / 136\n",
      "    Batch 134 / 136\n",
      "    Batch 135 / 136\n",
      "    Batch 136 / 136\n",
      "ROC-AUC score: 0.8761\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset after training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the true positive rate and true negative rate vs threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    tpr, fpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    tnr = 1 - fpr\n",
    "    plt.plot(thresholds, tpr, label='tpr')\n",
    "    plt.plot(thresholds, tnr, label='tnr')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.title('TPR / TNR vs Threshold')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an appropriate threshold and generate classification report on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 1 / 136\n",
      "    Batch 2 / 136\n",
      "    Batch 3 / 136\n",
      "    Batch 4 / 136\n",
      "    Batch 5 / 136\n",
      "    Batch 6 / 136\n",
      "    Batch 7 / 136\n",
      "    Batch 8 / 136\n",
      "    Batch 9 / 136\n",
      "    Batch 10 / 136\n",
      "    Batch 11 / 136\n",
      "    Batch 12 / 136\n",
      "    Batch 13 / 136\n",
      "    Batch 14 / 136\n",
      "    Batch 15 / 136\n",
      "    Batch 16 / 136\n",
      "    Batch 17 / 136\n",
      "    Batch 18 / 136\n",
      "    Batch 19 / 136\n",
      "    Batch 20 / 136\n",
      "    Batch 21 / 136\n",
      "    Batch 22 / 136\n",
      "    Batch 23 / 136\n",
      "    Batch 24 / 136\n",
      "    Batch 25 / 136\n",
      "    Batch 26 / 136\n",
      "    Batch 27 / 136\n",
      "    Batch 28 / 136\n",
      "    Batch 29 / 136\n",
      "    Batch 30 / 136\n",
      "    Batch 31 / 136\n",
      "    Batch 32 / 136\n",
      "    Batch 33 / 136\n",
      "    Batch 34 / 136\n",
      "    Batch 35 / 136\n",
      "    Batch 36 / 136\n",
      "    Batch 37 / 136\n",
      "    Batch 38 / 136\n",
      "    Batch 39 / 136\n",
      "    Batch 40 / 136\n",
      "    Batch 41 / 136\n",
      "    Batch 42 / 136\n",
      "    Batch 43 / 136\n",
      "    Batch 44 / 136\n",
      "    Batch 45 / 136\n",
      "    Batch 46 / 136\n",
      "    Batch 47 / 136\n",
      "    Batch 48 / 136\n",
      "    Batch 49 / 136\n",
      "    Batch 50 / 136\n",
      "    Batch 51 / 136\n",
      "    Batch 52 / 136\n",
      "    Batch 53 / 136\n",
      "    Batch 54 / 136\n",
      "    Batch 55 / 136\n",
      "    Batch 56 / 136\n",
      "    Batch 57 / 136\n",
      "    Batch 58 / 136\n",
      "    Batch 59 / 136\n",
      "    Batch 60 / 136\n",
      "    Batch 61 / 136\n",
      "    Batch 62 / 136\n",
      "    Batch 63 / 136\n",
      "    Batch 64 / 136\n",
      "    Batch 65 / 136\n",
      "    Batch 66 / 136\n",
      "    Batch 67 / 136\n",
      "    Batch 68 / 136\n",
      "    Batch 69 / 136\n",
      "    Batch 70 / 136\n",
      "    Batch 71 / 136\n",
      "    Batch 72 / 136\n",
      "    Batch 73 / 136\n",
      "    Batch 74 / 136\n",
      "    Batch 75 / 136\n",
      "    Batch 76 / 136\n",
      "    Batch 77 / 136\n",
      "    Batch 78 / 136\n",
      "    Batch 79 / 136\n",
      "    Batch 80 / 136\n",
      "    Batch 81 / 136\n",
      "    Batch 82 / 136\n",
      "    Batch 83 / 136\n",
      "    Batch 84 / 136\n",
      "    Batch 85 / 136\n",
      "    Batch 86 / 136\n",
      "    Batch 87 / 136\n",
      "    Batch 88 / 136\n",
      "    Batch 89 / 136\n",
      "    Batch 90 / 136\n",
      "    Batch 91 / 136\n",
      "    Batch 92 / 136\n",
      "    Batch 93 / 136\n",
      "    Batch 94 / 136\n",
      "    Batch 95 / 136\n",
      "    Batch 96 / 136\n",
      "    Batch 97 / 136\n",
      "    Batch 98 / 136\n",
      "    Batch 99 / 136\n",
      "    Batch 100 / 136\n",
      "    Batch 101 / 136\n",
      "    Batch 102 / 136\n",
      "    Batch 103 / 136\n",
      "    Batch 104 / 136\n",
      "    Batch 105 / 136\n",
      "    Batch 106 / 136\n",
      "    Batch 107 / 136\n",
      "    Batch 108 / 136\n",
      "    Batch 109 / 136\n",
      "    Batch 110 / 136\n",
      "    Batch 111 / 136\n",
      "    Batch 112 / 136\n",
      "    Batch 113 / 136\n",
      "    Batch 114 / 136\n",
      "    Batch 115 / 136\n",
      "    Batch 116 / 136\n",
      "    Batch 117 / 136\n",
      "    Batch 118 / 136\n",
      "    Batch 119 / 136\n",
      "    Batch 120 / 136\n",
      "    Batch 121 / 136\n",
      "    Batch 122 / 136\n",
      "    Batch 123 / 136\n",
      "    Batch 124 / 136\n",
      "    Batch 125 / 136\n",
      "    Batch 126 / 136\n",
      "    Batch 127 / 136\n",
      "    Batch 128 / 136\n",
      "    Batch 129 / 136\n",
      "    Batch 130 / 136\n",
      "    Batch 131 / 136\n",
      "    Batch 132 / 136\n",
      "    Batch 133 / 136\n",
      "    Batch 134 / 136\n",
      "    Batch 135 / 136\n",
      "    Batch 136 / 136\n",
      "Threshold: 0.6535, accuracy: 0.7996\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.80      0.80      2163\n",
      "         1.0       0.80      0.80      0.80      2163\n",
      "\n",
      "    accuracy                           0.80      4326\n",
      "   macro avg       0.80      0.80      0.80      4326\n",
      "weighted avg       0.80      0.80      0.80      4326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx1 = np.where(tpr <= tnr)[0]\n",
    "idx2 = np.where(tpr >= tnr)[0]\n",
    "t = thresholds[idx1[-1]]\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "with torch.no_grad():\n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        edges, features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        all_pairs = torch.mm(out, out.t())\n",
    "        scores = all_pairs[edges.T]\n",
    "        predictions = (scores >= t).long()\n",
    "        y_true.extend(labels.detach().numpy())\n",
    "        y_pred.extend(predictions.detach().numpy())\n",
    "        total_correct += torch.sum(predictions == labels.long()).item()\n",
    "        total_examples += len(labels) \n",
    "        print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "print('Threshold: {:.4f}, accuracy: {:.4f}'.format(t, total_correct / total_examples))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "print('Classification report\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-radoslaw-email/ia-radoslaw-email.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: test\n",
      "Number of vertices: 167\n",
      "Number of static edges: 5358\n",
      "Number of temporal edges: 62195\n",
      "Number of examples/datapoints: 3576\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the test dataset after training.\n",
      "    Batch 3 / 112: loss 0.6510, accuracy 0.7083\n",
      "    ROC-AUC score: 0.9676\n",
      "    Batch 6 / 112: loss 0.6676, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8532\n",
      "    Batch 9 / 112: loss 0.5935, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8203\n",
      "    Batch 12 / 112: loss 0.6356, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9048\n",
      "    Batch 15 / 112: loss 0.6200, accuracy 0.8021\n",
      "    ROC-AUC score: 0.8651\n",
      "    Batch 18 / 112: loss 0.5962, accuracy 0.8333\n",
      "    ROC-AUC score: 0.8789\n",
      "    Batch 21 / 112: loss 0.6223, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 24 / 112: loss 0.5824, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9351\n",
      "    Batch 27 / 112: loss 0.6190, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 30 / 112: loss 0.5680, accuracy 0.8646\n",
      "    ROC-AUC score: 0.8988\n",
      "    Batch 33 / 112: loss 0.6077, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8588\n",
      "    Batch 36 / 112: loss 0.6532, accuracy 0.7500\n",
      "    ROC-AUC score: 0.7965\n",
      "    Batch 39 / 112: loss 0.5988, accuracy 0.8021\n",
      "    ROC-AUC score: 0.9636\n",
      "    Batch 42 / 112: loss 0.6151, accuracy 0.7917\n",
      "    ROC-AUC score: 0.9312\n",
      "    Batch 45 / 112: loss 0.6130, accuracy 0.8021\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 48 / 112: loss 0.6081, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 51 / 112: loss 0.6401, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8828\n",
      "    Batch 54 / 112: loss 0.6116, accuracy 0.8229\n",
      "    ROC-AUC score: 0.8000\n",
      "    Batch 57 / 112: loss 0.5658, accuracy 0.7708\n",
      "    ROC-AUC score: 0.9042\n",
      "    Batch 60 / 112: loss 0.6412, accuracy 0.7708\n",
      "    ROC-AUC score: 0.7686\n",
      "    Batch 63 / 112: loss 0.6200, accuracy 0.8229\n",
      "    ROC-AUC score: 0.9098\n",
      "    Batch 66 / 112: loss 0.5973, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9514\n",
      "    Batch 69 / 112: loss 0.5522, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9216\n",
      "    Batch 72 / 112: loss 0.5979, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8968\n",
      "    Batch 75 / 112: loss 0.6153, accuracy 0.7917\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 78 / 112: loss 0.5765, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8849\n",
      "    Batch 81 / 112: loss 0.6308, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 84 / 112: loss 0.6356, accuracy 0.7812\n",
      "    ROC-AUC score: 0.9216\n",
      "    Batch 87 / 112: loss 0.6224, accuracy 0.7917\n",
      "    ROC-AUC score: 0.9258\n",
      "    Batch 90 / 112: loss 0.5978, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9807\n",
      "    Batch 93 / 112: loss 0.5828, accuracy 0.7396\n",
      "    ROC-AUC score: 0.7206\n",
      "    Batch 96 / 112: loss 0.5867, accuracy 0.8021\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 99 / 112: loss 0.6095, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9208\n",
      "    Batch 102 / 112: loss 0.6290, accuracy 0.7604\n",
      "    ROC-AUC score: 0.9490\n",
      "    Batch 105 / 112: loss 0.6008, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9717\n",
      "    Batch 108 / 112: loss 0.6312, accuracy 0.7708\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 111 / 112: loss 0.6164, accuracy 0.7604\n",
      "    ROC-AUC score: 0.7804\n",
      "Loss 0.6106, accuracy 0.7931\n",
      "ROC-AUC score: 0.8545\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.72      0.78      1788\n",
      "         1.0       0.76      0.87      0.81      1788\n",
      "\n",
      "    accuracy                           0.79      3576\n",
      "   macro avg       0.80      0.79      0.79      3576\n",
      "weighted avg       0.80      0.79      0.79      3576\n",
      "\n",
      "Finished testing.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'test',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the test dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished testing.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
