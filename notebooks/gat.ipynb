{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import visdom\n",
    "\n",
    "from datasets import link_prediction\n",
    "from layers import MeanAggregator, LSTMAggregator, MaxPoolAggregator, MeanPoolAggregator\n",
    "import models\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up arguments for datasets, models and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"task\" : \"link_prediction\",\n",
    "    \n",
    "    \"dataset\" : \"IAContactsHypertext\",\n",
    "    \"dataset_path\" : \"/Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"generate_neg_examples\" : False,\n",
    "    \n",
    "    \"duplicate_examples\" : True,\n",
    "    \"repeat_examples\" : True,\n",
    "    \n",
    "    \"self_loop\" : True,\n",
    "    \"normalize_adj\" : False,\n",
    "    \n",
    "    \"cuda\" : True,\n",
    "    \"model\" : \"GAT\",\n",
    "    \"num_heads\" : [1, 1],\n",
    "    \"hidden_dims\" : [64],\n",
    "    \"dropout\" : 0,\n",
    "    \n",
    "    \"epochs\" : 3,\n",
    "    \"batch_size\" : 32,\n",
    "    \"lr\" : 5e-4,\n",
    "    \"weight_decay\" : 5e-3,\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"visdom\" : True,\n",
    "    \n",
    "    \"load\" : False,\n",
    "    \"save\" : False\n",
    "}\n",
    "config = args\n",
    "config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "\n",
    "if config['cuda'] and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "config['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset, dataloader and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contacts_hypertext2009/ia-contacts_hypertext2009.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: train\n",
      "Number of vertices: 113\n",
      "Number of static edges: 1010\n",
      "Number of temporal edges: 6245\n",
      "Number of examples/datapoints: 7886\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'train',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "input_dim, output_dim = dataset.get_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (attn): ModuleList(\n",
      "    (0): GraphAttention(\n",
      "      (fcs): ModuleList(\n",
      "        (0): Linear(in_features=113, out_features=64, bias=True)\n",
      "      )\n",
      "      (a): ModuleList(\n",
      "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0)\n",
      "      (softmax): Softmax()\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): GraphAttention(\n",
      "      (fcs): ModuleList(\n",
      "        (0): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (a): ModuleList(\n",
      "        (0): Linear(in_features=2, out_features=1, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0)\n",
      "      (softmax): Softmax()\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0)\n",
      "  (elu): ELU(alpha=1.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.GAT(input_dim, config['hidden_dims'],\n",
    "                   output_dim, config['num_heads'],\n",
    "                   config['dropout'], config['device'])\n",
    "model.apply(models.init_weights)\n",
    "model.to(config['device'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score for the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset before training.\n",
      "    Batch 1 / 247\n",
      "    Batch 2 / 247\n",
      "    Batch 3 / 247\n",
      "    Batch 4 / 247\n",
      "    Batch 5 / 247\n",
      "    Batch 6 / 247\n",
      "    Batch 7 / 247\n",
      "    Batch 8 / 247\n",
      "    Batch 9 / 247\n",
      "    Batch 10 / 247\n",
      "    Batch 11 / 247\n",
      "    Batch 12 / 247\n",
      "    Batch 13 / 247\n",
      "    Batch 14 / 247\n",
      "    Batch 15 / 247\n",
      "    Batch 16 / 247\n",
      "    Batch 17 / 247\n",
      "    Batch 18 / 247\n",
      "    Batch 19 / 247\n",
      "    Batch 20 / 247\n",
      "    Batch 21 / 247\n",
      "    Batch 22 / 247\n",
      "    Batch 23 / 247\n",
      "    Batch 24 / 247\n",
      "    Batch 25 / 247\n",
      "    Batch 26 / 247\n",
      "    Batch 27 / 247\n",
      "    Batch 28 / 247\n",
      "    Batch 29 / 247\n",
      "    Batch 30 / 247\n",
      "    Batch 31 / 247\n",
      "    Batch 32 / 247\n",
      "    Batch 33 / 247\n",
      "    Batch 34 / 247\n",
      "    Batch 35 / 247\n",
      "    Batch 36 / 247\n",
      "    Batch 37 / 247\n",
      "    Batch 38 / 247\n",
      "    Batch 39 / 247\n",
      "    Batch 40 / 247\n",
      "    Batch 41 / 247\n",
      "    Batch 42 / 247\n",
      "    Batch 43 / 247\n",
      "    Batch 44 / 247\n",
      "    Batch 45 / 247\n",
      "    Batch 46 / 247\n",
      "    Batch 47 / 247\n",
      "    Batch 48 / 247\n",
      "    Batch 49 / 247\n",
      "    Batch 50 / 247\n",
      "    Batch 51 / 247\n",
      "    Batch 52 / 247\n",
      "    Batch 53 / 247\n",
      "    Batch 54 / 247\n",
      "    Batch 55 / 247\n",
      "    Batch 56 / 247\n",
      "    Batch 57 / 247\n",
      "    Batch 58 / 247\n",
      "    Batch 59 / 247\n",
      "    Batch 60 / 247\n",
      "    Batch 61 / 247\n",
      "    Batch 62 / 247\n",
      "    Batch 63 / 247\n",
      "    Batch 64 / 247\n",
      "    Batch 65 / 247\n",
      "    Batch 66 / 247\n",
      "    Batch 67 / 247\n",
      "    Batch 68 / 247\n",
      "    Batch 69 / 247\n",
      "    Batch 70 / 247\n",
      "    Batch 71 / 247\n",
      "    Batch 72 / 247\n",
      "    Batch 73 / 247\n",
      "    Batch 74 / 247\n",
      "    Batch 75 / 247\n",
      "    Batch 76 / 247\n",
      "    Batch 77 / 247\n",
      "    Batch 78 / 247\n",
      "    Batch 79 / 247\n",
      "    Batch 80 / 247\n",
      "    Batch 81 / 247\n",
      "    Batch 82 / 247\n",
      "    Batch 83 / 247\n",
      "    Batch 84 / 247\n",
      "    Batch 85 / 247\n",
      "    Batch 86 / 247\n",
      "    Batch 87 / 247\n",
      "    Batch 88 / 247\n",
      "    Batch 89 / 247\n",
      "    Batch 90 / 247\n",
      "    Batch 91 / 247\n",
      "    Batch 92 / 247\n",
      "    Batch 93 / 247\n",
      "    Batch 94 / 247\n",
      "    Batch 95 / 247\n",
      "    Batch 96 / 247\n",
      "    Batch 97 / 247\n",
      "    Batch 98 / 247\n",
      "    Batch 99 / 247\n",
      "    Batch 100 / 247\n",
      "    Batch 101 / 247\n",
      "    Batch 102 / 247\n",
      "    Batch 103 / 247\n",
      "    Batch 104 / 247\n",
      "    Batch 105 / 247\n",
      "    Batch 106 / 247\n",
      "    Batch 107 / 247\n",
      "    Batch 108 / 247\n",
      "    Batch 109 / 247\n",
      "    Batch 110 / 247\n",
      "    Batch 111 / 247\n",
      "    Batch 112 / 247\n",
      "    Batch 113 / 247\n",
      "    Batch 114 / 247\n",
      "    Batch 115 / 247\n",
      "    Batch 116 / 247\n",
      "    Batch 117 / 247\n",
      "    Batch 118 / 247\n",
      "    Batch 119 / 247\n",
      "    Batch 120 / 247\n",
      "    Batch 121 / 247\n",
      "    Batch 122 / 247\n",
      "    Batch 123 / 247\n",
      "    Batch 124 / 247\n",
      "    Batch 125 / 247\n",
      "    Batch 126 / 247\n",
      "    Batch 127 / 247\n",
      "    Batch 128 / 247\n",
      "    Batch 129 / 247\n",
      "    Batch 130 / 247\n",
      "    Batch 131 / 247\n",
      "    Batch 132 / 247\n",
      "    Batch 133 / 247\n",
      "    Batch 134 / 247\n",
      "    Batch 135 / 247\n",
      "    Batch 136 / 247\n",
      "    Batch 137 / 247\n",
      "    Batch 138 / 247\n",
      "    Batch 139 / 247\n",
      "    Batch 140 / 247\n",
      "    Batch 141 / 247\n",
      "    Batch 142 / 247\n",
      "    Batch 143 / 247\n",
      "    Batch 144 / 247\n",
      "    Batch 145 / 247\n",
      "    Batch 146 / 247\n",
      "    Batch 147 / 247\n",
      "    Batch 148 / 247\n",
      "    Batch 149 / 247\n",
      "    Batch 150 / 247\n",
      "    Batch 151 / 247\n",
      "    Batch 152 / 247\n",
      "    Batch 153 / 247\n",
      "    Batch 154 / 247\n",
      "    Batch 155 / 247\n",
      "    Batch 156 / 247\n",
      "    Batch 157 / 247\n",
      "    Batch 158 / 247\n",
      "    Batch 159 / 247\n",
      "    Batch 160 / 247\n",
      "    Batch 161 / 247\n",
      "    Batch 162 / 247\n",
      "    Batch 163 / 247\n",
      "    Batch 164 / 247\n",
      "    Batch 165 / 247\n",
      "    Batch 166 / 247\n",
      "    Batch 167 / 247\n",
      "    Batch 168 / 247\n",
      "    Batch 169 / 247\n",
      "    Batch 170 / 247\n",
      "    Batch 171 / 247\n",
      "    Batch 172 / 247\n",
      "    Batch 173 / 247\n",
      "    Batch 174 / 247\n",
      "    Batch 175 / 247\n",
      "    Batch 176 / 247\n",
      "    Batch 177 / 247\n",
      "    Batch 178 / 247\n",
      "    Batch 179 / 247\n",
      "    Batch 180 / 247\n",
      "    Batch 181 / 247\n",
      "    Batch 182 / 247\n",
      "    Batch 183 / 247\n",
      "    Batch 184 / 247\n",
      "    Batch 185 / 247\n",
      "    Batch 186 / 247\n",
      "    Batch 187 / 247\n",
      "    Batch 188 / 247\n",
      "    Batch 189 / 247\n",
      "    Batch 190 / 247\n",
      "    Batch 191 / 247\n",
      "    Batch 192 / 247\n",
      "    Batch 193 / 247\n",
      "    Batch 194 / 247\n",
      "    Batch 195 / 247\n",
      "    Batch 196 / 247\n",
      "    Batch 197 / 247\n",
      "    Batch 198 / 247\n",
      "    Batch 199 / 247\n",
      "    Batch 200 / 247\n",
      "    Batch 201 / 247\n",
      "    Batch 202 / 247\n",
      "    Batch 203 / 247\n",
      "    Batch 204 / 247\n",
      "    Batch 205 / 247\n",
      "    Batch 206 / 247\n",
      "    Batch 207 / 247\n",
      "    Batch 208 / 247\n",
      "    Batch 209 / 247\n",
      "    Batch 210 / 247\n",
      "    Batch 211 / 247\n",
      "    Batch 212 / 247\n",
      "    Batch 213 / 247\n",
      "    Batch 214 / 247\n",
      "    Batch 215 / 247\n",
      "    Batch 216 / 247\n",
      "    Batch 217 / 247\n",
      "    Batch 218 / 247\n",
      "    Batch 219 / 247\n",
      "    Batch 220 / 247\n",
      "    Batch 221 / 247\n",
      "    Batch 222 / 247\n",
      "    Batch 223 / 247\n",
      "    Batch 224 / 247\n",
      "    Batch 225 / 247\n",
      "    Batch 226 / 247\n",
      "    Batch 227 / 247\n",
      "    Batch 228 / 247\n",
      "    Batch 229 / 247\n",
      "    Batch 230 / 247\n",
      "    Batch 231 / 247\n",
      "    Batch 232 / 247\n",
      "    Batch 233 / 247\n",
      "    Batch 234 / 247\n",
      "    Batch 235 / 247\n",
      "    Batch 236 / 247\n",
      "    Batch 237 / 247\n",
      "    Batch 238 / 247\n",
      "    Batch 239 / 247\n",
      "    Batch 240 / 247\n",
      "    Batch 241 / 247\n",
      "    Batch 242 / 247\n",
      "    Batch 243 / 247\n",
      "    Batch 244 / 247\n",
      "    Batch 245 / 247\n",
      "    Batch 246 / 247\n",
      "    Batch 247 / 247\n",
      "ROC-AUC score: 0.4780\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset before training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Training.\n",
      "Epoch 1 / 3\n",
      "    Batch 3 / 247: loss 0.9873\n",
      "    ROC-AUC score: 0.3961\n",
      "    Batch 6 / 247: loss 0.8091\n",
      "    ROC-AUC score: 0.7176\n",
      "    Batch 9 / 247: loss 0.8724\n",
      "    ROC-AUC score: 0.5059\n",
      "    Batch 12 / 247: loss 0.7415\n",
      "    ROC-AUC score: 0.4841\n",
      "    Batch 15 / 247: loss 0.7847\n",
      "    ROC-AUC score: 0.4980\n",
      "    Batch 18 / 247: loss 0.7850\n",
      "    ROC-AUC score: 0.4702\n",
      "    Batch 21 / 247: loss 0.7221\n",
      "    ROC-AUC score: 0.3441\n",
      "    Batch 24 / 247: loss 0.8215\n",
      "    ROC-AUC score: 0.5686\n",
      "    Batch 27 / 247: loss 0.6875\n",
      "    ROC-AUC score: 0.5833\n",
      "    Batch 30 / 247: loss 0.7625\n",
      "    ROC-AUC score: 0.6397\n",
      "    Batch 33 / 247: loss 0.7719\n",
      "    ROC-AUC score: 0.6587\n",
      "    Batch 36 / 247: loss 0.7002\n",
      "    ROC-AUC score: 0.6875\n",
      "    Batch 39 / 247: loss 0.6731\n",
      "    ROC-AUC score: 0.4042\n",
      "    Batch 42 / 247: loss 0.7042\n",
      "    ROC-AUC score: 0.6599\n",
      "    Batch 45 / 247: loss 0.7236\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 48 / 247: loss 0.9591\n",
      "    ROC-AUC score: 0.4008\n",
      "    Batch 51 / 247: loss 0.6662\n",
      "    ROC-AUC score: 0.6719\n",
      "    Batch 54 / 247: loss 0.6726\n",
      "    ROC-AUC score: 0.6314\n",
      "    Batch 57 / 247: loss 0.7254\n",
      "    ROC-AUC score: 0.6367\n",
      "    Batch 60 / 247: loss 0.6882\n",
      "    ROC-AUC score: 0.5385\n",
      "    Batch 63 / 247: loss 0.7118\n",
      "    ROC-AUC score: 0.6854\n",
      "    Batch 66 / 247: loss 0.6886\n",
      "    ROC-AUC score: 0.5686\n",
      "    Batch 69 / 247: loss 0.7308\n",
      "    ROC-AUC score: 0.6836\n",
      "    Batch 72 / 247: loss 0.6716\n",
      "    ROC-AUC score: 0.5977\n",
      "    Batch 75 / 247: loss 0.7101\n",
      "    ROC-AUC score: 0.4917\n",
      "    Batch 78 / 247: loss 0.6756\n",
      "    ROC-AUC score: 0.3922\n",
      "    Batch 81 / 247: loss 0.7084\n",
      "    ROC-AUC score: 0.5794\n",
      "    Batch 84 / 247: loss 0.6896\n",
      "    ROC-AUC score: 0.5951\n",
      "    Batch 87 / 247: loss 0.6591\n",
      "    ROC-AUC score: 0.7333\n",
      "    Batch 90 / 247: loss 0.6896\n",
      "    ROC-AUC score: 0.4667\n",
      "    Batch 93 / 247: loss 0.6811\n",
      "    ROC-AUC score: 0.6719\n",
      "    Batch 96 / 247: loss 0.6919\n",
      "    ROC-AUC score: 0.7098\n",
      "    Batch 99 / 247: loss 0.6798\n",
      "    ROC-AUC score: 0.5977\n",
      "    Batch 102 / 247: loss 0.7009\n",
      "    ROC-AUC score: 0.7235\n",
      "    Batch 105 / 247: loss 0.6651\n",
      "    ROC-AUC score: 0.7176\n",
      "    Batch 108 / 247: loss 0.7427\n",
      "    ROC-AUC score: 0.6039\n",
      "    Batch 111 / 247: loss 0.7104\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 114 / 247: loss 0.6686\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 117 / 247: loss 0.6851\n",
      "    ROC-AUC score: 0.5584\n",
      "    Batch 120 / 247: loss 0.6730\n",
      "    ROC-AUC score: 0.5458\n",
      "    Batch 123 / 247: loss 0.8236\n",
      "    ROC-AUC score: 0.6706\n",
      "    Batch 126 / 247: loss 0.6704\n",
      "    ROC-AUC score: 0.6277\n",
      "    Batch 129 / 247: loss 0.7280\n",
      "    ROC-AUC score: 0.5583\n",
      "    Batch 132 / 247: loss 0.6839\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 135 / 247: loss 0.6701\n",
      "    ROC-AUC score: 0.6277\n",
      "    Batch 138 / 247: loss 0.6645\n",
      "    ROC-AUC score: 0.6602\n",
      "    Batch 141 / 247: loss 0.6765\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 144 / 247: loss 0.6552\n",
      "    ROC-AUC score: 0.6437\n",
      "    Batch 147 / 247: loss 0.6678\n",
      "    ROC-AUC score: 0.6865\n",
      "    Batch 150 / 247: loss 0.6973\n",
      "    ROC-AUC score: 0.7063\n",
      "    Batch 153 / 247: loss 0.6815\n",
      "    ROC-AUC score: 0.5992\n",
      "    Batch 156 / 247: loss 0.6758\n",
      "    ROC-AUC score: 0.6537\n",
      "    Batch 159 / 247: loss 0.6543\n",
      "    ROC-AUC score: 0.7460\n",
      "    Batch 162 / 247: loss 0.6624\n",
      "    ROC-AUC score: 0.7000\n",
      "    Batch 165 / 247: loss 0.6620\n",
      "    ROC-AUC score: 0.8164\n",
      "    Batch 168 / 247: loss 0.6778\n",
      "    ROC-AUC score: 0.6275\n",
      "    Batch 171 / 247: loss 0.6845\n",
      "    ROC-AUC score: 0.5451\n",
      "    Batch 174 / 247: loss 0.6662\n",
      "    ROC-AUC score: 0.5273\n",
      "    Batch 177 / 247: loss 0.6698\n",
      "    ROC-AUC score: 0.5870\n",
      "    Batch 180 / 247: loss 0.6274\n",
      "    ROC-AUC score: 0.7656\n",
      "    Batch 183 / 247: loss 0.6626\n",
      "    ROC-AUC score: 0.6356\n",
      "    Batch 186 / 247: loss 0.6809\n",
      "    ROC-AUC score: 0.6763\n",
      "    Batch 189 / 247: loss 0.6524\n",
      "    ROC-AUC score: 0.7344\n",
      "    Batch 192 / 247: loss 0.6466\n",
      "    ROC-AUC score: 0.6196\n",
      "    Batch 195 / 247: loss 0.6583\n",
      "    ROC-AUC score: 0.6641\n",
      "    Batch 198 / 247: loss 0.7065\n",
      "    ROC-AUC score: 0.6194\n",
      "    Batch 201 / 247: loss 0.6711\n",
      "    ROC-AUC score: 0.5216\n",
      "    Batch 204 / 247: loss 0.6824\n",
      "    ROC-AUC score: 0.5625\n",
      "    Batch 207 / 247: loss 0.6821\n",
      "    ROC-AUC score: 0.8078\n",
      "    Batch 210 / 247: loss 0.6678\n",
      "    ROC-AUC score: 0.5708\n",
      "    Batch 213 / 247: loss 0.7004\n",
      "    ROC-AUC score: 0.5887\n",
      "    Batch 216 / 247: loss 0.6981\n",
      "    ROC-AUC score: 0.4980\n",
      "    Batch 219 / 247: loss 0.6647\n",
      "    ROC-AUC score: 0.5635\n",
      "    Batch 222 / 247: loss 0.6805\n",
      "    ROC-AUC score: 0.6763\n",
      "    Batch 225 / 247: loss 0.6764\n",
      "    ROC-AUC score: 0.6078\n",
      "    Batch 228 / 247: loss 0.6797\n",
      "    ROC-AUC score: 0.7778\n",
      "    Batch 231 / 247: loss 0.6810\n",
      "    ROC-AUC score: 0.5833\n",
      "    Batch 234 / 247: loss 0.6785\n",
      "    ROC-AUC score: 0.5397\n",
      "    Batch 237 / 247: loss 0.6509\n",
      "    ROC-AUC score: 0.7817\n",
      "    Batch 240 / 247: loss 0.6891\n",
      "    ROC-AUC score: 0.5176\n",
      "    Batch 243 / 247: loss 0.7357\n",
      "    ROC-AUC score: 0.5833\n",
      "    Batch 246 / 247: loss 0.6817\n",
      "    ROC-AUC score: 0.6478\n",
      "Epoch 2 / 3\n",
      "    Batch 3 / 247: loss 0.6549\n",
      "    ROC-AUC score: 0.6275\n",
      "    Batch 6 / 247: loss 0.6589\n",
      "    ROC-AUC score: 0.8135\n",
      "    Batch 9 / 247: loss 0.6840\n",
      "    ROC-AUC score: 0.4510\n",
      "    Batch 12 / 247: loss 0.6503\n",
      "    ROC-AUC score: 0.6510\n",
      "    Batch 15 / 247: loss 0.6509\n",
      "    ROC-AUC score: 0.7373\n",
      "    Batch 18 / 247: loss 0.6393\n",
      "    ROC-AUC score: 0.9514\n",
      "    Batch 21 / 247: loss 0.6437\n",
      "    ROC-AUC score: 0.6523\n",
      "    Batch 24 / 247: loss 0.6702\n",
      "    ROC-AUC score: 0.7344\n",
      "    Batch 27 / 247: loss 0.7190\n",
      "    ROC-AUC score: 0.3020\n",
      "    Batch 30 / 247: loss 0.6555\n",
      "    ROC-AUC score: 0.7143\n",
      "    Batch 33 / 247: loss 0.7548\n",
      "    ROC-AUC score: 0.6508\n",
      "    Batch 36 / 247: loss 0.6286\n",
      "    ROC-AUC score: 0.7804\n",
      "    Batch 39 / 247: loss 0.6536\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 42 / 247: loss 0.6571\n",
      "    ROC-AUC score: 0.7305\n",
      "    Batch 45 / 247: loss 0.6518\n",
      "    ROC-AUC score: 0.7421\n",
      "    Batch 48 / 247: loss 0.6288\n",
      "    ROC-AUC score: 0.8178\n",
      "    Batch 51 / 247: loss 0.6632\n",
      "    ROC-AUC score: 0.4727\n",
      "    Batch 54 / 247: loss 0.6670\n",
      "    ROC-AUC score: 0.6071\n",
      "    Batch 57 / 247: loss 0.6711\n",
      "    ROC-AUC score: 0.6417\n",
      "    Batch 60 / 247: loss 0.6550\n",
      "    ROC-AUC score: 0.7765\n",
      "    Batch 63 / 247: loss 0.6308\n",
      "    ROC-AUC score: 0.8698\n",
      "    Batch 66 / 247: loss 0.6644\n",
      "    ROC-AUC score: 0.6508\n",
      "    Batch 69 / 247: loss 0.6879\n",
      "    ROC-AUC score: 0.6417\n",
      "    Batch 72 / 247: loss 0.6626\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 75 / 247: loss 0.6405\n",
      "    ROC-AUC score: 0.8125\n",
      "    Batch 78 / 247: loss 0.6591\n",
      "    ROC-AUC score: 0.5456\n",
      "    Batch 81 / 247: loss 0.7159\n",
      "    ROC-AUC score: 0.6523\n",
      "    Batch 84 / 247: loss 0.6956\n",
      "    ROC-AUC score: 0.6445\n",
      "    Batch 87 / 247: loss 0.6545\n",
      "    ROC-AUC score: 0.7545\n",
      "    Batch 90 / 247: loss 0.6251\n",
      "    ROC-AUC score: 0.6235\n",
      "    Batch 93 / 247: loss 0.6326\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 96 / 247: loss 0.7576\n",
      "    ROC-AUC score: 0.6230\n",
      "    Batch 99 / 247: loss 0.6542\n",
      "    ROC-AUC score: 0.6078\n",
      "    Batch 102 / 247: loss 0.6483\n",
      "    ROC-AUC score: 0.6640\n",
      "    Batch 105 / 247: loss 0.6679\n",
      "    ROC-AUC score: 0.6455\n",
      "    Batch 108 / 247: loss 0.6550\n",
      "    ROC-AUC score: 0.6208\n",
      "    Batch 111 / 247: loss 0.6301\n",
      "    ROC-AUC score: 0.5675\n",
      "    Batch 114 / 247: loss 0.6354\n",
      "    ROC-AUC score: 0.6356\n",
      "    Batch 117 / 247: loss 0.6415\n",
      "    ROC-AUC score: 0.7608\n",
      "    Batch 120 / 247: loss 0.6609\n",
      "    ROC-AUC score: 0.6275\n",
      "    Batch 123 / 247: loss 0.6307\n",
      "    ROC-AUC score: 0.7085\n",
      "    Batch 126 / 247: loss 0.6609\n",
      "    ROC-AUC score: 0.7792\n",
      "    Batch 129 / 247: loss 0.6915\n",
      "    ROC-AUC score: 0.5725\n",
      "    Batch 132 / 247: loss 0.6711\n",
      "    ROC-AUC score: 0.6802\n",
      "    Batch 135 / 247: loss 0.6283\n",
      "    ROC-AUC score: 0.6548\n",
      "    Batch 138 / 247: loss 0.6458\n",
      "    ROC-AUC score: 0.5952\n",
      "    Batch 141 / 247: loss 0.6289\n",
      "    ROC-AUC score: 0.8549\n",
      "    Batch 144 / 247: loss 0.7147\n",
      "    ROC-AUC score: 0.4848\n",
      "    Batch 147 / 247: loss 0.6340\n",
      "    ROC-AUC score: 0.7176\n",
      "    Batch 150 / 247: loss 0.6515\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 153 / 247: loss 0.6932\n",
      "    ROC-AUC score: 0.6190\n",
      "    Batch 156 / 247: loss 0.6463\n",
      "    ROC-AUC score: 0.7109\n",
      "    Batch 159 / 247: loss 0.6438\n",
      "    ROC-AUC score: 0.5792\n",
      "    Batch 162 / 247: loss 0.6428\n",
      "    ROC-AUC score: 0.6917\n",
      "    Batch 165 / 247: loss 0.6892\n",
      "    ROC-AUC score: 0.6425\n",
      "    Batch 168 / 247: loss 0.7052\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 171 / 247: loss 0.6393\n",
      "    ROC-AUC score: 0.6984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 174 / 247: loss 0.6370\n",
      "    ROC-AUC score: 0.6902\n",
      "    Batch 177 / 247: loss 0.6329\n",
      "    ROC-AUC score: 0.8320\n",
      "    Batch 180 / 247: loss 0.6846\n",
      "    ROC-AUC score: 0.6270\n",
      "    Batch 183 / 247: loss 0.6305\n",
      "    ROC-AUC score: 0.7216\n",
      "    Batch 186 / 247: loss 0.6614\n",
      "    ROC-AUC score: 0.7852\n",
      "    Batch 189 / 247: loss 0.5969\n",
      "    ROC-AUC score: 0.8138\n",
      "    Batch 192 / 247: loss 0.6276\n",
      "    ROC-AUC score: 0.6349\n",
      "    Batch 195 / 247: loss 0.6156\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 198 / 247: loss 0.6582\n",
      "    ROC-AUC score: 0.6039\n",
      "    Batch 201 / 247: loss 0.6183\n",
      "    ROC-AUC score: 0.7183\n",
      "    Batch 204 / 247: loss 0.7445\n",
      "    ROC-AUC score: 0.5859\n",
      "    Batch 207 / 247: loss 0.6393\n",
      "    ROC-AUC score: 0.5668\n",
      "    Batch 210 / 247: loss 0.6908\n",
      "    ROC-AUC score: 0.7341\n",
      "    Batch 213 / 247: loss 0.6715\n",
      "    ROC-AUC score: 0.7647\n",
      "    Batch 216 / 247: loss 0.6257\n",
      "    ROC-AUC score: 0.7652\n",
      "    Batch 219 / 247: loss 0.6889\n",
      "    ROC-AUC score: 0.5397\n",
      "    Batch 222 / 247: loss 0.6695\n",
      "    ROC-AUC score: 0.7460\n",
      "    Batch 225 / 247: loss 0.6503\n",
      "    ROC-AUC score: 0.8646\n",
      "    Batch 228 / 247: loss 0.6732\n",
      "    ROC-AUC score: 0.5583\n",
      "    Batch 231 / 247: loss 0.6969\n",
      "    ROC-AUC score: 0.5411\n",
      "    Batch 234 / 247: loss 0.6711\n",
      "    ROC-AUC score: 0.7098\n",
      "    Batch 237 / 247: loss 0.6893\n",
      "    ROC-AUC score: 0.3929\n",
      "    Batch 240 / 247: loss 0.6501\n",
      "    ROC-AUC score: 0.8118\n",
      "    Batch 243 / 247: loss 0.6218\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 246 / 247: loss 0.7193\n",
      "    ROC-AUC score: 0.6196\n",
      "Epoch 3 / 3\n",
      "    Batch 3 / 247: loss 0.7335\n",
      "    ROC-AUC score: 0.5357\n",
      "    Batch 6 / 247: loss 0.7373\n",
      "    ROC-AUC score: 0.5587\n",
      "    Batch 9 / 247: loss 0.6555\n",
      "    ROC-AUC score: 0.5608\n",
      "    Batch 12 / 247: loss 0.6390\n",
      "    ROC-AUC score: 0.5625\n",
      "    Batch 15 / 247: loss 0.6179\n",
      "    ROC-AUC score: 0.6235\n",
      "    Batch 18 / 247: loss 0.6639\n",
      "    ROC-AUC score: 0.6562\n",
      "    Batch 21 / 247: loss 0.6555\n",
      "    ROC-AUC score: 0.7294\n",
      "    Batch 24 / 247: loss 0.6511\n",
      "    ROC-AUC score: 0.6824\n",
      "    Batch 27 / 247: loss 0.6310\n",
      "    ROC-AUC score: 0.9246\n",
      "    Batch 30 / 247: loss 0.6375\n",
      "    ROC-AUC score: 0.7012\n",
      "    Batch 33 / 247: loss 0.6357\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 36 / 247: loss 0.6111\n",
      "    ROC-AUC score: 0.7969\n",
      "    Batch 39 / 247: loss 0.6639\n",
      "    ROC-AUC score: 0.6784\n",
      "    Batch 42 / 247: loss 0.6368\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 45 / 247: loss 0.6600\n",
      "    ROC-AUC score: 0.6275\n",
      "    Batch 48 / 247: loss 0.6896\n",
      "    ROC-AUC score: 0.5833\n",
      "    Batch 51 / 247: loss 0.6341\n",
      "    ROC-AUC score: 0.8039\n",
      "    Batch 54 / 247: loss 0.6846\n",
      "    ROC-AUC score: 0.6353\n",
      "    Batch 57 / 247: loss 0.6594\n",
      "    ROC-AUC score: 0.5977\n",
      "    Batch 60 / 247: loss 0.6235\n",
      "    ROC-AUC score: 0.7579\n",
      "    Batch 63 / 247: loss 0.6191\n",
      "    ROC-AUC score: 0.7024\n",
      "    Batch 66 / 247: loss 0.6546\n",
      "    ROC-AUC score: 0.5176\n",
      "    Batch 69 / 247: loss 0.6832\n",
      "    ROC-AUC score: 0.5754\n",
      "    Batch 72 / 247: loss 0.6556\n",
      "    ROC-AUC score: 0.6758\n",
      "    Batch 75 / 247: loss 0.6461\n",
      "    ROC-AUC score: 0.6500\n",
      "    Batch 78 / 247: loss 0.6493\n",
      "    ROC-AUC score: 0.7344\n",
      "    Batch 81 / 247: loss 0.6549\n",
      "    ROC-AUC score: 0.7222\n",
      "    Batch 84 / 247: loss 0.6581\n",
      "    ROC-AUC score: 0.6032\n",
      "    Batch 87 / 247: loss 0.6732\n",
      "    ROC-AUC score: 0.7103\n",
      "    Batch 90 / 247: loss 0.6761\n",
      "    ROC-AUC score: 0.7004\n",
      "    Batch 93 / 247: loss 0.6684\n",
      "    ROC-AUC score: 0.7266\n",
      "    Batch 96 / 247: loss 0.7091\n",
      "    ROC-AUC score: 0.5830\n",
      "    Batch 99 / 247: loss 0.6303\n",
      "    ROC-AUC score: 0.6761\n",
      "    Batch 102 / 247: loss 0.6737\n",
      "    ROC-AUC score: 0.8314\n",
      "    Batch 105 / 247: loss 0.6148\n",
      "    ROC-AUC score: 0.8016\n",
      "    Batch 108 / 247: loss 0.6086\n",
      "    ROC-AUC score: 0.6549\n",
      "    Batch 111 / 247: loss 0.6783\n",
      "    ROC-AUC score: 0.6094\n",
      "    Batch 114 / 247: loss 0.6279\n",
      "    ROC-AUC score: 0.7738\n",
      "    Batch 117 / 247: loss 0.5928\n",
      "    ROC-AUC score: 0.6094\n",
      "    Batch 120 / 247: loss 0.6374\n",
      "    ROC-AUC score: 0.8157\n",
      "    Batch 123 / 247: loss 0.6840\n",
      "    ROC-AUC score: 0.6111\n",
      "    Batch 126 / 247: loss 0.7059\n",
      "    ROC-AUC score: 0.6230\n",
      "    Batch 129 / 247: loss 0.6184\n",
      "    ROC-AUC score: 0.6667\n",
      "    Batch 132 / 247: loss 0.6043\n",
      "    ROC-AUC score: 0.8375\n",
      "    Batch 135 / 247: loss 0.6209\n",
      "    ROC-AUC score: 0.7619\n",
      "    Batch 138 / 247: loss 0.6631\n",
      "    ROC-AUC score: 0.5234\n",
      "    Batch 141 / 247: loss 0.6343\n",
      "    ROC-AUC score: 0.5542\n",
      "    Batch 144 / 247: loss 0.6309\n",
      "    ROC-AUC score: 0.6588\n",
      "    Batch 147 / 247: loss 0.6554\n",
      "    ROC-AUC score: 0.6941\n",
      "    Batch 150 / 247: loss 0.6227\n",
      "    ROC-AUC score: 0.6190\n",
      "    Batch 153 / 247: loss 0.6299\n",
      "    ROC-AUC score: 0.5833\n",
      "    Batch 156 / 247: loss 0.6432\n",
      "    ROC-AUC score: 0.6708\n",
      "    Batch 159 / 247: loss 0.6960\n",
      "    ROC-AUC score: 0.8375\n",
      "    Batch 162 / 247: loss 0.5957\n",
      "    ROC-AUC score: 0.7167\n",
      "    Batch 165 / 247: loss 0.6413\n",
      "    ROC-AUC score: 0.8235\n",
      "    Batch 168 / 247: loss 0.6711\n",
      "    ROC-AUC score: 0.7961\n",
      "    Batch 171 / 247: loss 0.6829\n",
      "    ROC-AUC score: 0.5547\n",
      "    Batch 174 / 247: loss 0.6468\n",
      "    ROC-AUC score: 0.7005\n",
      "    Batch 177 / 247: loss 0.6359\n",
      "    ROC-AUC score: 0.7734\n",
      "    Batch 180 / 247: loss 0.6664\n",
      "    ROC-AUC score: 0.7091\n",
      "    Batch 183 / 247: loss 0.6537\n",
      "    ROC-AUC score: 0.6941\n",
      "    Batch 186 / 247: loss 0.6457\n",
      "    ROC-AUC score: 0.6471\n",
      "    Batch 189 / 247: loss 0.6693\n",
      "    ROC-AUC score: 0.6745\n",
      "    Batch 192 / 247: loss 0.6435\n",
      "    ROC-AUC score: 0.4683\n",
      "    Batch 195 / 247: loss 0.6431\n",
      "    ROC-AUC score: 0.7579\n",
      "    Batch 198 / 247: loss 0.6197\n",
      "    ROC-AUC score: 0.7222\n",
      "    Batch 201 / 247: loss 0.6108\n",
      "    ROC-AUC score: 0.8431\n",
      "    Batch 204 / 247: loss 0.6591\n",
      "    ROC-AUC score: 0.6559\n",
      "    Batch 207 / 247: loss 0.6449\n",
      "    ROC-AUC score: 0.8045\n",
      "    Batch 210 / 247: loss 0.7188\n",
      "    ROC-AUC score: 0.6353\n",
      "    Batch 213 / 247: loss 0.5898\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 216 / 247: loss 0.6436\n",
      "    ROC-AUC score: 0.7625\n",
      "    Batch 219 / 247: loss 0.6957\n",
      "    ROC-AUC score: 0.7749\n",
      "    Batch 222 / 247: loss 0.6410\n",
      "    ROC-AUC score: 0.6583\n",
      "    Batch 225 / 247: loss 0.6734\n",
      "    ROC-AUC score: 0.6559\n",
      "    Batch 228 / 247: loss 0.7048\n",
      "    ROC-AUC score: 0.4155\n",
      "    Batch 231 / 247: loss 0.6930\n",
      "    ROC-AUC score: 0.7166\n",
      "    Batch 234 / 247: loss 0.6315\n",
      "    ROC-AUC score: 0.7250\n",
      "    Batch 237 / 247: loss 0.6726\n",
      "    ROC-AUC score: 0.6389\n",
      "    Batch 240 / 247: loss 0.6509\n",
      "    ROC-AUC score: 0.6587\n",
      "    Batch 243 / 247: loss 0.6626\n",
      "    ROC-AUC score: 0.5451\n",
      "    Batch 246 / 247: loss 0.6313\n",
      "    ROC-AUC score: 0.6316\n",
      "Finished training.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    use_visdom = config['visdom']\n",
    "    if use_visdom:\n",
    "        vis = visdom.Visdom()\n",
    "        loss_window = None\n",
    "    criterion = utils.get_criterion(config['task'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                           weight_decay=config['weight_decay'])\n",
    "    epochs = config['epochs']\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.8)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200], gamma=0.5)\n",
    "    model.train()\n",
    "    print('--------------------------------')\n",
    "    print('Training.')\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "        running_loss = 0.0\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            loss = criterion(scores, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "            if (idx + 1) % stats_per_batch == 0:\n",
    "                running_loss /= stats_per_batch\n",
    "                print('    Batch {} / {}: loss {:.4f}'.format(\n",
    "                    idx+1, num_batches, running_loss))\n",
    "                if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "                    area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "                    print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "                running_loss = 0.0\n",
    "                num_correct, num_examples = 0, 0\n",
    "            if use_visdom:\n",
    "                if loss_window is None:\n",
    "                    loss_window = vis.line(\n",
    "                        Y=[loss.item()],\n",
    "                        X=[epoch*num_batches+idx],\n",
    "                        opts=dict(xlabel='batch', ylabel='Loss', title='Training Loss', legend=['Loss']))\n",
    "                else:\n",
    "                    vis.line(\n",
    "                        [loss.item()],\n",
    "                        [epoch*num_batches+idx],\n",
    "                        win=loss_window,\n",
    "                        update='append')\n",
    "            scheduler.step()\n",
    "    if use_visdom:\n",
    "        vis.close(win=loss_window)\n",
    "    print('Finished training.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['load']:\n",
    "    if config['save']:\n",
    "        print('--------------------------------')\n",
    "        directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                'trained_models')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        fname = utils.get_fname(config)\n",
    "        path = os.path.join(directory, fname)\n",
    "        print('Saving model at {}'.format(path))\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print('Finished saving model.')\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC score after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Computing ROC-AUC score for the training dataset after training.\n",
      "    Batch 1 / 247\n",
      "    Batch 2 / 247\n",
      "    Batch 3 / 247\n",
      "    Batch 4 / 247\n",
      "    Batch 5 / 247\n",
      "    Batch 6 / 247\n",
      "    Batch 7 / 247\n",
      "    Batch 8 / 247\n",
      "    Batch 9 / 247\n",
      "    Batch 10 / 247\n",
      "    Batch 11 / 247\n",
      "    Batch 12 / 247\n",
      "    Batch 13 / 247\n",
      "    Batch 14 / 247\n",
      "    Batch 15 / 247\n",
      "    Batch 16 / 247\n",
      "    Batch 17 / 247\n",
      "    Batch 18 / 247\n",
      "    Batch 19 / 247\n",
      "    Batch 20 / 247\n",
      "    Batch 21 / 247\n",
      "    Batch 22 / 247\n",
      "    Batch 23 / 247\n",
      "    Batch 24 / 247\n",
      "    Batch 25 / 247\n",
      "    Batch 26 / 247\n",
      "    Batch 27 / 247\n",
      "    Batch 28 / 247\n",
      "    Batch 29 / 247\n",
      "    Batch 30 / 247\n",
      "    Batch 31 / 247\n",
      "    Batch 32 / 247\n",
      "    Batch 33 / 247\n",
      "    Batch 34 / 247\n",
      "    Batch 35 / 247\n",
      "    Batch 36 / 247\n",
      "    Batch 37 / 247\n",
      "    Batch 38 / 247\n",
      "    Batch 39 / 247\n",
      "    Batch 40 / 247\n",
      "    Batch 41 / 247\n",
      "    Batch 42 / 247\n",
      "    Batch 43 / 247\n",
      "    Batch 44 / 247\n",
      "    Batch 45 / 247\n",
      "    Batch 46 / 247\n",
      "    Batch 47 / 247\n",
      "    Batch 48 / 247\n",
      "    Batch 49 / 247\n",
      "    Batch 50 / 247\n",
      "    Batch 51 / 247\n",
      "    Batch 52 / 247\n",
      "    Batch 53 / 247\n",
      "    Batch 54 / 247\n",
      "    Batch 55 / 247\n",
      "    Batch 56 / 247\n",
      "    Batch 57 / 247\n",
      "    Batch 58 / 247\n",
      "    Batch 59 / 247\n",
      "    Batch 60 / 247\n",
      "    Batch 61 / 247\n",
      "    Batch 62 / 247\n",
      "    Batch 63 / 247\n",
      "    Batch 64 / 247\n",
      "    Batch 65 / 247\n",
      "    Batch 66 / 247\n",
      "    Batch 67 / 247\n",
      "    Batch 68 / 247\n",
      "    Batch 69 / 247\n",
      "    Batch 70 / 247\n",
      "    Batch 71 / 247\n",
      "    Batch 72 / 247\n",
      "    Batch 73 / 247\n",
      "    Batch 74 / 247\n",
      "    Batch 75 / 247\n",
      "    Batch 76 / 247\n",
      "    Batch 77 / 247\n",
      "    Batch 78 / 247\n",
      "    Batch 79 / 247\n",
      "    Batch 80 / 247\n",
      "    Batch 81 / 247\n",
      "    Batch 82 / 247\n",
      "    Batch 83 / 247\n",
      "    Batch 84 / 247\n",
      "    Batch 85 / 247\n",
      "    Batch 86 / 247\n",
      "    Batch 87 / 247\n",
      "    Batch 88 / 247\n",
      "    Batch 89 / 247\n",
      "    Batch 90 / 247\n",
      "    Batch 91 / 247\n",
      "    Batch 92 / 247\n",
      "    Batch 93 / 247\n",
      "    Batch 94 / 247\n",
      "    Batch 95 / 247\n",
      "    Batch 96 / 247\n",
      "    Batch 97 / 247\n",
      "    Batch 98 / 247\n",
      "    Batch 99 / 247\n",
      "    Batch 100 / 247\n",
      "    Batch 101 / 247\n",
      "    Batch 102 / 247\n",
      "    Batch 103 / 247\n",
      "    Batch 104 / 247\n",
      "    Batch 105 / 247\n",
      "    Batch 106 / 247\n",
      "    Batch 107 / 247\n",
      "    Batch 108 / 247\n",
      "    Batch 109 / 247\n",
      "    Batch 110 / 247\n",
      "    Batch 111 / 247\n",
      "    Batch 112 / 247\n",
      "    Batch 113 / 247\n",
      "    Batch 114 / 247\n",
      "    Batch 115 / 247\n",
      "    Batch 116 / 247\n",
      "    Batch 117 / 247\n",
      "    Batch 118 / 247\n",
      "    Batch 119 / 247\n",
      "    Batch 120 / 247\n",
      "    Batch 121 / 247\n",
      "    Batch 122 / 247\n",
      "    Batch 123 / 247\n",
      "    Batch 124 / 247\n",
      "    Batch 125 / 247\n",
      "    Batch 126 / 247\n",
      "    Batch 127 / 247\n",
      "    Batch 128 / 247\n",
      "    Batch 129 / 247\n",
      "    Batch 130 / 247\n",
      "    Batch 131 / 247\n",
      "    Batch 132 / 247\n",
      "    Batch 133 / 247\n",
      "    Batch 134 / 247\n",
      "    Batch 135 / 247\n",
      "    Batch 136 / 247\n",
      "    Batch 137 / 247\n",
      "    Batch 138 / 247\n",
      "    Batch 139 / 247\n",
      "    Batch 140 / 247\n",
      "    Batch 141 / 247\n",
      "    Batch 142 / 247\n",
      "    Batch 143 / 247\n",
      "    Batch 144 / 247\n",
      "    Batch 145 / 247\n",
      "    Batch 146 / 247\n",
      "    Batch 147 / 247\n",
      "    Batch 148 / 247\n",
      "    Batch 149 / 247\n",
      "    Batch 150 / 247\n",
      "    Batch 151 / 247\n",
      "    Batch 152 / 247\n",
      "    Batch 153 / 247\n",
      "    Batch 154 / 247\n",
      "    Batch 155 / 247\n",
      "    Batch 156 / 247\n",
      "    Batch 157 / 247\n",
      "    Batch 158 / 247\n",
      "    Batch 159 / 247\n",
      "    Batch 160 / 247\n",
      "    Batch 161 / 247\n",
      "    Batch 162 / 247\n",
      "    Batch 163 / 247\n",
      "    Batch 164 / 247\n",
      "    Batch 165 / 247\n",
      "    Batch 166 / 247\n",
      "    Batch 167 / 247\n",
      "    Batch 168 / 247\n",
      "    Batch 169 / 247\n",
      "    Batch 170 / 247\n",
      "    Batch 171 / 247\n",
      "    Batch 172 / 247\n",
      "    Batch 173 / 247\n",
      "    Batch 174 / 247\n",
      "    Batch 175 / 247\n",
      "    Batch 176 / 247\n",
      "    Batch 177 / 247\n",
      "    Batch 178 / 247\n",
      "    Batch 179 / 247\n",
      "    Batch 180 / 247\n",
      "    Batch 181 / 247\n",
      "    Batch 182 / 247\n",
      "    Batch 183 / 247\n",
      "    Batch 184 / 247\n",
      "    Batch 185 / 247\n",
      "    Batch 186 / 247\n",
      "    Batch 187 / 247\n",
      "    Batch 188 / 247\n",
      "    Batch 189 / 247\n",
      "    Batch 190 / 247\n",
      "    Batch 191 / 247\n",
      "    Batch 192 / 247\n",
      "    Batch 193 / 247\n",
      "    Batch 194 / 247\n",
      "    Batch 195 / 247\n",
      "    Batch 196 / 247\n",
      "    Batch 197 / 247\n",
      "    Batch 198 / 247\n",
      "    Batch 199 / 247\n",
      "    Batch 200 / 247\n",
      "    Batch 201 / 247\n",
      "    Batch 202 / 247\n",
      "    Batch 203 / 247\n",
      "    Batch 204 / 247\n",
      "    Batch 205 / 247\n",
      "    Batch 206 / 247\n",
      "    Batch 207 / 247\n",
      "    Batch 208 / 247\n",
      "    Batch 209 / 247\n",
      "    Batch 210 / 247\n",
      "    Batch 211 / 247\n",
      "    Batch 212 / 247\n",
      "    Batch 213 / 247\n",
      "    Batch 214 / 247\n",
      "    Batch 215 / 247\n",
      "    Batch 216 / 247\n",
      "    Batch 217 / 247\n",
      "    Batch 218 / 247\n",
      "    Batch 219 / 247\n",
      "    Batch 220 / 247\n",
      "    Batch 221 / 247\n",
      "    Batch 222 / 247\n",
      "    Batch 223 / 247\n",
      "    Batch 224 / 247\n",
      "    Batch 225 / 247\n",
      "    Batch 226 / 247\n",
      "    Batch 227 / 247\n",
      "    Batch 228 / 247\n",
      "    Batch 229 / 247\n",
      "    Batch 230 / 247\n",
      "    Batch 231 / 247\n",
      "    Batch 232 / 247\n",
      "    Batch 233 / 247\n",
      "    Batch 234 / 247\n",
      "    Batch 235 / 247\n",
      "    Batch 236 / 247\n",
      "    Batch 237 / 247\n",
      "    Batch 238 / 247\n",
      "    Batch 239 / 247\n",
      "    Batch 240 / 247\n",
      "    Batch 241 / 247\n",
      "    Batch 242 / 247\n",
      "    Batch 243 / 247\n",
      "    Batch 244 / 247\n",
      "    Batch 245 / 247\n",
      "    Batch 246 / 247\n",
      "    Batch 247 / 247\n",
      "ROC-AUC score: 0.6920\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    print('--------------------------------')\n",
    "    print('Computing ROC-AUC score for the training dataset after training.')\n",
    "    y_true, y_scores = [], []\n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    with torch.no_grad():\n",
    "        for (idx, batch) in enumerate(loader):\n",
    "            edges, features, node_layers, mappings, rows, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            out = model(features, node_layers, mappings, rows)\n",
    "            all_pairs = torch.mm(out, out.t())\n",
    "            scores = all_pairs[edges.T]\n",
    "            y_true.extend(labels.detach().numpy())\n",
    "            y_scores.extend(scores.detach().numpy())\n",
    "            print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    area = roc_auc_score(y_true, y_scores)\n",
    "    print('ROC-AUC score: {:.4f}'.format(area))\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the true positive rate and true negative rate vs threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not config['load']:\n",
    "    tpr, fpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    tnr = 1 - fpr\n",
    "    plt.plot(thresholds, tpr, label='tpr')\n",
    "    plt.plot(thresholds, tnr, label='tnr')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.title('TPR / TNR vs Threshold')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an appropriate threshold and generate classification report on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 1 / 247\n",
      "    Batch 2 / 247\n",
      "    Batch 3 / 247\n",
      "    Batch 4 / 247\n",
      "    Batch 5 / 247\n",
      "    Batch 6 / 247\n",
      "    Batch 7 / 247\n",
      "    Batch 8 / 247\n",
      "    Batch 9 / 247\n",
      "    Batch 10 / 247\n",
      "    Batch 11 / 247\n",
      "    Batch 12 / 247\n",
      "    Batch 13 / 247\n",
      "    Batch 14 / 247\n",
      "    Batch 15 / 247\n",
      "    Batch 16 / 247\n",
      "    Batch 17 / 247\n",
      "    Batch 18 / 247\n",
      "    Batch 19 / 247\n",
      "    Batch 20 / 247\n",
      "    Batch 21 / 247\n",
      "    Batch 22 / 247\n",
      "    Batch 23 / 247\n",
      "    Batch 24 / 247\n",
      "    Batch 25 / 247\n",
      "    Batch 26 / 247\n",
      "    Batch 27 / 247\n",
      "    Batch 28 / 247\n",
      "    Batch 29 / 247\n",
      "    Batch 30 / 247\n",
      "    Batch 31 / 247\n",
      "    Batch 32 / 247\n",
      "    Batch 33 / 247\n",
      "    Batch 34 / 247\n",
      "    Batch 35 / 247\n",
      "    Batch 36 / 247\n",
      "    Batch 37 / 247\n",
      "    Batch 38 / 247\n",
      "    Batch 39 / 247\n",
      "    Batch 40 / 247\n",
      "    Batch 41 / 247\n",
      "    Batch 42 / 247\n",
      "    Batch 43 / 247\n",
      "    Batch 44 / 247\n",
      "    Batch 45 / 247\n",
      "    Batch 46 / 247\n",
      "    Batch 47 / 247\n",
      "    Batch 48 / 247\n",
      "    Batch 49 / 247\n",
      "    Batch 50 / 247\n",
      "    Batch 51 / 247\n",
      "    Batch 52 / 247\n",
      "    Batch 53 / 247\n",
      "    Batch 54 / 247\n",
      "    Batch 55 / 247\n",
      "    Batch 56 / 247\n",
      "    Batch 57 / 247\n",
      "    Batch 58 / 247\n",
      "    Batch 59 / 247\n",
      "    Batch 60 / 247\n",
      "    Batch 61 / 247\n",
      "    Batch 62 / 247\n",
      "    Batch 63 / 247\n",
      "    Batch 64 / 247\n",
      "    Batch 65 / 247\n",
      "    Batch 66 / 247\n",
      "    Batch 67 / 247\n",
      "    Batch 68 / 247\n",
      "    Batch 69 / 247\n",
      "    Batch 70 / 247\n",
      "    Batch 71 / 247\n",
      "    Batch 72 / 247\n",
      "    Batch 73 / 247\n",
      "    Batch 74 / 247\n",
      "    Batch 75 / 247\n",
      "    Batch 76 / 247\n",
      "    Batch 77 / 247\n",
      "    Batch 78 / 247\n",
      "    Batch 79 / 247\n",
      "    Batch 80 / 247\n",
      "    Batch 81 / 247\n",
      "    Batch 82 / 247\n",
      "    Batch 83 / 247\n",
      "    Batch 84 / 247\n",
      "    Batch 85 / 247\n",
      "    Batch 86 / 247\n",
      "    Batch 87 / 247\n",
      "    Batch 88 / 247\n",
      "    Batch 89 / 247\n",
      "    Batch 90 / 247\n",
      "    Batch 91 / 247\n",
      "    Batch 92 / 247\n",
      "    Batch 93 / 247\n",
      "    Batch 94 / 247\n",
      "    Batch 95 / 247\n",
      "    Batch 96 / 247\n",
      "    Batch 97 / 247\n",
      "    Batch 98 / 247\n",
      "    Batch 99 / 247\n",
      "    Batch 100 / 247\n",
      "    Batch 101 / 247\n",
      "    Batch 102 / 247\n",
      "    Batch 103 / 247\n",
      "    Batch 104 / 247\n",
      "    Batch 105 / 247\n",
      "    Batch 106 / 247\n",
      "    Batch 107 / 247\n",
      "    Batch 108 / 247\n",
      "    Batch 109 / 247\n",
      "    Batch 110 / 247\n",
      "    Batch 111 / 247\n",
      "    Batch 112 / 247\n",
      "    Batch 113 / 247\n",
      "    Batch 114 / 247\n",
      "    Batch 115 / 247\n",
      "    Batch 116 / 247\n",
      "    Batch 117 / 247\n",
      "    Batch 118 / 247\n",
      "    Batch 119 / 247\n",
      "    Batch 120 / 247\n",
      "    Batch 121 / 247\n",
      "    Batch 122 / 247\n",
      "    Batch 123 / 247\n",
      "    Batch 124 / 247\n",
      "    Batch 125 / 247\n",
      "    Batch 126 / 247\n",
      "    Batch 127 / 247\n",
      "    Batch 128 / 247\n",
      "    Batch 129 / 247\n",
      "    Batch 130 / 247\n",
      "    Batch 131 / 247\n",
      "    Batch 132 / 247\n",
      "    Batch 133 / 247\n",
      "    Batch 134 / 247\n",
      "    Batch 135 / 247\n",
      "    Batch 136 / 247\n",
      "    Batch 137 / 247\n",
      "    Batch 138 / 247\n",
      "    Batch 139 / 247\n",
      "    Batch 140 / 247\n",
      "    Batch 141 / 247\n",
      "    Batch 142 / 247\n",
      "    Batch 143 / 247\n",
      "    Batch 144 / 247\n",
      "    Batch 145 / 247\n",
      "    Batch 146 / 247\n",
      "    Batch 147 / 247\n",
      "    Batch 148 / 247\n",
      "    Batch 149 / 247\n",
      "    Batch 150 / 247\n",
      "    Batch 151 / 247\n",
      "    Batch 152 / 247\n",
      "    Batch 153 / 247\n",
      "    Batch 154 / 247\n",
      "    Batch 155 / 247\n",
      "    Batch 156 / 247\n",
      "    Batch 157 / 247\n",
      "    Batch 158 / 247\n",
      "    Batch 159 / 247\n",
      "    Batch 160 / 247\n",
      "    Batch 161 / 247\n",
      "    Batch 162 / 247\n",
      "    Batch 163 / 247\n",
      "    Batch 164 / 247\n",
      "    Batch 165 / 247\n",
      "    Batch 166 / 247\n",
      "    Batch 167 / 247\n",
      "    Batch 168 / 247\n",
      "    Batch 169 / 247\n",
      "    Batch 170 / 247\n",
      "    Batch 171 / 247\n",
      "    Batch 172 / 247\n",
      "    Batch 173 / 247\n",
      "    Batch 174 / 247\n",
      "    Batch 175 / 247\n",
      "    Batch 176 / 247\n",
      "    Batch 177 / 247\n",
      "    Batch 178 / 247\n",
      "    Batch 179 / 247\n",
      "    Batch 180 / 247\n",
      "    Batch 181 / 247\n",
      "    Batch 182 / 247\n",
      "    Batch 183 / 247\n",
      "    Batch 184 / 247\n",
      "    Batch 185 / 247\n",
      "    Batch 186 / 247\n",
      "    Batch 187 / 247\n",
      "    Batch 188 / 247\n",
      "    Batch 189 / 247\n",
      "    Batch 190 / 247\n",
      "    Batch 191 / 247\n",
      "    Batch 192 / 247\n",
      "    Batch 193 / 247\n",
      "    Batch 194 / 247\n",
      "    Batch 195 / 247\n",
      "    Batch 196 / 247\n",
      "    Batch 197 / 247\n",
      "    Batch 198 / 247\n",
      "    Batch 199 / 247\n",
      "    Batch 200 / 247\n",
      "    Batch 201 / 247\n",
      "    Batch 202 / 247\n",
      "    Batch 203 / 247\n",
      "    Batch 204 / 247\n",
      "    Batch 205 / 247\n",
      "    Batch 206 / 247\n",
      "    Batch 207 / 247\n",
      "    Batch 208 / 247\n",
      "    Batch 209 / 247\n",
      "    Batch 210 / 247\n",
      "    Batch 211 / 247\n",
      "    Batch 212 / 247\n",
      "    Batch 213 / 247\n",
      "    Batch 214 / 247\n",
      "    Batch 215 / 247\n",
      "    Batch 216 / 247\n",
      "    Batch 217 / 247\n",
      "    Batch 218 / 247\n",
      "    Batch 219 / 247\n",
      "    Batch 220 / 247\n",
      "    Batch 221 / 247\n",
      "    Batch 222 / 247\n",
      "    Batch 223 / 247\n",
      "    Batch 224 / 247\n",
      "    Batch 225 / 247\n",
      "    Batch 226 / 247\n",
      "    Batch 227 / 247\n",
      "    Batch 228 / 247\n",
      "    Batch 229 / 247\n",
      "    Batch 230 / 247\n",
      "    Batch 231 / 247\n",
      "    Batch 232 / 247\n",
      "    Batch 233 / 247\n",
      "    Batch 234 / 247\n",
      "    Batch 235 / 247\n",
      "    Batch 236 / 247\n",
      "    Batch 237 / 247\n",
      "    Batch 238 / 247\n",
      "    Batch 239 / 247\n",
      "    Batch 240 / 247\n",
      "    Batch 241 / 247\n",
      "    Batch 242 / 247\n",
      "    Batch 243 / 247\n",
      "    Batch 244 / 247\n",
      "    Batch 245 / 247\n",
      "    Batch 246 / 247\n",
      "    Batch 247 / 247\n",
      "Threshold: 0.1780, accuracy: 0.6593\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.66      0.66      3943\n",
      "         1.0       0.66      0.66      0.66      3943\n",
      "\n",
      "    accuracy                           0.66      7886\n",
      "   macro avg       0.66      0.66      0.66      7886\n",
      "weighted avg       0.66      0.66      0.66      7886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx1 = np.where(tpr <= tnr)[0]\n",
    "idx2 = np.where(tpr >= tnr)[0]\n",
    "t = thresholds[idx1[-1]]\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "with torch.no_grad():\n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        edges, features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        all_pairs = torch.mm(out, out.t())\n",
    "        scores = all_pairs[edges.T]\n",
    "        predictions = (scores >= t).long()\n",
    "        y_true.extend(labels.detach().numpy())\n",
    "        y_pred.extend(predictions.detach().numpy())\n",
    "        total_correct += torch.sum(predictions == labels.long()).item()\n",
    "        total_examples += len(labels) \n",
    "        print('    Batch {} / {}'.format(idx+1, num_batches))\n",
    "print('Threshold: {:.4f}, accuracy: {:.4f}'.format(t, total_correct / total_examples))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "print('Classification report\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contact/ia-contact.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: val\n",
      "Number of vertices: 274\n",
      "Number of static edges: 2112\n",
      "Number of temporal edges: 14122\n",
      "Number of examples/datapoints: 14122\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the validation dataset after training.\n",
      "    Batch 3 / 442: loss 0.4659, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8353\n",
      "    Batch 6 / 442: loss 0.3777, accuracy 0.8958\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 9 / 442: loss 0.5202, accuracy 0.8229\n",
      "    ROC-AUC score: 0.9365\n",
      "    Batch 12 / 442: loss 0.5451, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8219\n",
      "    Batch 15 / 442: loss 0.5577, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8849\n",
      "    Batch 18 / 442: loss 0.4859, accuracy 0.7708\n",
      "    ROC-AUC score: 0.9255\n",
      "    Batch 21 / 442: loss 0.4226, accuracy 0.7917\n",
      "    ROC-AUC score: 0.9206\n",
      "    Batch 24 / 442: loss 0.4835, accuracy 0.9062\n",
      "    ROC-AUC score: 0.9917\n",
      "    Batch 27 / 442: loss 0.4924, accuracy 0.8333\n",
      "    ROC-AUC score: 0.8947\n",
      "    Batch 30 / 442: loss 0.4409, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9792\n",
      "    Batch 33 / 442: loss 0.5034, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9150\n",
      "    Batch 36 / 442: loss 0.4032, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9333\n",
      "    Batch 39 / 442: loss 0.4832, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8214\n",
      "    Batch 42 / 442: loss 0.5575, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9833\n",
      "    Batch 45 / 442: loss 0.4416, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9643\n",
      "    Batch 48 / 442: loss 0.4739, accuracy 0.8958\n",
      "    ROC-AUC score: 0.9444\n",
      "    Batch 51 / 442: loss 0.4807, accuracy 0.7708\n",
      "    ROC-AUC score: 0.9292\n",
      "    Batch 54 / 442: loss 0.5468, accuracy 0.7812\n",
      "    ROC-AUC score: 0.8902\n",
      "    Batch 57 / 442: loss 0.4288, accuracy 0.8958\n",
      "    ROC-AUC score: 0.8824\n",
      "    Batch 60 / 442: loss 0.4655, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9542\n",
      "    Batch 63 / 442: loss 0.4617, accuracy 0.8229\n",
      "    ROC-AUC score: 0.9453\n",
      "    Batch 66 / 442: loss 0.3663, accuracy 0.8750\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 69 / 442: loss 0.5348, accuracy 0.8125\n",
      "    ROC-AUC score: 0.7976\n",
      "    Batch 72 / 442: loss 0.5148, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9563\n",
      "    Batch 75 / 442: loss 0.4882, accuracy 0.8333\n",
      "    ROC-AUC score: 0.8571\n",
      "    Batch 78 / 442: loss 0.4633, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9555\n",
      "    Batch 81 / 442: loss 0.4966, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9414\n",
      "    Batch 84 / 442: loss 0.4520, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9727\n",
      "    Batch 87 / 442: loss 0.4430, accuracy 0.8021\n",
      "    ROC-AUC score: 0.8945\n",
      "    Batch 90 / 442: loss 0.4576, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9725\n",
      "    Batch 93 / 442: loss 0.3711, accuracy 0.8646\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 96 / 442: loss 0.4928, accuracy 0.9062\n",
      "    ROC-AUC score: 0.9882\n",
      "    Batch 99 / 442: loss 0.4698, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8477\n",
      "    Batch 102 / 442: loss 0.4687, accuracy 0.8958\n",
      "    ROC-AUC score: 0.9804\n",
      "    Batch 105 / 442: loss 0.5288, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9802\n",
      "    Batch 108 / 442: loss 0.4598, accuracy 0.7917\n",
      "    ROC-AUC score: 0.9134\n",
      "    Batch 111 / 442: loss 0.4248, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9762\n",
      "    Batch 114 / 442: loss 0.4514, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9960\n",
      "    Batch 117 / 442: loss 0.4153, accuracy 0.8125\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 120 / 442: loss 0.5556, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9802\n",
      "    Batch 123 / 442: loss 0.4947, accuracy 0.7812\n",
      "    ROC-AUC score: 0.9765\n",
      "    Batch 126 / 442: loss 0.5507, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9608\n",
      "    Batch 129 / 442: loss 0.4516, accuracy 0.8438\n",
      "    ROC-AUC score: 0.8902\n",
      "    Batch 132 / 442: loss 0.3986, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9922\n",
      "    Batch 135 / 442: loss 0.4419, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9960\n",
      "    Batch 138 / 442: loss 0.4421, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9762\n",
      "    Batch 141 / 442: loss 0.4548, accuracy 0.8438\n",
      "    ROC-AUC score: 0.8984\n",
      "    Batch 144 / 442: loss 0.4505, accuracy 0.8646\n",
      "    ROC-AUC score: 0.8889\n",
      "    Batch 147 / 442: loss 0.5339, accuracy 0.8854\n",
      "    ROC-AUC score: 0.8594\n",
      "    Batch 150 / 442: loss 0.4829, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9757\n",
      "    Batch 153 / 442: loss 0.5286, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9227\n",
      "    Batch 156 / 442: loss 0.5542, accuracy 0.7917\n",
      "    ROC-AUC score: 0.7569\n",
      "    Batch 159 / 442: loss 0.4897, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9206\n",
      "    Batch 162 / 442: loss 0.4080, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9500\n",
      "    Batch 165 / 442: loss 0.4727, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9686\n",
      "    Batch 168 / 442: loss 0.4871, accuracy 0.8229\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 171 / 442: loss 0.5178, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9922\n",
      "    Batch 174 / 442: loss 0.4627, accuracy 0.8229\n",
      "    ROC-AUC score: 0.8730\n",
      "    Batch 177 / 442: loss 0.3990, accuracy 0.7604\n",
      "    ROC-AUC score: 0.9697\n",
      "    Batch 180 / 442: loss 0.4134, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9484\n",
      "    Batch 183 / 442: loss 0.4776, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9686\n",
      "    Batch 186 / 442: loss 0.4619, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9255\n",
      "    Batch 189 / 442: loss 0.4253, accuracy 0.8021\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 192 / 442: loss 0.4751, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9412\n",
      "    Batch 195 / 442: loss 0.4157, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8941\n",
      "    Batch 198 / 442: loss 0.4754, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9682\n",
      "    Batch 201 / 442: loss 0.4715, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8227\n",
      "    Batch 204 / 442: loss 0.4575, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9059\n",
      "    Batch 207 / 442: loss 0.4278, accuracy 0.8229\n",
      "    ROC-AUC score: 0.9960\n",
      "    Batch 210 / 442: loss 0.4257, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9667\n",
      "    Batch 213 / 442: loss 0.4576, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9490\n",
      "    Batch 216 / 442: loss 0.4280, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9917\n",
      "    Batch 219 / 442: loss 0.4050, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8421\n",
      "    Batch 222 / 442: loss 0.4399, accuracy 0.8958\n",
      "    ROC-AUC score: 0.9765\n",
      "    Batch 225 / 442: loss 0.4570, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9275\n",
      "    Batch 228 / 442: loss 0.4287, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9838\n",
      "    Batch 231 / 442: loss 0.5214, accuracy 0.8125\n",
      "    ROC-AUC score: 0.8078\n",
      "    Batch 234 / 442: loss 0.4593, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9725\n",
      "    Batch 237 / 442: loss 0.5253, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9373\n",
      "    Batch 240 / 442: loss 0.4554, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9414\n",
      "    Batch 243 / 442: loss 0.4601, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9570\n",
      "    Batch 246 / 442: loss 0.4486, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9683\n",
      "    Batch 249 / 442: loss 0.5219, accuracy 0.8229\n",
      "    ROC-AUC score: 0.9312\n",
      "    Batch 252 / 442: loss 0.5186, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9373\n",
      "    Batch 255 / 442: loss 0.4214, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9292\n",
      "    Batch 258 / 442: loss 0.4436, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9766\n",
      "    Batch 261 / 442: loss 0.4546, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9843\n",
      "    Batch 264 / 442: loss 0.5360, accuracy 0.8229\n",
      "    ROC-AUC score: 0.8667\n",
      "    Batch 267 / 442: loss 0.4907, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9643\n",
      "    Batch 270 / 442: loss 0.5581, accuracy 0.8438\n",
      "    ROC-AUC score: 0.8947\n",
      "    Batch 273 / 442: loss 0.4639, accuracy 0.8021\n",
      "    ROC-AUC score: 0.9563\n",
      "    Batch 276 / 442: loss 0.4386, accuracy 0.7812\n",
      "    ROC-AUC score: 0.9069\n",
      "    Batch 279 / 442: loss 0.4144, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9804\n",
      "    Batch 282 / 442: loss 0.4604, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9569\n",
      "    Batch 285 / 442: loss 0.4976, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9804\n",
      "    Batch 288 / 442: loss 0.4510, accuracy 0.8125\n",
      "    ROC-AUC score: 0.8947\n",
      "    Batch 291 / 442: loss 0.4972, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9610\n",
      "    Batch 294 / 442: loss 0.4829, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9879\n",
      "    Batch 297 / 442: loss 0.4941, accuracy 0.9375\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 300 / 442: loss 0.4595, accuracy 0.8542\n",
      "    ROC-AUC score: 0.8042\n",
      "    Batch 303 / 442: loss 0.4213, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 306 / 442: loss 0.4435, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9804\n",
      "    Batch 309 / 442: loss 0.4709, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9762\n",
      "    Batch 312 / 442: loss 0.3869, accuracy 0.8229\n",
      "    ROC-AUC score: 0.8958\n",
      "    Batch 315 / 442: loss 0.5191, accuracy 0.8750\n",
      "    ROC-AUC score: 0.8696\n",
      "    Batch 318 / 442: loss 0.4682, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9492\n",
      "    Batch 321 / 442: loss 0.4777, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9286\n",
      "    Batch 324 / 442: loss 0.4501, accuracy 0.8646\n",
      "    ROC-AUC score: 0.8958\n",
      "    Batch 327 / 442: loss 0.4264, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9570\n",
      "    Batch 330 / 442: loss 0.4752, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9333\n",
      "    Batch 333 / 442: loss 0.4282, accuracy 0.7917\n",
      "    ROC-AUC score: 0.8528\n",
      "    Batch 336 / 442: loss 0.4193, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9603\n",
      "    Batch 339 / 442: loss 0.4173, accuracy 0.8646\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 342 / 442: loss 0.4087, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9569\n",
      "    Batch 345 / 442: loss 0.4324, accuracy 0.8229\n",
      "    ROC-AUC score: 0.9765\n",
      "    Batch 348 / 442: loss 0.4143, accuracy 0.8021\n",
      "    ROC-AUC score: 0.9762\n",
      "    Batch 351 / 442: loss 0.4531, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9961\n",
      "    Batch 354 / 442: loss 0.4770, accuracy 0.7812\n",
      "    ROC-AUC score: 0.9843\n",
      "    Batch 357 / 442: loss 0.3814, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9804\n",
      "    Batch 360 / 442: loss 0.4964, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9062\n",
      "    Batch 363 / 442: loss 0.4039, accuracy 0.9062\n",
      "    ROC-AUC score: 0.9922\n",
      "    Batch 366 / 442: loss 0.5390, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9190\n",
      "    Batch 369 / 442: loss 0.5305, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9109\n",
      "    Batch 372 / 442: loss 0.4573, accuracy 0.9167\n",
      "    ROC-AUC score: 0.9686\n",
      "    Batch 375 / 442: loss 0.3952, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9961\n",
      "    Batch 378 / 442: loss 0.4500, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9451\n",
      "    Batch 381 / 442: loss 0.5206, accuracy 0.8438\n",
      "    ROC-AUC score: 1.0000\n",
      "    Batch 384 / 442: loss 0.4735, accuracy 0.8750\n",
      "    ROC-AUC score: 0.9492\n",
      "    Batch 387 / 442: loss 0.4793, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 390 / 442: loss 0.4084, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9610\n",
      "    Batch 393 / 442: loss 0.4699, accuracy 0.8229\n",
      "    ROC-AUC score: 0.9492\n",
      "    Batch 396 / 442: loss 0.4401, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9960\n",
      "    Batch 399 / 442: loss 0.4265, accuracy 0.8854\n",
      "    ROC-AUC score: 0.9569\n",
      "    Batch 402 / 442: loss 0.4095, accuracy 0.7917\n",
      "    ROC-AUC score: 0.9048\n",
      "    Batch 405 / 442: loss 0.3899, accuracy 0.9479\n",
      "    ROC-AUC score: 0.9960\n",
      "    Batch 408 / 442: loss 0.4774, accuracy 0.8021\n",
      "    ROC-AUC score: 0.9325\n",
      "    Batch 411 / 442: loss 0.4548, accuracy 0.8438\n",
      "    ROC-AUC score: 0.9271\n",
      "    Batch 414 / 442: loss 0.5199, accuracy 0.7917\n",
      "    ROC-AUC score: 0.9286\n",
      "    Batch 417 / 442: loss 0.4172, accuracy 0.9167\n",
      "    ROC-AUC score: 0.9393\n",
      "    Batch 420 / 442: loss 0.4545, accuracy 0.8646\n",
      "    ROC-AUC score: 0.9150\n",
      "    Batch 423 / 442: loss 0.4899, accuracy 0.8125\n",
      "    ROC-AUC score: 0.8292\n",
      "    Batch 426 / 442: loss 0.5062, accuracy 0.8021\n",
      "    ROC-AUC score: 0.9563\n",
      "    Batch 429 / 442: loss 0.3574, accuracy 0.9062\n",
      "    ROC-AUC score: 0.9792\n",
      "    Batch 432 / 442: loss 0.3912, accuracy 0.8542\n",
      "    ROC-AUC score: 0.9490\n",
      "    Batch 435 / 442: loss 0.5631, accuracy 0.7917\n",
      "    ROC-AUC score: 0.9569\n",
      "    Batch 438 / 442: loss 0.5300, accuracy 0.8854\n",
      "    ROC-AUC score: 0.8706\n",
      "    Batch 441 / 442: loss 0.3967, accuracy 0.8333\n",
      "    ROC-AUC score: 0.9625\n",
      "Loss 0.4639, accuracy 0.8451\n",
      "ROC-AUC score: 0.9416\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.96      0.86      7061\n",
      "         1.0       0.95      0.73      0.82      7061\n",
      "\n",
      "    accuracy                           0.85     14122\n",
      "   macro avg       0.86      0.85      0.84     14122\n",
      "weighted avg       0.86      0.85      0.84     14122\n",
      "\n",
      "Finished validating.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'val',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the validation dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished validating.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading dataset from /Users/raunak/Documents/Datasets/temporal-networks-network-repository/ia-contact/ia-contact.edges\n",
      "Finished reading data.\n",
      "Setting up graph.\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: test\n",
      "Number of vertices: 274\n",
      "Number of static edges: 2472\n",
      "Number of temporal edges: 21183\n",
      "Number of examples/datapoints: 14100\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Computing ROC-AUC score for the test dataset after training.\n",
      "    Batch 3 / 441: loss 0.4724, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8955\n",
      "    Batch 6 / 441: loss 0.4283, accuracy 0.7188\n",
      "    ROC-AUC score: 0.9960\n",
      "    Batch 9 / 441: loss 0.4950, accuracy 0.7604\n",
      "    ROC-AUC score: 0.8929\n",
      "    Batch 12 / 441: loss 0.5190, accuracy 0.8125\n",
      "    ROC-AUC score: 0.9127\n",
      "    Batch 15 / 441: loss 0.4771, accuracy 0.7292\n",
      "    ROC-AUC score: 0.8471\n",
      "    Batch 18 / 441: loss 0.4853, accuracy 0.6562\n",
      "    ROC-AUC score: 0.9167\n",
      "    Batch 21 / 441: loss 0.4466, accuracy 0.7292\n",
      "    ROC-AUC score: 0.9264\n",
      "    Batch 24 / 441: loss 0.5247, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8340\n",
      "    Batch 27 / 441: loss 0.5177, accuracy 0.6875\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 30 / 441: loss 0.6283, accuracy 0.6458\n",
      "    ROC-AUC score: 0.7843\n",
      "    Batch 33 / 441: loss 0.5420, accuracy 0.6562\n",
      "    ROC-AUC score: 0.9264\n",
      "    Batch 36 / 441: loss 0.5775, accuracy 0.6458\n",
      "    ROC-AUC score: 0.8863\n",
      "    Batch 39 / 441: loss 0.5252, accuracy 0.6771\n",
      "    ROC-AUC score: 0.7686\n",
      "    Batch 42 / 441: loss 0.5538, accuracy 0.5938\n",
      "    ROC-AUC score: 0.7583\n",
      "    Batch 45 / 441: loss 0.5283, accuracy 0.6979\n",
      "    ROC-AUC score: 0.9219\n",
      "    Batch 48 / 441: loss 0.4938, accuracy 0.7500\n",
      "    ROC-AUC score: 0.8009\n",
      "    Batch 51 / 441: loss 0.4897, accuracy 0.6875\n",
      "    ROC-AUC score: 0.9333\n",
      "    Batch 54 / 441: loss 0.5226, accuracy 0.6667\n",
      "    ROC-AUC score: 0.8627\n",
      "    Batch 57 / 441: loss 0.6158, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8203\n",
      "    Batch 60 / 441: loss 0.6487, accuracy 0.7396\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 63 / 441: loss 0.5739, accuracy 0.6771\n",
      "    ROC-AUC score: 0.9062\n",
      "    Batch 66 / 441: loss 0.5108, accuracy 0.6979\n",
      "    ROC-AUC score: 0.9843\n",
      "    Batch 69 / 441: loss 0.4658, accuracy 0.6562\n",
      "    ROC-AUC score: 0.8095\n",
      "    Batch 72 / 441: loss 0.6695, accuracy 0.6771\n",
      "    ROC-AUC score: 0.8958\n",
      "    Batch 75 / 441: loss 0.5582, accuracy 0.6354\n",
      "    ROC-AUC score: 0.9020\n",
      "    Batch 78 / 441: loss 0.5283, accuracy 0.6979\n",
      "    ROC-AUC score: 0.8867\n",
      "    Batch 81 / 441: loss 0.5909, accuracy 0.7188\n",
      "    ROC-AUC score: 0.9273\n",
      "    Batch 84 / 441: loss 0.5029, accuracy 0.7083\n",
      "    ROC-AUC score: 0.9221\n",
      "    Batch 87 / 441: loss 0.6194, accuracy 0.6667\n",
      "    ROC-AUC score: 0.9286\n",
      "    Batch 90 / 441: loss 0.5507, accuracy 0.6250\n",
      "    ROC-AUC score: 0.9206\n",
      "    Batch 93 / 441: loss 0.5962, accuracy 0.7188\n",
      "    ROC-AUC score: 0.8889\n",
      "    Batch 96 / 441: loss 0.4778, accuracy 0.6667\n",
      "    ROC-AUC score: 0.9008\n",
      "    Batch 99 / 441: loss 0.5749, accuracy 0.7083\n",
      "    ROC-AUC score: 0.8672\n"
     ]
    }
   ],
   "source": [
    "if config['load']:\n",
    "    directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                             'trained_models')\n",
    "    fname = utils.get_fname(config)\n",
    "    path = os.path.join(directory, fname)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                config['generate_neg_examples'], 'test',\n",
    "                config['duplicate_examples'], config['repeat_examples'],\n",
    "                config['num_layers'], config['self_loop'],\n",
    "                config['normalize_adj'])\n",
    "dataset = utils.get_dataset(dataset_args)\n",
    "loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                    shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "criterion = utils.get_criterion(config['task'])\n",
    "stats_per_batch = config['stats_per_batch']\n",
    "num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "model.eval()\n",
    "print('--------------------------------')\n",
    "print('Computing ROC-AUC score for the test dataset after training.')\n",
    "running_loss, total_loss = 0.0, 0.0\n",
    "num_correct, num_examples = 0, 0\n",
    "total_correct, total_examples = 0, 0\n",
    "y_true, y_scores, y_pred = [], [], []\n",
    "for (idx, batch) in enumerate(loader):\n",
    "    edges, features, node_layers, mappings, rows, labels = batch\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features, node_layers, mappings, rows)\n",
    "    all_pairs = torch.mm(out, out.t())\n",
    "    scores = all_pairs[edges.T]\n",
    "    loss = criterion(scores, labels.float())\n",
    "    running_loss += loss.item()\n",
    "    total_loss += loss.item()\n",
    "    predictions = (scores >= t).long()\n",
    "    num_correct += torch.sum(predictions == labels.long()).item()\n",
    "    total_correct += torch.sum(predictions == labels.long()).item()\n",
    "    num_examples += len(labels)\n",
    "    total_examples += len(labels)\n",
    "    y_true.extend(labels.detach().numpy())\n",
    "    y_scores.extend(scores.detach().numpy())\n",
    "    y_pred.extend(predictions.detach().numpy())\n",
    "    if (idx + 1) % stats_per_batch == 0:\n",
    "        running_loss /= stats_per_batch\n",
    "        accuracy = num_correct / num_examples\n",
    "        print('    Batch {} / {}: loss {:.4f}, accuracy {:.4f}'.format(\n",
    "            idx+1, num_batches, running_loss, accuracy))\n",
    "        if (torch.sum(labels.long() == 0).item() > 0) and (torch.sum(labels.long() == 1).item() > 0):\n",
    "            area = roc_auc_score(labels.detach().numpy(), scores.detach().numpy())\n",
    "            print('    ROC-AUC score: {:.4f}'.format(area))\n",
    "        running_loss = 0.0\n",
    "        num_correct, num_examples = 0, 0\n",
    "total_loss /= num_batches\n",
    "total_accuracy = total_correct / total_examples\n",
    "print('Loss {:.4f}, accuracy {:.4f}'.format(total_loss, total_accuracy))\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_scores = np.array(y_scores).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "report = classification_report(y_true, y_pred)\n",
    "area = roc_auc_score(y_true, y_scores)\n",
    "print('ROC-AUC score: {:.4f}'.format(area))\n",
    "print('Classification report\\n', report)\n",
    "print('Finished testing.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
